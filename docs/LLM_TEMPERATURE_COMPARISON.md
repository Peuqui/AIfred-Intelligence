# ğŸŒ¡ï¸ LLM Model & Temperature Comparison

**Test Date:** 2025-01-19
**Test Query:** "Welche Nobelpreise wurden dieses Jahr vergeben?"
**Research Mode:** Web-Suche AusfÃ¼hrlich (3 Quellen)
**Sources Used:** ZDFheute, Blick.ch, Tagesschau (identical for all tests)

---

## ğŸ“Š Test Results Summary

| Model | Quantization | Temp | Inference Time | Accuracy | Hallucinations | Rating | Recommendation |
|-------|-------------|------|----------------|----------|----------------|--------|----------------|
| **deepseek-r1:8b-0528-qwen3** | Q8 | 0.8 | ~45s | âŒ Poor | âš ï¸âš ï¸âš ï¸ Severe | 2/10 | âŒ NICHT verwenden |
| **deepseek-r1:8b-0528-qwen3** | Q8 | 0.2 | ~45s | âŒ Poor | âš ï¸âš ï¸âš ï¸ Severe | 2/10 | âŒ NICHT verwenden |
| **gemma2:9b-instruct** | Q8 | 0.8 | ~40s | âš ï¸ Medium | âš ï¸ Medium | 6/10 | âš ï¸ Nur mit Temp 0.2 |
| **gemma2:9b-instruct** | Q8 | 0.2 | ~40s | âœ… Good | âœ… None | 8/10 | âœ… OK fÃ¼r schnelle Recherche |
| **qwen2.5:14b** | Q4 | 0.2 | **33s** | âœ… Excellent | âœ… None | **8.5/10** | â­ **EMPFOHLEN** (beste Balance) |
| **qwen2.5:14b-instruct** | Q8 | 0.8 | 84s | âš ï¸ Medium | âš ï¸ Category errors | 7/10 | âš ï¸ Zu langsam |
| **qwen2.5:14b-instruct** | Q8 | 0.2 | 62s | âœ… Perfect | âœ… None | **9.5/10** | âœ… BESTE QUALITÃ„T (aber langsam) |

---

## ğŸ”¬ Detailed Test Results

### 1. DeepSeek-R1:8b-0528-qwen3-q8_0

**Temperature 0.8:**
```
Inference: ~45s
VRAM: 8.9 GB (fits in 12GB)
```

**Hallucinations Found:**
- âŒ **Invented Names:** "Alice und Bob Johnson" (klassische Krypto-Placeholder-Namen!)
- âŒ **False Nobel Count:** "8 Nobelpreise" (es gibt nur 6 Kategorien)
- âŒ **False Death Story:** "starb durch Explosion" (Nobel starb friedlich)
- âŒ **Invented Date:** "7. Oktober 2025" (nicht in Quellen)
- âŒ **Category Confusion:** Physik/Medizin gemischt

**User Feedback:** *"Das ist kein besonders tolles Modell, ja."*

**Temperature 0.2:**
```
Inference: ~45s
Result: KEINE VERBESSERUNG - identische Halluzinationen
```

**Conclusion:**
â›” **NICHT GEEIGNET fÃ¼r faktische Recherche!** Temperature-Reduktion hat KEINEN Effekt auf Halluzinationen. Das Reasoning-Feature fÃ¼hrt zu Spekulation und Konfabulation.

---

### 2. Gemma2:9b-instruct-q8_0

**Temperature 0.8:**
```
Inference: ~40s
VRAM: 9.8 GB (fits in 12GB)
```

**Hallucinations Found:**
- âš ï¸ **WÃ¤hrung falsch:** "Ã¼ber 800'000 Franken" (Quelle sagt: "11 Millionen SEK" â‰ˆ "1 Million Euro")
- âœ… Keine erfundenen Namen
- âœ… Kategorien korrekt

**Temperature 0.2:**
```
Inference: ~40s
Result: DEUTLICH BESSER
```

**Improvements:**
- âœ… Keine WÃ¤hrungs-Halluzinationen mehr
- âœ… Konservativer in Aussagen
- âœ… Ehrlich bei fehlenden Details ("nicht spezifiziert")

**Rating:** 8/10 - Gute Faktentreue mit Temp 0.2

---

### 3. qwen2.5:14b (Q4 Quantization)

**Temperature 0.2:**
```
Inference: 33s âš¡ (SCHNELLSTES Modell!)
VRAM: ~8-9 GB (fits in 12GB)
```

**Performance:**
- âœ… **Keine Halluzinationen**
- âœ… Ehrlich bei fehlenden Namen: "Namen der PreistrÃ¤ger sind laut den Quellen nicht spezifiziert"
- âœ… Korrekte Fakten (11 Mio SEK, Standard-Nobelpreis-Zeitplan)
- âœ… Keine Kategorie-Verwirrung
- âœ… **SCHNELLSTE Inferenz** (33s!)

**Rating:** 8.5/10 - **BESTE SPEED/QUALITY BALANCE**

**Empfehlung:** â­ **DEFAULT MODEL** fÃ¼r AIfred Intelligence

---

### 4. qwen2.5:14b-instruct-q8_0

**Temperature 0.8:**
```
Inference: 84s (LANGSAMSTES Modell)
VRAM: 15 GB (OVERFLOW â†’ nutzt System-RAM)
```

**Issues:**
- âš ï¸ **Kategorie-Verwirrung:** Physik-Nobelpreis als "Physiologie oder Medizin" bezeichnet
- âš ï¸ **2x langsamer** als Q4 wegen RAM-Overflow
- âœ… Ansonsten korrekte Fakten

**Temperature 0.2:**
```
Inference: 62s (immer noch langsam)
Result: PERFEKTE QUALITÃ„T
```

**Improvements:**
- âœ… **Keine Kategorie-Fehler mehr**
- âœ… **Perfekte Faktentreue** (9.5/10)
- âœ… Ehrlich bei fehlenden Details
- âœ… Keine Halluzinationen
- âŒ **Aber:** 2x langsamer als Q4 (62s vs 33s)

**Rating:** 9.5/10 - Beste QualitÃ¤t, aber Geschwindigkeit leidet

---

## ğŸ” Key Findings

### 1. Temperature Impact

| Temperature | Effect | Use Case |
|------------|--------|----------|
| **0.0** | Deterministisch, identische Ausgaben | Debugging, Tests |
| **0.2** | â­ **Faktentreu, konservativ** | **RECHERCHE (empfohlen)** |
| **0.8** | Kreativ, variiert, risikoreich | Kreative Tasks (Gedichte, Brainstorming) |
| **1.5+** | Sehr kreativ, unpredictable | Experimentelles Schreiben |

**Wichtig:** Bei **DeepSeek-R1** hat Temperature KEINEN Effekt auf Halluzinationen!

### 2. Q8 vs Q4 Quantization

| Aspect | Q4 | Q8 |
|--------|----|----|
| **QualitÃ¤t** | Gut (8.5/10) | Exzellent (9.5/10) |
| **Geschwindigkeit** | âš¡ Schnell (33s) | ğŸ¢ Langsam (62s) |
| **VRAM** | ~8-9 GB | ~15 GB (Overflow!) |
| **RAM-Overflow** | âŒ Nein | âœ… Ja (nutzt System-RAM) |
| **Empfehlung** | â­ **Standard** | Nur fÃ¼r hÃ¶chste QualitÃ¤t |

**Conclusion:** Q4 mit Temp 0.2 ist **ausreichend** fÃ¼r faktische Recherche!

### 3. Model Recommendations

**FÃ¼r Web-Recherche (faktische Aufgaben):**
1. â­ **qwen2.5:14b** (Q4, Temp 0.2) - **BESTE WAHL**
2. âœ… gemma2:9b-instruct-q8_0 (Temp 0.2) - Schnelle Alternative
3. âš ï¸ qwen2.5:14b-instruct-q8_0 (Temp 0.2) - Nur wenn Zeit keine Rolle spielt
4. âŒ **deepseek-r1:8b** - **NICHT verwenden** (severe hallucinations)

**FÃ¼r kreative Aufgaben (Gedichte, Brainstorming):**
- Alle Modelle mit Temp 0.8-1.5 OK (auÃŸer DeepSeek-R1)

---

## ğŸš« Anti-Hallucination System Prompts

**Implementiert in:** `lib/agent_core.py` (Lines 498-504, 333-340)

```python
# ğŸš« ABSOLUTES VERBOT - NIEMALS ERFINDEN:
- âŒ KEINE Namen von Personen, PreistrÃ¤gern, Wissenschaftlern (auÃŸer explizit in Quellen genannt!)
- âŒ KEINE Daten, Termine, Jahreszahlen (auÃŸer explizit in Quellen genannt!)
- âŒ KEINE Entdeckungen, Erfindungen, wissenschaftliche Details (auÃŸer explizit beschrieben!)
- âŒ KEINE Zahlen, Statistiken, Messungen (auÃŸer explizit in Quellen!)
- âŒ KEINE Zitate oder wÃ¶rtliche Rede (auÃŸer explizit zitiert!)
- âš ï¸ BEI UNSICHERHEIT: "Laut den Quellen ist [Detail] nicht spezifiziert"
- âŒ NIEMALS aus Kontext "raten" oder "folgern" was gemeint sein kÃ¶nnte!
```

**EffektivitÃ¤t:**
- âœ… **Funktioniert** bei Qwen2.5, Gemma2
- âŒ **Funktioniert NICHT** bei DeepSeek-R1 (Reasoning-Modell spekuliert trotzdem)

---

## ğŸ“ˆ Performance Comparison Chart

```
Inference Time (seconds):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

qwen2.5:14b Q4 (Temp 0.2)          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 33s âš¡ FASTEST
gemma2:9b Q8 (Temp 0.2)             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 40s
deepseek-r1:8b Q8 (any temp)        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 45s
qwen2.5:14b-instruct Q8 (Temp 0.2)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 62s
qwen2.5:14b-instruct Q8 (Temp 0.8)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 84s ğŸ¢ SLOWEST

Factual Accuracy (Rating):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

qwen2.5:14b-instruct Q8 (Temp 0.2)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 9.5/10 â­ BEST
qwen2.5:14b Q4 (Temp 0.2)           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8.5/10 â­ EMPFOHLEN
gemma2:9b Q8 (Temp 0.2)             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8/10
qwen2.5:14b-instruct Q8 (Temp 0.8)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 7/10
gemma2:9b Q8 (Temp 0.8)             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 6/10
deepseek-r1:8b Q8 (any temp)        â–ˆâ–ˆâ–ˆâ–ˆ 2/10 âŒ WORST
```

---

## ğŸ¯ Final Configuration Recommendation

**Default Settings (implemented in `lib/config.py`):**
```python
DEFAULT_SETTINGS = {
    "model": "qwen2.5:14b",      # Q4 quantization
    "temperature": 0.2,          # Factual accuracy
    # ...
}
```

**Reasoning:**
- âš¡ **Schnellste Inferenz** (33s) unter allen getesteten Modellen
- âœ… **Hohe Faktentreue** (8.5/10) ohne Halluzinationen
- ğŸ’¾ **Passt ins VRAM** (kein RAM-Overflow)
- ğŸ¯ **Beste Speed/Quality Balance** fÃ¼r Recherche-Tasks

---

## ğŸ”® Future Work: Adaptive Temperature

**Planned Feature:** Automatische Temperature-Anpassung basierend auf User-Intent

**Hybrid Approach:**
1. **Keyword-basierte Erkennung** (schnell):
   - "Wetter", "News", "aktuell", "Nobelpreis" â†’ Temp 0.2 (Recherche)
   - "Gedicht", "kreativ", "Geschichte", "erfinde" â†’ Temp 0.8+ (Kreativ)

2. **LLM-basierte Intent Detection** (bei Unsicherheit):
   - Fallback wenn Keywords nicht eindeutig
   - qwen3:1.7b klassifiziert Intent (~2-3s overhead)

3. **Default fÃ¼r Research Mode:**
   - Web-Suche Schnell/AusfÃ¼hrlich â†’ Temp 0.2 (fest)
   - Eigenes Wissen â†’ Adaptive (Keyword + LLM)

**Implementation Status:** ğŸ“‹ TODO (siehe TODO.md)

---

**Last Updated:** 2025-01-19 03:00 CET
**Tested By:** User (mp)
**Test Duration:** ~2 hours (extensive cross-model comparison)
