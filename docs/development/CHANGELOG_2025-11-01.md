# Changelog - 2025-11-01

## UI Improvements & Bug Fixes

### 1. AI-Antwortfenster & Eingabefenster Clearing âœ…

**Problem:**
- Fenster wurden nicht gelÃ¶scht nach Ãœbertragung in Chat-History
- Oder zeigten History als Fallback (verwirrendes Verhalten)

**LÃ¶sung:**
- Beide Fenster zeigen nur noch ihre jeweiligen State-Variablen (kein History-Fallback)
- Clearing beim "result" Event mit expliziten yields
- `is_generating` wird sofort auf False gesetzt um UI-Flackern zu vermeiden

**Dateien geÃ¤ndert:**
- `aifred/aifred.py` - UI-Komponenten vereinfacht
- `aifred/state.py` - Clearing-Logik beim result-Event

**EinschrÃ¤nkung:**
Fenster werden nach Cache-Metadata-Generierung gelÃ¶scht (2-3s VerzÃ¶gerung) aufgrund Reflex Framework Limitation (Event-Handler buffert Updates).

---

### 2. Temperature-Labels (faktisch/gemischt/kreativ) âœ…

**Problem:**
- Debug-Ausgabe zeigte nur Zahlen: `ğŸŒ¡ï¸ Temperature: 0.2 (auto)`
- Schwer zu verstehen welcher Intent erkannt wurde

**LÃ¶sung:**
- Neue Funktion `get_temperature_label()` in `intent_detector.py`
- Labels in allen Modi: "faktisch", "gemischt", "kreativ"
- Ausgabe jetzt: `ğŸŒ¡ï¸ Temperature: 0.2 (auto, faktisch)`

**Dateien geÃ¤ndert:**
- `aifred/lib/intent_detector.py` - get_temperature_label() Funktion
- `aifred/lib/conversation_handler.py` - Labels fÃ¼r "Eigenes Wissen"
- `aifred/lib/research/cache_handler.py` - Labels fÃ¼r Cache-Hits
- `aifred/lib/research/context_builder.py` - Labels fÃ¼r RAG

---

### 3. RAG Intent-Detection âœ…

**Problem:**
- RAG-Recherche verwendete hardcoded Temperature 0.7 (spÃ¤ter 0.5)
- Wetterfragen (faktisch) liefen mit 0.5 statt 0.2
- Keine Unterscheidung zwischen faktischen/kreativen Recherchen

**LÃ¶sung:**
- RAG nutzt jetzt KI-basierte Intent-Detection (wie "Eigenes Wissen" Modus)
- Automatik-LLM analysiert User-Frage und wÃ¤hlt passende Temperature:
  - FAKTISCH â†’ 0.2 (Wetter, News, Fakten)
  - KREATIV â†’ 0.8 (Gedichte, Geschichten)
  - GEMISCHT â†’ 0.5 (Kombination)

**Dateien geÃ¤ndert:**
- `aifred/lib/research/context_builder.py` - Intent-Detection Integration

**Beispiel:**
```
User: "Wie wird das Wetter morgen?"
â†’ Intent: FAKTISCH â†’ Temperature: 0.2 âœ…

User: "Schreibe ein Gedicht Ã¼ber Regen"
â†’ Intent: KREATIV â†’ Temperature: 0.8 âœ…
```

---

### 4. Cache-Initialisierung Fix âœ…

**Problem:**
- Cache wurde nicht initialisiert bei Hot-Reload
- Fehler: "âš ï¸ DEBUG Cache-Speicherung fehlgeschlagen: Cache nicht initialisiert"

**LÃ¶sung:**
- Cache wird direkt beim Module-Import initialisiert
- `_research_cache` und `_research_cache_lock` nicht mehr `None`
- `on_load()` setzt Cache immer (auch bei bestehender Session)

**Dateien geÃ¤ndert:**
- `aifred/lib/cache_manager.py` - Cache bei Import initialisieren
- `aifred/state.py` - Cache immer in on_load() setzen

---

### 5. Debug-Messages bereinigt âœ…

**Problem:**
- Doppelte Messages: "ğŸ”§ Cache-Metadata wird generiert..." + "ğŸ“ Starte Cache-Metadata Generierung..."

**LÃ¶sung:**
- Erste Message entfernt
- Nur noch eine klare Message bleibt

**Dateien geÃ¤ndert:**
- `aifred/lib/research/context_builder.py` - Redundante Message entfernt

---

## Commits

```
80e0a63 - Fix: UI clearing timing + Temperature labels + RAG intent detection
7f2e092 - Fix: Add explicit yields after clearing to force immediate UI update
897d8bd - Fix: Set is_generating=False immediately after result to prevent UI flicker
397c546 - Fix: Add 100ms delay after clearing to ensure UI renders
b8cb3ba - Fix: Eingabefenster zeigt nur current_user_message (kein History-Fallback)
6d374e1 - Fix: Remove loop break + Remove duplicate cache metadata message
```

---

## Technische Details

### Intent-Detection Flow

1. **User stellt Frage** (z.B. "Wie wird das Wetter morgen?")
2. **Automatik-Mode**: KI entscheidet ob Web-Recherche nÃ¶tig
3. **Falls JA â†’ RAG-Recherche:**
   - Web-Suche + Scraping
   - **Intent-Detection lÃ¤uft** mit Automatik-LLM (qwen2.5:3b)
   - Prompt aus `prompts/intent_detection.txt`
   - LLM antwortet: "FAKTISCH" / "KREATIV" / "GEMISCHT"
   - Temperature wird entsprechend gesetzt (0.2 / 0.8 / 0.5)
4. **Haupt-LLM generiert Antwort** mit adaptiver Temperature

### Cache-Metadata Generation

- LÃ¤uft NACH Haupt-Antwort (nicht blockierend fÃ¼r User)
- Verwendet Automatik-LLM (qwen2.5:3b)
- Erstellt kurze Zusammenfassung (max 60 WÃ¶rter)
- Wird fÃ¼r spÃ¤tere Context-Integration genutzt
- Temperature: 0.1 (sehr konsistent)
- num_ctx: min(2048, haupt_llm_limit // 2)

---

## Breaking Changes

Keine Breaking Changes - alle Ã„nderungen sind abwÃ¤rtskompatibel.

---

## Known Issues

1. **UI-Clearing VerzÃ¶gerung:**
   - Fenster werden nach Cache-Metadata gelÃ¶scht (2-3s VerzÃ¶gerung)
   - Grund: Reflex Framework buffert State-Updates
   - Workaround: Akzeptieren oder separate Background-Task (komplex)

---

## Testing Notes

Getestet mit:
- Reflex 0.8.17
- Python 3.12
- Ollama Backend
- qwen3:8b (Haupt-LLM)
- qwen2.5:3b (Automatik-LLM)

Test-Szenarien:
- âœ… Wetter-Frage (faktisch â†’ 0.2)
- âœ… "Hallo bist du da" (kein Research, eigenes Wissen)
- âœ… Cache-Metadata Generierung
- âœ… Hot-Reload (Cache bleibt initialisiert)
- âœ… Debug-Messages korrekt

---

## Future Improvements

1. **UI-Clearing sofort nach Result** (erfordert Reflex Framework Ã„nderung oder Background-Task)
2. **Cache-Metadata Fortschrittsanzeige** (optional, wenn User es sehen will)
3. **Intent-Detection Caching** (wenn gleiche Frage mehrfach gestellt wird)

---

**Erstellt:** 2025-11-01
**Autor:** Claude (AI Assistant)
**Commits:** 80e0a63 bis 6d374e1
