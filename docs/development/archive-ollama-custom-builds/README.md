# Ollama GPU-Beschleunigung Dokumentation

**AMD Radeon 780M (gfx1103) mit ROCm 6.3 auf Ubuntu 22.04**

## Status: âœ… Erfolgreich implementiert

GPU-Beschleunigung fÃ¼r Ollama wurde erfolgreich auf einem System mit AMD Radeon 780M integrierter Grafik implementiert und getestet.

## SchnellÃ¼bersicht

- **GPU:** AMD Radeon 780M (gfx1103, RDNA 3 iGPU)
- **VRAM:** 15.3 GiB (shared mit System-RAM)
- **ROCm:** Version 6.3.0
- **Ollama:** v0.12.5 (aus Docker extrahiert)
- **Performance:** 3x schneller bei kleinen Modellen, ~1.1x bei mittleren

## Dokumentations-Struktur

### 1. [OLLAMA-GPU-ACCELERATION.md](OLLAMA-GPU-ACCELERATION.md)
**Haupt-Dokumentation**

EnthÃ¤lt:
- âœ… Zusammenfassung der erreichten Ergebnisse
- ğŸ“Š Performance-Benchmarks (CPU vs. GPU)
- ğŸ—ï¸ Architektur-Ãœbersicht
- ğŸ”‘ Kritische Erfolgsfaktoren
- ğŸ“š Lessons Learned
- ğŸ¯ Best Practices

**Lies dies zuerst** fÃ¼r einen vollstÃ¤ndigen Ãœberblick Ã¼ber das Projekt.

### 2. [OLLAMA-GPU-INSTALLATION.md](OLLAMA-GPU-INSTALLATION.md)
**Schritt-fÃ¼r-Schritt Installationsanleitung**

EnthÃ¤lt:
- ğŸ“‹ Voraussetzungen und System-Checks
- ğŸ”§ ROCm 6.3 Installation
- ğŸ“¦ Ollama Binary-Extraktion aus Docker
- ğŸ“š Bibliotheken-Extraktion (2.3 GB)
- âš™ï¸ Systemd Service-Konfiguration
- âœ”ï¸ Verifikation und Tests
- ğŸ“ˆ Performance-Benchmarking

**Folge dieser Anleitung** fÃ¼r eine Neuinstallation.

### 3. [OLLAMA-GPU-TROUBLESHOOTING.md](OLLAMA-GPU-TROUBLESHOOTING.md)
**Umfassender Troubleshooting-Guide**

EnthÃ¤lt:
- ğŸ› Alle aufgetretenen Fehler mit LÃ¶sungen
- ğŸ” Debugging-Werkzeuge und -Techniken
- âš ï¸ HÃ¤ufige Probleme und Fixes
- ğŸ“ Checkliste fÃ¼r Fehlersuche
- ğŸ’¡ Erweiterte Diagnose-Methoden

**Konsultiere diesen Guide** bei Problemen oder Fehlern.

## Wichtigste Erkenntnisse

### Der entscheidende Durchbruch

Das Kernproblem war **Library-InkompatibilitÃ¤t**, nicht Hardware oder Code:

```
âŒ Custom-Built Libraries â†’ 0 VRAM erkannt
âœ… Docker-Image Libraries â†’ 15.3 GiB erkannt
```

**LÃ¶sung:** Komplette Bibliotheksumgebung aus `ollama/ollama:rocm` Docker-Image extrahieren.

### Kritische Konfiguration

```bash
# /etc/systemd/system/ollama.service
Environment="HSA_OVERRIDE_GFX_VERSION=11.0.0"  # KRITISCH!

# NICHT 11.0.2 oder 11.0.3!
```

### Warum Docker-Bibliotheken?

1. **Getestet und kompatibel** - Exakte Version-Matches
2. **VollstÃ¤ndig** - Alle Dependencies (2.3 GB)
3. **Konfliktfrei** - UnabhÃ¤ngig von System-Paketen
4. **Funktioniert garantiert** - Offizielle Ollama-Builds

## Performance-Ergebnisse

| Modell | GrÃ¶ÃŸe | CPU | GPU | Speedup |
|--------|-------|-----|-----|---------|
| qwen2.5:0.5b | 397 MB | 1.18s | 0.38s | **3.1x** âœ… |
| qwen3:8b | 5.2 GB | 23.0s | 20.7s | **1.1x** âš ï¸ |

**Interpretation:**
- Kleine Modelle (< 1B): Starker GPU-Vorteil
- Mittlere Modelle (7-8B): Geringer Vorteil (iGPU shared memory Bottleneck)
- GroÃŸe Modelle (> 30B): Noch nicht getestet

## Quick-Start fÃ¼r Verifizierung

```bash
# 1. Service-Status
systemctl status ollama

# 2. GPU-Erkennung
journalctl -u ollama -n 50 --no-pager | grep "15.3 GiB"
# Erwartung: available="15.3 GiB" âœ…

# 3. Modell-Test
ollama run qwen2.5:0.5b "test"

# 4. Layer-Offloading
journalctl -u ollama -n 100 --no-pager | grep offload
# Erwartung: offloaded 25/25 layers to GPU âœ…
```

## Wichtige Dateien auf diesem System

| Datei/Verzeichnis | Zweck | GrÃ¶ÃŸe |
|-------------------|-------|-------|
| `/usr/local/bin/ollama` | Ollama v0.12.5 Binary | 33 MB |
| `/usr/local/lib/ollama/rocm/` | ROCm 6.3 Bibliotheken | 2.3 GB |
| `/etc/systemd/system/ollama.service` | Service-Konfiguration | 1 KB |
| `/opt/rocm-6.3.0/` | ROCm 6.3 Installation | ~21 GB |

## Anwendungen, die GPU nutzen

### AIfred-Intelligence
- **Pfad:** `~/Projekte/AIfred-Intelligence/`
- **Modelle:** qwen3:1.7b (Haupt), qwen3:4b (Automatik)
- **Status:** LÃ¤uft mit GPU-Beschleunigung âœ…

Weitere Projekte kÃ¶nnen Ollama nutzen - alle profitieren automatisch von GPU.

## Wartung

### Logs Ã¼berprÃ¼fen
```bash
journalctl -u ollama -f  # Live-Logs
journalctl -u ollama -n 100 --no-pager  # Letzte 100 Zeilen
```

### Service neu starten
```bash
sudo systemctl restart ollama
```

### GPU-Monitoring
```bash
export HSA_OVERRIDE_GFX_VERSION=11.0.0
watch -n 1 '/opt/rocm-6.3.0/bin/rocm-smi'
```

### GPU deaktivieren (CPU-Only Modus)
```bash
# Einfachste Methode: Environment-Variable
OLLAMA_NUM_GPU=0 ollama run qwen3:8b "test"

# FÃ¼r Benchmarks:
OLLAMA_NUM_GPU=0 time ollama run qwen3:8b "sage hallo"  # CPU
time ollama run qwen3:8b "sage hallo"                    # GPU
```

Siehe [OLLAMA-GPU-INSTALLATION.md](OLLAMA-GPU-INSTALLATION.md#cpu-only-modus-gpu-deaktivieren) fÃ¼r weitere Methoden.

## Backup-Strategie

### Was sichern?
1. âœ… Binary: `/usr/local/bin/ollama`
2. âœ… Service: `/etc/systemd/system/ollama.service`
3. ğŸ“ Dokumentation: Diese Anleitungen

### Was NICHT sichern?
- âŒ Bibliotheken (2.3 GB) - Bei Bedarf aus Docker neu extrahieren
- âŒ Modelle - KÃ¶nnen jederzeit neu heruntergeladen werden

## WeiterfÃ¼hrende Ressourcen

- **Ollama Dokumentation:** https://github.com/ollama/ollama
- **ROCm Dokumentation:** https://rocm.docs.amd.com/
- **Docker Hub:** https://hub.docker.com/r/ollama/ollama

## Timeline der Entwicklung

1. **Tag 1-2:** ROCm 6.2.4 Installation, custom Builds â†’ 0 VRAM
2. **Tag 3:** Upgrade auf ROCm 6.3, v0.12.6-rc0 Test â†’ Immer noch 0 VRAM
3. **Tag 4:** Docker-Test â†’ **Durchbruch!** GPU funktioniert in Docker
4. **Tag 5:** Library-Extraktion â†’ âœ… **Erfolg!** 15.3 GiB VRAM
5. **Tag 6:** Benchmarking und Dokumentation

## Autoren und Danksagung

**Entwickelt:** 11.-16. Oktober 2025
**Dokumentiert:** 16. Oktober 2025

Besonderer Dank an:
- Ollama Team fÃ¼r Docker-Images mit funktionierenden Bibliotheken
- AMD ROCm Team fÃ¼r GPU-Support
- Community fÃ¼r Hinweise auf iGPU-Probleme

## Support

Bei Fragen oder Problemen:
1. ğŸ“– Konsultiere [TROUBLESHOOTING.md](OLLAMA-GPU-TROUBLESHOOTING.md)
2. ğŸ” ÃœberprÃ¼fe Logs: `journalctl -u ollama -n 200`
3. âœ… Nutze Checkliste im Troubleshooting-Guide

---

**Status:** Produktionsbereit âœ…
**Letzte Aktualisierung:** 16. Oktober 2025
**Version:** 1.0
