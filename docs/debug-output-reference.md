# Debug-Ausgabe Referenz

## Original Gradio-Version - Muster fÃ¼r gewÃ¼nschte Informationen

Dies ist die Debug-Ausgabe aus der ursprÃ¼nglichen Gradio-Version, die als Referenz dient fÃ¼r die Informationen, die auch in der Reflex-Version vorhanden sein sollen.

```
13:36:35 | ğŸ“¨ User Request empfangen
13:36:35 | âš¡ Explizite Recherche erkannt â†’ Web-Suche startet
13:36:35 | ğŸ”§ Query-Optimierung startet
13:36:41 | âš¡ 31 t/s
13:36:43 | ğŸŒ Web-Suche: SearXNG, Tavily AI, Brave Search (3 APIs)
13:36:43 | ğŸ”„ Deduplizierung: 29 URLs â†’ 24 unique (5 Duplikate)
13:36:43 | âš–ï¸ KI bewertet URLs mit: qwen2.5:3b
13:36:50 | âš¡ 23 t/s
13:36:50 | ğŸŒ Web-Scraping startet (parallel)
13:36:52 | âœ… Web-Scraping fertig: 1 URLs erfolgreich
13:36:52 | ğŸ§© 1 Quellen mit Inhalt gefunden
13:36:52 | ğŸ“ Systemprompt wird erstellt
13:36:52 | ğŸ“Š Systemprompt: 370217 Zeichen
13:36:52 | ğŸ“Š Messages: 2, Gesamt: 370291 Zeichen (~92572 Tokens)
13:36:52 | âš ï¸ WARNUNG: Kontext Ã¼berschritten! (92572 Tokens > 40960 Tokens Limit)
13:36:52 | âš ï¸ Ã„ltere Messages werden abgeschnitten!
13:36:52 | ğŸªŸ Context Window: 40960 Tokens (auto)
13:36:52 | ğŸŒ¡ï¸ Temperature: 0.2 (auto, faktisch)
13:36:52 | ğŸ¤– Haupt-LLM startet: qwen3:8b (mit 1 Quellen)
13:39:12 | âš¡ 12 t/s
13:39:12 | âœ… Haupt-LLM fertig (140.5s, 3626 Zeichen, Agent-Total: 157.4s)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
13:39:18 | ğŸ“ Erstelle Cache-Zusammenfassung...
13:39:34 | âš¡ 13 t/s
13:39:34 | âœ… Zusammenfassung erstellt
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

## Wichtige Informationen aus dieser Ausgabe:

### 1. Prozess-Flow mit Timing
- User Request empfangen
- Explizite Recherche erkannt
- Query-Optimierung startet
- Tokens/s fÃ¼r Query-Opt (31 t/s)

### 2. Web-Suche Details
- Verwendete APIs (SearXNG, Tavily AI, Brave Search)
- Deduplizierung: Total URLs â†’ Unique URLs (Duplikate)
- URL-Bewertung mit Model

### 3. Scraping-Fortschritt
- Web-Scraping startet
- Web-Scraping fertig: X URLs erfolgreich
- X Quellen mit Inhalt gefunden

### 4. Context-Management
- Systemprompt: X Zeichen
- Messages: X, Gesamt: X Zeichen (~X Tokens)
- Kontext-Warnungen wenn Ã¼berschritten
- Context Window: X Tokens (auto/manual)
- Temperature: X (auto/manual, Intent)

### 5. LLM-Execution
- Haupt-LLM startet: Model (mit X Quellen)
- Tokens/s wÃ¤hrend Generation
- Haupt-LLM fertig (Zeit, Zeichen, Agent-Total)

### 6. Cache-Operationen
- Cache-Zusammenfassung erstellen
- Zusammenfassung erstellt mit Tokens/s

## Aktuelle Reflex-Version

Die aktuelle Reflex-Version hat bereits viele dieser Informationen im Debug-Log.
Einige kÃ¶nnten noch hinzugefÃ¼gt/verbessert werden:

### âœ… Bereits vorhanden:
- User Request empfangen
- Explizite/Automatik-Entscheidung
- Web-Suche mit API-Namen
- URL-Deduplizierung
- URL-Bewertung
- Scraping-Status
- Context-GrÃ¶ÃŸe
- Context Window & Temperature
- LLM Start/Fertig mit Tokens/s
- Cache-Operationen

### âš ï¸ KÃ¶nnte verbessert werden:
- Tokens/s fÃ¼r Automatik-LLM Calls (Query-Opt, URL-Rating, Decision)
- Klarere Separator-Linien zwischen Phasen
- Konsistentere Emoji-Verwendung
- Timing-Informationen fÃ¼r einzelne Schritte

## Notizen:

- Die Separator-Linien (â”€) helfen, verschiedene Phasen visuell zu trennen
- Tokens/s nach jeder LLM-Operation zeigt Performance
- Die Warnung bei Context-Ãœberschreitung ist wichtig fÃ¼r Debugging
- Agent-Total Zeit zeigt Gesamt-Dauer der Operation
