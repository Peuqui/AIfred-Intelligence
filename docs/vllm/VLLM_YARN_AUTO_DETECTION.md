# vLLM YaRN & Auto-Detection

**Datum:** 2025-11-13
**Status:** âœ… Implementiert & Getestet

---

## ğŸ¯ Features

### 1. YaRN Context Extension (RoPE Scaling)

**Was ist YaRN?**
- **Y**et **a**nother **R**oPE e**N**largement
- Erweitert den Context durch RoPE (Rotary Position Embedding) Skalierung
- ErmÃ¶glicht lÃ¤ngere Kontext-Fenster als das Model nativ unterstÃ¼tzt

**Einstellungen in UI:**
- Toggle: `enable_yarn` (Ein/Aus)
- Faktor: `yarn_factor` (1.0 - 8.0, Schritte: 0.5)
- Live-Preview: Zeigt geschÃ¤tzte Tokens (`vllm_max_tokens * yarn_factor`)

**Beispiele:**
```
Basis: 26,624 tokens (RTX 3060 Hardware-Limit)
YaRN 1.5x: ~40,000 tokens (nativ)
YaRN 2.0x: ~53,000 tokens
YaRN 4.0x: ~106,000 tokens (benÃ¶tigt mehr VRAM!)
```

**Wichtig:**
- âš ï¸ YaRN-Faktor > 2.0 kann VRAM Ã¼berschreiten â†’ Crash-Risiko
- ğŸ”„ BenÃ¶tigt vLLM Backend-Neustart nach Ã„nderung
- ğŸ’¾ Doppelter VRAM-Verbrauch proportional zum Faktor

---

### 2. Automatische Context-Erkennung

**Problem gelÃ¶st:**
- Jede GPU hat unterschiedliche VRAM-Limits
- Hardcoded Werte (z.B. 26.608 fÃ¼r RTX 3060) funktionieren nicht auf anderen GPUs
- User wissen nicht, was ihr Hardware-Limit ist

**LÃ¶sung: 2-Stufen Auto-Detection**

#### Stufe 1: Native Context Versuch
```
ğŸ“Š Native Context: 40,960 tokens (from config.json)
ğŸ”§ Auto-Detection: Trying native context (40,960 tokens)...
```

vLLM versucht mit nativem Model-Context zu starten (z.B. 40K fÃ¼r Qwen3-8B-AWQ).

#### Stufe 2: Hardware-Limit Erkennung
```
âš ï¸ Native context too large, detecting hardware limit...
ğŸ“Š Hardware Limit detected: 26,624 tokens (VRAM-constrained)
ğŸ”„ Restarting with hardware limit...
âœ… vLLM started successfully with 26,624 tokens
```

Falls VRAM nicht ausreicht:
1. Parse Error-Message: `"estimated maximum model length is 26624"`
2. Extrahiere Hardware-Limit via Regex
3. Stoppe crashed Process
4. Restart mit erkanntem Limit

**Regex Pattern:**
```python
r"(?:estimated )?maximum model length is (\d+)"
```

Matched beide Formate:
- `"Maximum model length is 26608 for this GPU"`
- `"the estimated maximum model length is 26624"`

---

## ğŸ’¾ Settings Persistence

**Settings-Datei:** `~/.config/aifred/settings.json`

**Gespeicherte Werte:**
```json
{
  "enable_yarn": false,
  "yarn_factor": 1.0,
  "vllm_max_tokens": 26624,      // Auto-detected (0 = noch nicht erkannt)
  "vllm_native_context": 40960    // From config.json
}
```

**Verhalten:**

| Szenario | vllm_max_tokens | Verhalten |
|----------|-----------------|-----------|
| **First Run** | 0 (Default) | Auto-Detection â†’ Speichert erkannten Wert |
| **Second Run** | 26624 (gespeichert) | Direkt-Start mit bekanntem Limit (kein Crash!) |
| **GPU-Wechsel** | Alte GPU-Werte | User muss Settings lÃ¶schen oder neu erkennen |

**Debug-Log bei gespeichertem Wert:**
```
ğŸ“‹ Using saved context limit: 26,624 tokens (aus Settings)
âœ… vLLM started successfully with 26,624 tokens (~40s statt ~70s)
```

---

## ğŸ”§ Technische Details

### Code-Struktur

**1. vLLM Manager** ([aifred/lib/vllm_manager.py](../../aifred/lib/vllm_manager.py))
- `get_model_native_context()`: Liest config.json aus HuggingFace Cache
- `start_with_auto_detection()`: 2-Stufen Auto-Detection Logic
- `_read_stderr()`: Background-Thread fÃ¼r Error-Capture
- Regex-Parsing fÃ¼r Hardware-Limit Extraktion

**2. State Management** ([aifred/state.py](../../aifred/state.py))
- `vllm_max_tokens`: Hardware-constrained Context (Default: 0)
- `vllm_native_context`: Model-native Context (Default: 0)
- `enable_yarn`: YaRN Toggle
- `yarn_factor`: RoPE Scaling Factor
- `_save_settings()`: Speichert alle 4 Werte
- `on_load()`: LÃ¤dt Werte aus Settings beim Start

**3. UI** ([aifred/aifred.py](../../aifred/aifred.py))
- YaRN Toggle-Switch
- Numeric Input (1.0-8.0, step 0.5)
- Live Token Preview: `(~{vllm_max_tokens * yarn_factor} tokens)`
- Warning-Box bei `yarn_factor > 2.0`
- Info-Text: "Modell: 40K nativ | HW-Limit: 26K"

### Settings Files

**Location:** `~/.config/aifred/settings.json`
**Not in Git:** âœ… In `.gitignore` (`settings.json`, `**/settings.json`)

**Default Values** ([aifred/lib/settings.py](../../aifred/lib/settings.py)):
```python
{
    "enable_yarn": False,
    "yarn_factor": 1.0,
    "vllm_max_tokens": 0,       # 0 = auto-detect
    "vllm_native_context": 0    # 0 = auto-detect
}
```

---

## ğŸ“Š Performance

### Auto-Detection Timings (RTX 3060)

| Durchlauf | Native (40K) | Crash + Parse | Hardware (26K) | Total |
|-----------|--------------|---------------|----------------|-------|
| **First Start** | ~28s | ~2s | ~40s | **~70s** |
| **Second Start** | - | - | ~40s | **~40s** (gespeichert!) |

**Einsparung:** 30 Sekunden (43% schneller) bei jedem weiteren Start!

### YaRN Memory Usage

| YaRN Factor | Context | VRAM (geschÃ¤tzt) | RTX 3060 (12GB) |
|-------------|---------|------------------|-----------------|
| 1.0x | 26,624 | ~12 GB | âœ… Optimal |
| 1.5x | ~40,000 | ~18 GB | âŒ Zu viel |
| 2.0x | ~53,000 | ~24 GB | âŒ Zu viel |

**Empfehlung fÃ¼r RTX 3060:** Maximal 1.0x (kein YaRN), da bereits am VRAM-Limit.

---

## ğŸ› Bekannte Bugs (Fixed)

### âœ… Settings wurden nicht gespeichert
**Problem:** `vllm_max_tokens` wurde erkannt, aber nicht in Settings-Datei geschrieben.
**Fix:** `_save_settings()` & `on_load()` erweitert um YaRN/Context-Werte.

### âœ… Backend-Switch Error
**Problem:** `'LLMClient' object has no attribute 'backend'`
**Fix:** `llm_client.backend` â†’ `llm_client._get_backend()`

### âœ… Regex Pattern Mismatch
**Problem:** vLLM Error-Format war anders als erwartet.
**Fix:** Regex von `"Maximum model length is (\d+)"` zu `"(?:estimated )?maximum model length is (\d+)"`

### âœ… Stderr nicht erfasst
**Problem:** `communicate()` funktioniert nicht auf toten Prozess.
**Fix:** Background-Thread `_read_stderr()` liest kontinuierlich in Buffer.

### âœ… Second Start hing
**Problem:** Crashed Process nicht gestoppt vor Retry.
**Fix:** Explizites `await self.stop()` vor zweitem Start-Versuch.

---

## ğŸ¯ Future Ideas

### Progressive Debug-Output
**Problem:** Debug-Console zeigt alles erst am Ende (buffert wÃ¤hrend vLLM-Start).
**LÃ¶sung:** `_start_vllm_server()` als Generator mit `yield` nach jedem Log-Eintrag.
**Status:** â³ Geplant (komplexer Umbau, erstmal zurÃ¼ckgestellt)

### GPU-Wechsel Detection
**Problem:** Alte Settings von RTX 3060 funktionieren nicht auf RTX 4090.
**LÃ¶sung:** GPU-ID in Settings speichern, bei Wechsel Auto-Detection neu triggern.
**Status:** ğŸ’¡ Idee

### YaRN-Test Button
**Problem:** User weiÃŸ nicht, ob YaRN-Faktor zu hoch ist ohne Crash.
**LÃ¶sung:** "Test"-Button, der vLLM temporÃ¤r mit YaRN startet und Erfolg meldet.
**Status:** ğŸ’¡ Idee

---

## ğŸ“ Changelog

### 2025-11-13
- âœ… YaRN UI implementiert (Toggle, Factor, Live-Preview, Warning)
- âœ… Auto-Detection 2-Stufen-System (Native â†’ Hardware-Limit)
- âœ… Settings Persistence fÃ¼r alle YaRN/Context-Werte
- âœ… `.gitignore` erweitert (`settings.json`, `**/settings.json`)
- âœ… Backend-Switch Error gefixt (`llm_client._get_backend()`)
- âœ… Regex Pattern erweitert (beide vLLM Error-Formate)
- âœ… Stderr Capture via Background-Thread
- âœ… Process Cleanup vor Retry

---

**Autor:** AIfred Intelligence Team
**Maintainer:** AI Assistant + mp
**Related Docs:**
- [VLLM_RTX3060_CONFIG.md](VLLM_RTX3060_CONFIG.md) - GPU-spezifische Optimierung
- [VLLM_FIX_SUMMARY.md](VLLM_FIX_SUMMARY.md) - Crash-Fix Zusammenfassung
