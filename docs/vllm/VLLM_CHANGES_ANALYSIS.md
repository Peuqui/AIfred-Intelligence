# vLLM Changes Analysis - Debugging Crashes

**Ziel**: Systematisch zur√ºckgehen zu einer funktionierenden vLLM-Konfiguration

## Relevante Commits (seit November 2025)

### üî¥ 4bb64ca - fix: vLLM quantization auto-detection for better GPU compatibility
**Date**: 2025-11-12 (HEUTE)
**Ge√§ndert**:
- **ENTFERNT**: `detect_quantization()` Funktion
- **ENTFERNT**: `--quantization awq_marlin` Flag
- **Begr√ºndung**: vLLM auto-detects AWQ from config.json, expliziter Flag verursacht config mismatch

**Wichtigste √Ñnderung**:
```python
# VORHER (commit 3732393):
quant = detect_quantization(model)
if quant:
    cmd.extend(["--quantization", quant])

# NACHHER (commit 4bb64ca):
# Note: We don't add --quantization for AWQ models because:
# 1. AWQ models have quantization in their config.json
# 2. vLLM auto-detects AWQ and uses Marlin kernel on Ampere+ GPUs
# 3. Specifying --quantization awq_marlin causes a config mismatch error
logger.info(f"‚úÖ vLLM will auto-detect quantization from model config")
```

**Status**: ‚ö†Ô∏è Diese √Ñnderung ist aktuell AKTIV (HEAD)

---

### üü° 3732393 - feat: 8x Performance boost + Context auto-detection + Real tokenizer
**Date**: 2025-11-11
**Ge√§ndert**:
- **HINZUGEF√úGT**: `detect_quantization()` Funktion
- **HINZUGEF√úGT**: Automatisches Setzen von `--quantization` Flag
- **Logik**: Model-Name parsen ‚Üí awq/gptq/gguf erkennen ‚Üí Flag setzen

**vLLM Config in state.py (commit 3732393)**:
```python
self._vllm_manager = vLLMProcessManager(
    port=8001,
    max_model_len=16384,  # ‚ö†Ô∏è 16K Context (zu wenig!)
    gpu_memory_utilization=0.85
)
```

**Status**: ‚úÖ Dieser Commit war die LETZTE bekannte funktionierende Version

---

### üü¢ aa0ffad - feat: Upgrade vLLM models to Qwen2.5-Instruct-AWQ (128K context)
**Date**: 2025-11-09
**Ge√§ndert**: Nur Model-Downloads, KEINE vLLM-Code-√Ñnderungen

---

### üü¢ 2fa2365 - feat: Add Qwen3 AWQ models for vLLM (14B + 32B)
**Date**: 2025-11-08
**Ge√§ndert**: Nur Model-Downloads, KEINE vLLM-Code-√Ñnderungen

---

## State.py √Ñnderungen (seit heute)

### Aktuelle Konfiguration (HEAD):
```python
self._vllm_manager = vLLMProcessManager(
    port=8001,
    max_model_len=22256,  # ‚¨áÔ∏è Reduziert wegen P40 VRAM-Limit
    gpu_memory_utilization=0.95  # ‚¨ÜÔ∏è Erh√∂ht von 0.85
)
```

### Vorherige Konfiguration (commit 3732393):
```python
self._vllm_manager = vLLMProcessManager(
    port=8001,
    max_model_len=16384,  # 16K Context
    gpu_memory_utilization=0.85
)
```

---

## Rollback-Plan

### Schritt 1: Teste commit 3732393 (letzte funktionierende Version)
```bash
# Checkout alte vllm_manager.py
git checkout 3732393 -- aifred/lib/vllm_manager.py

# Teste mit 16K Context + 0.85 GPU Memory
# In state.py √§ndern:
max_model_len=16384
gpu_memory_utilization=0.85
```

**Erwartung**: Sollte funktionieren (war die funktionierende Version)

---

### Schritt 2: Teste mit h√∂herem Context
```bash
# Behalte commit 3732393 vllm_manager.py
# Teste mit 32K Context + 0.85 GPU Memory
max_model_len=32768
gpu_memory_utilization=0.85
```

**Erwartung**: Evtl. KV-Cache Error (zu wenig VRAM)

---

### Schritt 3: Teste mit mehr GPU Memory
```bash
# Behalte commit 3732393 vllm_manager.py
# Teste mit 32K Context + 0.95 GPU Memory
max_model_len=32768
gpu_memory_utilization=0.95
```

**Erwartung**: Evtl. funktioniert es mit mehr VRAM

---

### Schritt 4: Finde Maximum f√ºr P40
```bash
# Behalte commit 3732393 vllm_manager.py
# Binary Search f√ºr max_model_len:
# Test 1: 24000 (zwischen 16K und 32K)
# Test 2: 28000 oder 20000 (je nach Ergebnis)
# Test 3: ...
```

**Ziel**: Maximalen stabilen Context f√ºr Tesla P40 finden

---

## Unterschiede zwischen Commits

### detect_quantization() Funktion

**Commit 3732393 (FUNKTIONIERT)**:
```python
def detect_quantization(model_name: str) -> str:
    model_lower = model_name.lower()
    if "awq" in model_lower:
        return "awq_marlin"  # Fastest on Ampere+ GPUs
    elif "gguf" in model_lower:
        return "gguf"
    elif "gptq" in model_lower:
        return "gptq"
    else:
        return ""  # No quantization (FP16/BF16)

# Usage:
quant = detect_quantization(model)
if quant:
    cmd.extend(["--quantization", quant])
    logger.info(f"‚úÖ Using quantization: {quant}")
```

**Commit 4bb64ca (AKTUELL - CRASHT)**:
```python
# Funktion wurde ENTFERNT

# Begr√ºndung im Code:
# Note: We don't add --quantization for AWQ models because:
# 1. AWQ models have quantization in their config.json
# 2. vLLM auto-detects AWQ and uses Marlin kernel on Ampere+ GPUs
# 3. Specifying --quantization awq_marlin causes a config mismatch error
logger.info(f"‚úÖ vLLM will auto-detect quantization from model config")
```

---

## Hypothesen

### Hypothese 1: `--quantization` Flag ist n√∂tig
- ‚úÖ Commit 3732393 funktionierte MIT Flag
- ‚ùå Commit 4bb64ca crasht OHNE Flag
- **Test**: Restore `detect_quantization()` und `--quantization` Flag

### Hypothese 2: 22K Context ist zu viel
- ‚ö†Ô∏è vLLM sagte "max 22256 tokens possible"
- ‚ö†Ô∏è Aber evtl. ist das zu knapp?
- **Test**: Gehe zur√ºck zu 16K (funktionierte vorher)

### Hypothese 3: 0.95 GPU Memory ist zu viel
- ‚ö†Ô∏è Evtl. braucht vLLM mehr Overhead
- ‚ö†Ô∏è 0.85 war sicherer
- **Test**: Gehe zur√ºck zu 0.85

### Hypothese 4: Kombination aus allen Faktoren
- ‚ö†Ô∏è 16K + 0.85 + quantization Flag = funktioniert
- ‚ùå 22K + 0.95 + KEIN Flag = crasht
- **Test**: Systematisch einzelne Variablen √§ndern

---

## N√§chste Schritte

1. ‚úÖ **Restore commit 3732393 vllm_manager.py** komplett
2. ‚úÖ **Setze state.py zur√ºck auf 16K + 0.85**
3. üß™ **Teste ob vLLM startet**
4. üß™ **Falls ja: Schrittweise max_model_len erh√∂hen**
5. üß™ **Finde maximalen stabilen Wert f√ºr P40**
6. üìù **Dokumentiere finalen Wert**

---

## Tesla P40 Hardware Limits

- **VRAM**: 24 GB
- **Compute Capability**: 6.1 (Pascal)
- **FP16 Performance**: 1:64 ratio (sehr langsam!)
- **Recommended Backend**: Ollama (GGUF)
- **KV-Cache Limit**: ~3 GiB bei 95% GPU Memory

**Qwen3-8B-AWQ KV-Cache Requirements**:
- 16K context ‚Üí ~1.4 GiB KV cache ‚úÖ Funktioniert
- 22K context ‚Üí ~2.7 GiB KV cache ‚ö†Ô∏è Knapp
- 32K context ‚Üí ~4.0 GiB KV cache ‚ùå Zu viel
- 40K context ‚Üí ~5.6 GiB KV cache ‚ùå Viel zu viel

---

## Zusammenfassung

**Funktionierende Konfiguration (commit 3732393)**:
- ‚úÖ `detect_quantization()` vorhanden
- ‚úÖ `--quantization awq_marlin` Flag gesetzt
- ‚úÖ `max_model_len=16384` (16K)
- ‚úÖ `gpu_memory_utilization=0.85`

**Aktuelle Konfiguration (HEAD - CRASHT)**:
- ‚ùå `detect_quantization()` entfernt
- ‚ùå KEIN `--quantization` Flag
- ‚ùå `max_model_len=22256` (22K)
- ‚ùå `gpu_memory_utilization=0.95`

**Plan**: Schritt f√ºr Schritt zur√ºck zur funktionierenden Konfiguration gehen!
