# AIfred Intelligence - Dokumentations-Ãœbersicht

**Stand:** 2025-10-17
**Status:** âœ… Portable, Hardware-Erkennung, LLM-Parameter
**Projekt:** AIfred Intelligence (ehemals "Voice Assistant")

---

## ğŸ“š Dokumentations-Index

### ğŸš€ Haupt-Dokumentation

#### [../README.md](../README.md) - Projekt-Ãœbersicht & Quick Start
**Status:** âœ… Aktuell (Stand: 2025-10-14)

**Inhalt:**
- Projekt-Geschichte & Name "AIfred Intelligence"
- Features-Ãœbersicht (Voice, Multi-Model, Web-Recherche)
- Installation & Setup
- Nutzung & Workflows
- Systemd Service Setup
- Performance-Metriken

**Key Features:**
- ğŸ™ï¸ Multi-Modal Voice Interface (Whisper + Edge/Piper TTS)
- ğŸ¤– Multi-Model AI Support (qwen, llama, mistral, command-r)
- ğŸ” Agentic Web Research mit 3-stufigem Fallback
- ğŸ’­ Denkprozess-Transparenz mit `<think>` Tags
- ğŸ“Š Chat History mit Context & Model-Wechsel Separator

---

### ğŸ—ï¸ Architektur & Design

#### [architecture-agentic-features.md](architecture-agentic-features.md) - Agent-System
**Status:** âœ… Aktuell (Stand: 2025-10-13)

**Inhalt:**
- 5-Stufen Agentische Pipeline
- Tool-System (SearXNG, Brave, Tavily, Scraping)
- Research-Modi: Eigenes Wissen, Schnell, AusfÃ¼hrlich, Automatik
- Performance-Ziele & Metriken

**Architektur:**
```
User Query â†’ Decision â†’ Query Opt â†’ Search â†’ Rating â†’ Scrape â†’ Answer
```

---

### ğŸ”§ Setup & Konfiguration

#### [API_SETUP.md](API_SETUP.md) - Web-Search API Konfiguration
**Status:** âœ… Aktuell (Stand: 2025-10-13)

**Inhalt:**
- 3-Stufen Fallback System:
  1. **Brave Search API** (2.000/Monat, privacy-focused)
  2. **Tavily AI** (1.000/Monat, RAG-optimiert)
  3. **SearXNG** (Unlimited, self-hosted)
- Docker Setup fÃ¼r SearXNG
- API Keys Setup (.env)
- Troubleshooting

**Wichtig:** SearXNG lÃ¤uft auf `http://localhost:8888`

#### [MIGRATION.md](MIGRATION.md) - Migration Mini-PC â†’ WSL/Hauptrechner
**Status:** âœ… Aktuell (Stand: 2025-10-14)

**Inhalt:**
- 6-Phasen Migrations-Anleitung
- Export als tar.gz
- Import auf WSL2 (Windows 11)
- Voraussetzungen (Python, Ollama, Docker)
- Systemd Service Setup
- SSL/HTTPS Konfiguration
- PortabilitÃ¤ts-Features
- Troubleshooting

**Ziel-Hardware:**
- CPU: AMD Ryzen 9 9900X3D
- GPU: NVIDIA RTX 3060
- RAM: 32GB+
- OS: Windows 11 + WSL2 (Ubuntu)

---

### ğŸ¤– LLM Models & Benchmarks

#### [HARDWARE_DETECTION.md](HARDWARE_DETECTION.md) - Hardware-Erkennung & PortabilitÃ¤t â­ NEU
**Status:** âœ… Aktuell (Stand: 2025-10-17)

**Inhalt:**
- Automatische GPU-Erkennung (AMD, NVIDIA, CPU-only)
- Dynamische Parameter-Konfiguration basierend auf VRAM
- Bekannte Hardware-Probleme (AMD iGPU + 32B = GPU Hang)
- Context Window GrÃ¶ÃŸen & RAM/VRAM Verbrauch
- Ollama Bug: Compute Buffers nicht in VRAM-Kalkulation
- PortabilitÃ¤t zwischen Systemen (MiniPC vs. Hauptrechner)

**Wichtig:**
- AMD Radeon 780M crasht bei qwen3:32B mit GPU â†’ Auto-Fallback auf CPU
- RTX 3060 unterstÃ¼tzt 32B mit GPU (Hybrid-Modus)
- Context-GrÃ¶ÃŸen werden dynamisch angepasst (8K-64K je nach Hardware)

#### [LLM_PARAMETERS.md](LLM_PARAMETERS.md) - LLM-Parameter Guide â­ NEU
**Status:** âœ… Aktuell (Stand: 2025-10-17)

**Inhalt:**
- Alle Ollama Options-Parameter erklÃ¤rt (30+)
- Sampling: temperature, top_p, top_k, min_p, typical_p
- Repetition-Kontrolle: repeat_penalty, presence_penalty, frequency_penalty
- Output-Kontrolle: num_predict, stop, penalize_newline
- Context & Memory: num_ctx, num_keep, seed
- Mirostat Sampling (fortgeschritten)
- 6 Presets fÃ¼r verschiedene Aufgaben
- Best Practices & Troubleshooting

**Presets:**
- ğŸ¯ Fakten/Code (temp=0.3, top_p=0.5)
- ğŸ’¬ Chat (temp=0.8, top_p=0.9)
- ğŸ¨ Kreativ (temp=1.2, top_p=0.95)

#### [LLM_COMPARISON.md](LLM_COMPARISON.md) - Model-Vergleich
**Status:** âš ï¸ Teilweise veraltet (Stand: 2025-10-13)

**Inhalt:**
- Technischer Vergleich von 6 Modellen
- RAG Score, Tool-Use, Speed, RAM
- Use-Case Empfehlungen
- Benchmarks

**Fehlt:**
- âŒ qwen3:0.6b, 1.7b, 4b (neu installiert 2025-10-14)
- âŒ qwen2.5:32b Performance-Daten
- âŒ llama3.2:3b Entscheidungs-QualitÃ¤t Issues

**Action:** Sollte mit MODEL_BENCHMARK_RESULTS aktualisiert werden

#### [LLM_HELP_UI.md](LLM_HELP_UI.md) - User-Freundliche Model-Hilfe
**Status:** âš ï¸ Veraltet (Stand: 2025-10-13)

**Inhalt:**
- UI-optimierte Model-Ãœbersicht
- SchnellÃ¼bersicht-Tabelle
- Use-Case Empfehlungen

**Action:** Sollte mit neuen qwen3-Modellen aktualisiert werden

#### [MODEL_BENCHMARK_TEST.md](MODEL_BENCHMARK_TEST.md) - Benchmark Test-Plan
**Status:** âœ… Aktuell (Stand: 2025-10-14)

**Inhalt:**
- 6 Test-Szenarien fÃ¼r Model-Vergleich
- Entscheidungs-QualitÃ¤t Tests
- Geschwindigkeits-Tests
- Thinking-Quality Tests
- Tabellen zum AusfÃ¼llen

**Modelle getestet:**
- llama3.2:3b (Referenz - bekannt unzuverlÃ¤ssig)
- qwen3:0.6b, 1.7b, 4b, 8b (neu!)
- qwen2.5:32b (Referenz - langsam aber korrekt)

**Test-Fragen:**
1. Trump/Hamas Friedensabkommen (komplex, muss Web-Recherche sein)
2. "Guten Morgen" (einfach, keine Web-Recherche)
3. Wetter Berlin (muss IMMER Web-Recherche sein)
4. Emoji-Anfrage (KreativitÃ¤t)
5. Mathe-Reasoning (Thinking Quality)
6. Aktuelle News (Web-Recherche Trigger)

---

#### [MEMORY_MANAGEMENT.md](MEMORY_MANAGEMENT.md) - RAM/VRAM Management
**Status:** âœ… Aktuell (Stand: 2025-10-15)

**Inhalt:**
- Smart Model Loading mit RAM-Check
- Automatisches Entladen bei wenig Speicher
- Signal Handler fÃ¼r sauberen Shutdown
- Memory-Manager Funktionen

---

## ğŸ“Š Aktueller Status (2025-10-17)

### âœ… Fertiggestellt

1. **Projekt-Rename: "AIfred Intelligence"**
   - âœ… mobile_voice_assistant.py â†’ aifred_intelligence.py
   - âœ… README.md aktualisiert
   - âœ… systemd service aktualisiert
   - âœ… Alle Pfade portabel gemacht

2. **PortabilitÃ¤t**
   - âœ… Alle Pfade relativ mit PROJECT_ROOT
   - âœ… Platform-spezifische Piper Binary Erkennung
   - âœ… SSL optional & portable
   - âœ… MIGRATION.md Guide erstellt

3. **Inference-Zeit Tracking**
   - âœ… Entscheidungs-Zeit angezeigt (Automatik-Modus)
   - âœ… Query Optimization Zeit
   - âœ… URL Rating Zeit
   - âœ… Finale Inferenz Zeit
   - âœ… Separator im Log (â•â•â•) nach jeder Anfrage

4. **Model Downloads**
   - âœ… qwen3:0.6b (522 MB)
   - âœ… qwen3:1.7b (1.4 GB)
   - âœ… qwen3:4b (2.5 GB)
   - âœ… qwen3:8b (bereits installiert)

5. **Benchmark Infrastructure**
   - âœ… MODEL_BENCHMARK_TEST.md (manuell)
   - âœ… scripts/benchmark_models.py (automatisch)

6. **Hardware-Erkennung & PortabilitÃ¤t** â­ NEU (2025-10-17)
   - âœ… Automatische GPU-Typ Erkennung (AMD/NVIDIA)
   - âœ… VRAM-GrÃ¶ÃŸe via rocm-smi/nvidia-smi
   - âœ… iGPU Detection & StabilitÃ¤ts-Check
   - âœ… Dynamische Parameter-Konfiguration
   - âœ… CPU-Fallback fÃ¼r problematische Kombinationen
   - âœ… Context-Size an Hardware anpassen
   - âœ… Portabel zwischen MiniPC & Hauptrechner

7. **LLM-Parameter Dokumentation** â­ NEU (2025-10-17)
   - âœ… Alle 30+ Ollama Options erklÃ¤rt
   - âœ… 6 Presets fÃ¼r verschiedene Use-Cases
   - âœ… Best Practices & Troubleshooting
   - âœ… API-Verwendung (Python, REST, CLI)

### ğŸš§ In Arbeit

1. **LLM-Parameter UI** (2025-10-17)
   - ğŸ”„ Accordion mit wichtigsten Parametern
   - ğŸ”„ Temperature, Max Tokens, Repeat Penalty, Seed
   - ğŸ”„ Verschachteltes Accordion fÃ¼r Fortgeschrittene
   - ğŸ”„ Integration in Chat-Interface

2. **Dokumentation**
   - âœ… HARDWARE_DETECTION.md erstellt
   - âœ… LLM_PARAMETERS.md erstellt
   - âœ… INDEX.md aktualisiert (diese Datei)
   - â³ LLM_COMPARISON.md updaten mit qwen3

### ğŸ“ Noch zu tun

1. **LLM-Parameter UI implementieren**
   - [ ] Accordion unterhalb Recherche-Modus
   - [ ] 4 Basis-Parameter (Temperature, Max Tokens, Repeat, Seed)
   - [ ] Fortgeschritten-Accordion (Top P, Top K)
   - [ ] An ollama.chat() Ã¼bergeben

2. **Dokumentation finalisieren**
   - [ ] LLM_COMPARISON.md mit Benchmark-Daten updaten
   - [ ] LLM_HELP_UI.md mit qwen3-Modellen updaten

3. **Git Commit - Hardware Detection**
   - [ ] ollama_wrapper.py (Hardware-Erkennung)
   - [ ] Bugfix: num_ctx=128K â†’ 8K (RAM-Limit)
   - [ ] HARDWARE_DETECTION.md
   - [ ] LLM_PARAMETERS.md
   - [ ] INDEX.md Update

4. **Migration testen**
   - [ ] tar.gz Export erstellen
   - [ ] Auf WSL2/Hauptrechner importieren
   - [ ] Performance vergleichen (Mini-PC vs. 9900X3D)

---

## ğŸ¯ Quick Reference

### FÃ¼r User (Schnellstart)

**Neu hier?**
1. **Start:** [../README.md](../README.md) - Projekt-Ãœbersicht
2. **Setup:** [API_SETUP.md](API_SETUP.md) - Web-Suche konfigurieren
3. **Hardware:** [HARDWARE_DETECTION.md](HARDWARE_DETECTION.md) - GPU-Erkennung & Limits â­
4. **LLM-Params:** [LLM_PARAMETERS.md](LLM_PARAMETERS.md) - Temperature & Co. â­
5. **Models:** [MODEL_BENCHMARK_TEST.md](MODEL_BENCHMARK_TEST.md) - Welches Model?

**Migration auf anderen Rechner?**
- **Guide:** [MIGRATION.md](MIGRATION.md) - Schritt-fÃ¼r-Schritt

### FÃ¼r Entwickler

**Code verstehen?**
1. **Architektur:** [architecture-agentic-features.md](architecture-agentic-features.md)
2. **Code:**
   - `aifred_intelligence.py` - Haupt-App (74 KB)
   - `agent_tools.py` - Multi-API Search (22 KB)
   - `scripts/benchmark_models.py` - Automated Tests

**Testing:**
```bash
# Manuelle Tests
python aifred_intelligence.py

# Automated Benchmarks
python scripts/benchmark_models.py
```

### FÃ¼r Troubleshooting

**Problem?**
1. **API Setup:** [API_SETUP.md - Troubleshooting](API_SETUP.md#troubleshooting)
2. **Migration:** [MIGRATION.md - Troubleshooting](MIGRATION.md#troubleshooting)
3. **Logs:** `sudo journalctl -u aifred-intelligence.service -f`

---

## ğŸ“ˆ Versions-Historie

### v3.1 - Hardware Detection & LLM Parameters (2025-10-17) â­ NEU

**Major Changes:**
- ğŸ” **Hardware-Erkennung:** Automatisch GPU-Typ, VRAM, StabilitÃ¤t
- ğŸ›ï¸ **Dynamische Konfig:** Parameter basierend auf Hardware
- ğŸ›¡ï¸ **CPU-Fallback:** AMD iGPU + 32B = auto CPU (GPU crasht)
- ğŸ“Š **Context Limits:** 8K-64K je nach RAM/VRAM
- ğŸ“š **LLM-Parameter Guide:** 30+ Parameter erklÃ¤rt
- ğŸ› **Bugfix:** num_ctx=128K â†’ 8K (RAM-Limit)

**PortabilitÃ¤t:**
- âœ… Gleicher Code auf MiniPC (AMD iGPU) & Hauptrechner (RTX 3060)
- âœ… Automatische Anpassung an Hardware
- âœ… Kein manuelles Tuning nÃ¶tig

**Known Issues Fixed:**
- âœ… AMD 780M GPU Hang mit 32B â†’ Auto CPU-Fallback
- âœ… "model requires more system memory" â†’ Context reduziert
- âœ… Ollama Compute Buffers nicht in Kalkulation â†’ Manuelles Limit

**Dokumentation:**
- âœ… HARDWARE_DETECTION.md - VollstÃ¤ndiger Hardware-Guide
- âœ… LLM_PARAMETERS.md - Alle Ollama Options erklÃ¤rt
- âœ… 6 Presets fÃ¼r verschiedene Use-Cases

### v3.0 - Portability & Benchmarks (2025-10-14)

**Major Changes:**
- ğŸ© **Rename:** "Voice Assistant" â†’ "AIfred Intelligence"
- ğŸ“¦ **Portability:** Alle Pfade relativ, platform-aware
- â±ï¸ **Inference Tracking:** Zeiten fÃ¼r alle Pipeline-Steps
- ğŸ§ª **Model Benchmarks:** qwen3:0.6b/1.7b/4b getestet
- ğŸ“ **Migration Guide:** WSL2-ready tar.gz Export
- ğŸ§¹ **Cleanup:** Backup-Dateien entfernt, Docs reorganisiert

**Performance Insights:**
- llama3.2:3b: Schnell (6-9s) aber **unzuverlÃ¤ssig** (falsche Entscheidungen!)
- qwen3:4b: **Bester Kandidat** fÃ¼r Entscheidungen (genau + schnell)
- qwen2.5:32b: Langsam (84s) aber **100% korrekt**

**Beobachtungen:**
- llama3.2:3b entscheidet bei Trump/Hamas fÃ¤lschlicherweise "kein Web"
- qwen3:4b rivalisiert qwen2.5:72b in Benchmarks
- Separator (â•â•â•) verbessert Log-Lesbarkeit massiv

### v2.0 - Multi-API Web-Search (2025-10-13)

**Features:**
- ğŸŒ 3-Stufen Fallback (Brave â†’ Tavily â†’ SearXNG)
- ğŸ¤– Agent-Modi mit UI Settings
- ğŸ“Š LLM Model Auswahl Hilfe
- âœ… SearXNG self-hosted unlimited

**Fixes:**
- âœ… DuckDuckGo "0 URLs" Problem gelÃ¶st
- âœ… AI nutzt echte Web-Daten statt Training

### v1.x - Basis Voice Assistant (2024-10)

**Features:**
- ğŸ¤ Whisper STT
- ğŸ¤– Ollama Integration
- ğŸ”Š Edge/Piper TTS
- ğŸ“± Gradio UI
- ğŸ”’ HTTPS Support

---

## ğŸ”— Externe Ressourcen

### Code Repositories
- **AIfred Intelligence:** https://github.com/Peuqui/AIfred-Intelligence
- **System Setup:** https://github.com/Peuqui/minipc-linux

### APIs & Services
- **Brave Search API:** https://brave.com/search/api/
- **Tavily AI:** https://www.tavily.com/
- **SearXNG:** https://github.com/searxng/searxng (lÃ¤uft auf Port 8888)

### Tech Stack
- **Gradio 4.x:** https://gradio.app
- **Ollama:** https://ollama.com
- **Whisper (faster-whisper):** https://github.com/guillaumekln/faster-whisper
- **Edge TTS:** https://github.com/rany2/edge-tts
- **Piper TTS:** https://github.com/rhasspy/piper

---

## ğŸ“ Dokumentations-Richtlinien

### Neue Docs hinzufÃ¼gen

1. Erstelle Markdown in `/docs/`
2. FÃ¼ge Eintrag in INDEX.md hinzu
3. Setze Status & Datum
4. Verlinke verwandte Dokumente

### Status-Flags

- âœ… **Aktuell** - Matches current implementation
- âš ï¸ **Veraltet** - Needs update
- â³ **WIP** - Work in Progress
- ğŸ“ **Geplant** - Planned
- âŒ **Obsolet** - Should be archived/deleted

---

**Letzte Aktualisierung:** 2025-10-17
**Autor:** Claude Code
**Version:** 3.1 - Hardware Detection & LLM Parameters Release
