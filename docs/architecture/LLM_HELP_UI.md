# LLM Model-Auswahl Hilfe (UI Version)

Diese Tabellen sind fÃ¼r die Anzeige in der Web-UI optimiert.

## ğŸ“Š SchnellÃ¼bersicht - Modelle nach Hardware

---

### ğŸ–¥ï¸ **MINI-PC (GEM 10)** - AMD Radeon 780M iGPU (8GB VRAM)
**Hardware:** 32GB RAM total (8GB fÃ¼r iGPU, 24GB System)

#### ğŸ† Top-Empfehlungen fÃ¼r Mini-PC

| Model | GrÃ¶ÃŸe | GPU | Empfehlung | Bester Einsatz |
|-------|-------|-----|------------|----------------|
| **qwen2.5:7b-instruct-q4_K_M** | 4.7 GB | âœ… | â­â­â­â­â­ | **HAUPT-MODELL** - Beste Balance! ğŸ†• |
| **phi3:mini** | 2.2 GB | âœ… | â­â­â­â­â­ | **AIFRED AUTOMATIK** - Ultra-schnell! ğŸ†• |
| **llama3.1:8b** | 4.9 GB | âœ… | â­â­â­â­ | Meta's Allrounder |
| **mistral:latest** | 4.4 GB | âœ… | â­â­â­â­ | Code & Speed |
| **qwen2.5:3b** | 1.9 GB | âœ… | â­â­â­â­ | AIfred Backup (32K Context) |

#### ğŸš€ Mini-Modelle fÃ¼r Tests

| Model | GrÃ¶ÃŸe | Empfehlung | Bester Einsatz |
|-------|-------|------------|----------------|
| **qwen2.5:0.5b** | 397 MB | â­â­ | Tiny-Tests, sehr schnell |
| **qwen2.5-coder:0.5b** | 397 MB | â­â­ | Mini-Code-Completion |

#### ğŸ˜ CPU-Modelle (nutzen RAM, langsam aber beste QualitÃ¤t)

| Model | GrÃ¶ÃŸe | CPU-only | Empfehlung | Hinweis |
|-------|-------|----------|------------|---------|
| **qwen3:32b-q4_K_M** | 20 GB | âŒ | â­â­â­â­â­ | **BESTE QUALITÃ„T** - optimierte Q4 Version! ğŸ†• |
| **qwen2.5:14b** | 9 GB | âŒ | â­â­â­â­ | CPU-Backup fÃ¼r QualitÃ¤t |
| **mixtral:8x7b** | 26 GB | âŒ | â­â­â­â­â­ | MoE-Champion |
| **command-r** | 18 GB | âš ï¸ | â­â­â­â­ | RAG-optimiert |

---

### ğŸ’» **HAUPT-PC (Aragon)** - RTX 3060 12GB + Ryzen 9900X3D
**Hardware:** RTX 3060 12GB VRAM + 64GB RAM

#### ğŸ† Top-Empfehlungen fÃ¼r Haupt-PC (RTX 3060 12GB)

| Model | GrÃ¶ÃŸe | Empfehlung | Bester Einsatz |
|-------|-------|------------|----------------|
| **qwen2.5-coder:14b-q4_K_M** | 9 GB | â­â­â­â­â­ | **CODING** (92 Sprachen, beste Code-QualitÃ¤t) ğŸ†• |
| **qwen2.5:14b** | 9 GB | â­â­â­â­â­ | **Web-Recherche** (RAG Score 1.0!) |
| **qwen2.5:7b-instruct-q4_K_M** | 4.7 GB | â­â­â­â­â­ | **SPEED** - Schneller als 14B! ğŸ†• |
| **qwen3:8b** | 5.2 GB | â­â­â­â­ | Balance: Schnell + Gut |
| **llama3.1:8b** | 4.9 GB | â­â­â­â­ | Meta's Allrounder |
| **mistral:latest** | 4.4 GB | â­â­â­â­ | Code & Speed |
| **phi3:mini** | 2.2 GB | â­â­â­â­â­ | **AIFRED AUTOMATIK** ğŸ†• |
| **qwen2.5:3b** | 1.9 GB | â­â­â­â­ | AIfred Backup (32K Context) |
| **qwen2.5-coder:0.5b** | 397 MB | â­â­ | Mini-Code-Tests |

#### ğŸ˜ GroÃŸe Modelle fÃ¼r Haupt-PC (nutzen CPU + RAM)

| Model | GrÃ¶ÃŸe | GPU/CPU | Empfehlung | Hinweis |
|-------|-------|---------|------------|---------|
| **qwen3:32b-q4_K_M** | 20 GB | CPU+RAM | â­â­â­â­â­ | **BESTE QUALITÃ„T** - optimierte Q4 Version! ğŸ†• |
| **command-r** | 18 GB | GPU+CPU | â­â­â­â­ | Enterprise RAG, zitiert Quellen |
| **qwen2.5vl:7b-fp16** | 16 GB | CPU+RAM | â­â­â­â­ | **VISION + Text** (Bildanalyse, FP16 PrÃ¤zision) |

#### ğŸ“Š Embedding-Modelle (fÃ¼r RAG/Suche)

| Model | GrÃ¶ÃŸe | Dimensionen | Bester Einsatz |
|-------|-------|-------------|----------------|
| **mxbai-embed-large** | 669 MB | 1024 | Hochqualitative Embeddings fÃ¼r Suche |
| **qwen3-embedding:8b** | 4.7 GB | 8192 | Sehr groÃŸe Embeddings (prÃ¤zise) |

**Hinweis:** Embedding-Modelle sind KEINE Chat-Modelle! Sie konvertieren Text in Vektoren fÃ¼r Suche/RAG.

---

## ğŸ¯ Empfehlungen nach Use-Case

### ğŸ’» Coding & Development ğŸ†•
**Empfohlen:** `qwen2.5-coder:14b-instruct-q4_K_M`
- â­ **BESTE WAHL fÃ¼r Coding!**
- 92 Programmiersprachen
- Exzellente Code-Completion & Debugging
- Passt perfekt auf RTX 3060 12GB
- Weniger Halluzinationen als DeepSeek-R1 (14.3% â†’ <2%)
- HumanEval: 88.7% | MBPP: 83.5%

**FÃ¼r Mini-Code-Tasks:** `qwen2.5-coder:0.5b`
- Ultra-schnell
- Nur 397 MB
- Gut fÃ¼r einfache Code-Snippets

### ğŸ’¬ Web-Recherche (Haupt-Model)
**Empfohlen:** `qwen2.5:14b`
- Beste RAG-Scores (1.0 = perfekt!)
- Nutzt NUR Recherche-Daten
- Exzellent fÃ¼r faktische Aufgaben
- Passt perfekt auf RTX 3060 12GB

**Alternative:** `qwen3:8b`
- Schneller, weniger VRAM
- Immer noch sehr gut

### ğŸ¤– AIfred Intelligence Automatik ğŸ†•
**PRIMÃ„R:** `phi3:mini` â­â­â­â­â­
- â­ **BESTE WAHL fÃ¼r Automatik!**
- Hallucination-Rate: <3% (vs. DeepSeek-R1: 14.3%)
- Ultra-schnell: 40-60 tokens/sec
- Microsoft Production-Quality
- Performance wie 38B Modell!
- Nur 2.2 GB - lÃ¤uft parallel zu Haupt-LLM

**BACKUP:** `qwen2.5:3b`
- 32K Context (vs. Phi3's 4K) - wichtig fÃ¼r lÃ¤ngere Texte!
- Nur 1.9 GB
- Gute Fallback-Option
- Bereits installiert auf beiden Systemen

### ğŸ“š Komplexe Reasoning-Aufgaben
**Empfohlen:** `qwen3:32b-q4_K_M`
- Beste QualitÃ¤t fÃ¼r komplexe Probleme
- Math, Reasoning, Logik
- **RTX 3060:** Nutzt CPU + RAM (langsam, aber beste QualitÃ¤t)
- **RTX 4090:** LÃ¤uft auf GPU (schnell!)

### âš¡ Maximale Geschwindigkeit
**Empfohlen:** `qwen3:0.6b` oder `qwen2.5:0.5b`
- Extrem schnell (< 2 Sek fÃ¼r Antwort)
- FÃ¼r einfache Tasks ausreichend
- Ideal fÃ¼r Benchmarks

### ğŸ¢ Enterprise / Produktion
**Empfohlen:** `command-r` oder `qwen2.5:32b`
- Beste ZuverlÃ¤ssigkeit
- RAG-optimiert
- Function Calling

---

## ğŸ“Š Erweiterte Vergleichs-Tabelle

### Haupt-PC (RTX 3060 12GB) - GPU-optimiert

| Model | GrÃ¶ÃŸe | RAG Score | Tool-Use | Speed | VRAM | Context |
|-------|-------|-----------|----------|-------|------|---------|
| **qwen2.5-coder:14b** ğŸ†• | 9 GB | 0.92 | 0.96 | Mittel | âœ… 9 GB | 128K |
| **qwen2.5:14b** | 9 GB | 1.0 ğŸ† | 0.95 | Mittel | âœ… 9 GB | 128K |
| gemma2:9b-instruct-q8_0 | 9.8 GB | 0.88 | 0.89 | Mittel | âœ… 10 GB | 8K |
| deepseek-coder-v2:16b | 8.9 GB | 0.90 | 0.94 | Mittel | âœ… 9 GB | 16K |
| qwen3:8b | 5.2 GB | 0.933 | 0.90 | Schnell | âœ… 5 GB | 128K |
| gemma2:9b | 5.4 GB | 0.82 | 0.85 | Schnell | âœ… 5 GB | 8K |
| llama3.1:8b | 4.9 GB | 0.85 | 0.88 | Schnell | âœ… 5 GB | 128K |
| mistral:latest | 4.4 GB | 0.88 | 0.85 | Schnell | âœ… 4 GB | 32K |

### Mini-PC (AMD 780M iGPU 8GB) - iGPU-optimiert

| Model | GrÃ¶ÃŸe | RAG Score | Tool-Use | Speed | VRAM | Context |
|-------|-------|-----------|----------|-------|------|---------|
| qwen3:8b | 5.2 GB | 0.933 | 0.90 | Schnell | âœ… 5 GB | 128K |
| gemma2:9b | 5.4 GB | 0.82 | 0.85 | Schnell | âœ… 5 GB | 8K |
| llama3.1:8b | 4.9 GB | 0.85 | 0.88 | Schnell | âœ… 5 GB | 128K |
| mistral:latest | 4.4 GB | 0.88 | 0.85 | Schnell | âœ… 4 GB | 32K |
| qwen3:4b | 2.5 GB | 0.92 | 0.88 | Sehr schnell | âœ… 3 GB | 32K |
| llama3.2:3b | 2.0 GB | ~0.70 | 0.75 | Sehr schnell | âœ… 2 GB | 128K |
| qwen2.5:3b | 1.9 GB | 0.85 | 0.80 | Sehr schnell | âœ… 2 GB | 32K |
| qwen3:1.7b | 1.4 GB | 0.80 | 0.75 | Extrem schnell | âœ… 1 GB | 32K |

### GroÃŸe Modelle (CPU + RAM auf beiden Systemen)

| Model | GrÃ¶ÃŸe | RAG Score | Tool-Use | Speed | GPU | Context |
|-------|-------|-----------|----------|-------|-----|---------|
| qwen3:32b | 20 GB | 0.98 | 0.98 | Langsam | âŒ CPU | 128K |
| command-r | 18 GB | 0.92 | 0.95 | Langsam | âš ï¸ Hybrid | 128K |
| qwen2.5vl:7b-fp16 | 16 GB | - | - | Langsam | âŒ CPU | 32K |
| qwen3:8b-fp16 | 16 GB | 0.95 | 0.92 | Mittel | âŒ CPU | 128K |

**Legende:**
- **RAG Score:** Context Adherence (1.0 = perfekt, nutzt nur Recherche-Daten)
- **Tool-Use:** Function Calling / Agent F1 Score
- **Speed:** Inferenz-Geschwindigkeit auf Mini-PC
- **RAM:** GeschÃ¤tzter Speicherverbrauch
- **Context:** Natives Max Context Window

---

## ğŸ”§ Hardware-Erkennung & GPU-Support

### ğŸ–¥ï¸ Mini-PC: AMD Radeon 780M iGPU (8 GB VRAM effektiv)
**System:** 32GB RAM total (8GB fÃ¼r iGPU reserviert, 24GB System)

**âœ… LÃ¤uft perfekt auf iGPU (< 8 GB):**
- **qwen3:8b** (5.2 GB) â­ **EMPFOHLEN**
- **gemma2:9b** (5.4 GB)
- **llama3.1:8b** (4.9 GB)
- **mistral** (4.4 GB)
- **qwen3:4b, 1.7b, 0.6b** (alle kleinen Modelle)
- **llama3.2:3b, qwen2.5:3b, qwen2.5-coder:0.5b**

**âš ï¸ Grenzwertig (nutzt CPU-Fallback bei Bedarf):**
- **qwen2.5:14b** (9 GB) - Kann GPU-Limit Ã¼berschreiten
- **command-r** (18 GB) - Hybrid-Modus (teilweise Layers auf GPU)

**âŒ CPU-only (zu groÃŸ fÃ¼r 8GB iGPU):**
- **qwen3:32b** (20 GB) - GPU Hang â†’ Auto-Fallback CPU
- **qwen2.5vl:7b-fp16** (16 GB) - Zu groÃŸ
- **qwen3:8b-fp16** (16 GB) - Zu groÃŸ
- **qwen2.5-coder:14b** (9 GB) - NICHT fÃ¼r Mini-PC empfohlen!

**Status:** AIfred erkennt automatisch AMD iGPU und wechselt auf CPU bei groÃŸen Modellen.

---

### ğŸ’» Haupt-PC: NVIDIA RTX 3060 (12 GB VRAM)
**System:** RTX 3060 12GB + Ryzen 9900X3D + 64GB RAM

**âœ… LÃ¤uft perfekt auf GPU (< 12 GB):**
- **qwen2.5-coder:14b** (9 GB) â­ **EMPFOHLEN fÃ¼r Coding**
- **qwen2.5:14b** (9 GB) â­ **EMPFOHLEN fÃ¼r Research**
- **gemma2:9b-instruct-q8_0** (9.8 GB)
- **deepseek-coder-v2:16b** (8.9 GB)
- **qwen3:8b** (5.2 GB)
- **Alle Modelle < 5 GB**

**âš ï¸ Hybrid-Modus (GPU teilweise + CPU):**
- **command-r** (18 GB) - Einige Layers auf CPU

**âŒ CPU+RAM (zu groÃŸ fÃ¼r 12GB VRAM):**
- **qwen3:32b** (20 GB) - Nutzt 64GB System RAM (langsam, aber beste QualitÃ¤t)
- **qwen2.5vl:7b-fp16** (16 GB)
- **qwen3:8b-fp16** (16 GB)

**Vorteil RTX 3060:**
- GroÃŸe Modelle laufen zwar auf CPU, aber **deutlich schneller** als auf Mini-PC dank Ryzen 9900X3D!

---

## ğŸ’¡ Spezial-Tipps

### ğŸ¯ FÃ¼r Web-Recherche (bester RAG Score)
1. **qwen2.5:14b** - RAG Score 1.0 (perfekt!)
2. qwen3:32b - RAG Score 0.98
3. qwen2.5:32b - RAG Score 0.98

### âš¡ FÃ¼r Automatik-Modus (schnellste Entscheidung)
1. **qwen3:4b** - Beste QualitÃ¤t bei < 3B
2. qwen3:1.7b - Sehr schnell, gut genug
3. qwen2.5:3b - Ãœberraschend zuverlÃ¤ssig

### ğŸ§® FÃ¼r Coding & Math
1. **qwen2.5:32b** - Coding-optimiert
2. qwen3:32b - Beste Reasoning
3. qwen2.5:14b - Guter Kompromiss

### ğŸ“ FÃ¼r kreative Aufgaben
1. **qwen3:32b** - Kreativste Antworten
2. mixtral:8x7b - VielfÃ¤ltige Perspektiven
3. command-r - Strukturierte KreativitÃ¤t

---

## ğŸš€ Performance-Vergleich (Mini-PC, CPU-only)

| Model | Tokens/Sek | 100 WÃ¶rter | Startup | GPU-Support |
|-------|------------|------------|---------|-------------|
| qwen3:0.6b | ~50-70 | ~2 Sek | <1 Sek | âœ… Ja |
| qwen2.5:0.5b | ~50-70 | ~2 Sek | <1 Sek | âœ… Ja |
| qwen3:1.7b | ~35-50 | ~3 Sek | ~1 Sek | âœ… Ja |
| qwen2.5:3b | ~30-40 | ~4 Sek | ~1 Sek | âœ… Ja |
| llama3.2:3b | ~30-40 | ~5 Sek | ~1 Sek | âœ… Ja |
| qwen3:4b | ~25-35 | ~5 Sek | ~1 Sek | âœ… Ja |
| mistral | ~15-25 | ~8 Sek | ~2 Sek | âœ… Ja |
| qwen3:8b | ~15-25 | ~8 Sek | ~2 Sek | âœ… Ja |
| llama3.1:8b | ~15-25 | ~8 Sek | ~2 Sek | âœ… Ja |
| llama2:13b | ~10-15 | ~12 Sek | ~3 Sek | âš ï¸ Hybrid |
| qwen2.5:14b | ~8-12 | ~15 Sek | ~3 Sek | âœ… Ja |
| command-r | ~5-10 | ~20 Sek | ~5 Sek | âš ï¸ Hybrid |
| mixtral:8x7b | ~3-8 | ~25+ Sek | ~8 Sek | âŒ CPU-only |
| qwen2.5:32b | ~2-5 | ~40+ Sek | ~10 Sek | âŒ CPU-only |
| qwen3:32b | ~2-5 | ~40+ Sek | ~10 Sek | âŒ CPU-only |

**Mit GPU (AMD 780M):** ~2-3x schneller fÃ¼r unterstÃ¼tzte Modelle!

---

## ğŸ¨ LLM-Parameter Empfehlungen

### FÃ¼r Fakten & Code (prÃ¤zise)
```
Model: qwen2.5:14b oder qwen3:8b
Temperature: 0.3
Top P: 0.5
Top K: 20
Repeat Penalty: 1.1
```

### FÃ¼r Chat (ausgewogen)
```
Model: qwen3:8b oder qwen2.5:14b
Temperature: 0.8
Top P: 0.9
Top K: 40
Repeat Penalty: 1.1
```

### FÃ¼r KreativitÃ¤t (vielfÃ¤ltig)
```
Model: qwen3:32b oder mixtral:8x7b
Temperature: 1.2
Top P: 0.95
Top K: 80
Repeat Penalty: 1.0
```

### FÃ¼r Benchmarks (reproduzierbar)
```
Model: beliebig
Temperature: 0.3
Seed: 42
Max Tokens: 200
```

---

## ğŸ“‹ Model-Familien Ãœbersicht

### Qwen 3 Familie (neueste, beste Reasoning)
- **qwen3:32b** - Flagship, beste QualitÃ¤t
- **qwen3:8b** - Sweet Spot
- **qwen3:4b** - Beste kleine Model
- **qwen3:1.7b** - Schnell, kompakt
- **qwen3:0.6b** - Minimal

**Context:** 32K (kleine) bis 128K (groÃŸe)
**StÃ¤rken:** Reasoning, Math, Coding

### Qwen 2.5 Familie (RAG-optimiert)
- **qwen2.5:32b** - Enterprise Coding
- **qwen2.5:14b** - RAG Champion (Score 1.0!)
- **qwen2.5:3b** - Ãœberraschend gut
- **qwen2.5:0.5b** - Minimal

**Context:** 32K bis 128K
**StÃ¤rken:** RAG, Web-Recherche, Coding

### Llama Familie (Meta, bewÃ¤hrt)
- **llama3.1:8b** - Aktuell, zuverlÃ¤ssig
- **llama3.2:3b** - Klein, schnell
- **llama2:13b** - Legacy, breites Wissen

**Context:** 4K-128K
**StÃ¤rken:** Allgemein, stabil

### Andere
- **mixtral:8x7b** - Mixture-of-Experts (47B Parameter!)
- **command-r** - Cohere, Enterprise RAG
- **mistral** - Code-Generation

---

## ğŸ”„ Model-Wechsel Empfehlungen

### Aktuelles Setup optimieren

**Haupt-Model (Voice Assistant):**
- Von: llama2:13b oder llama3.1:8b
- Zu: **qwen2.5:14b** (beste RAG!)
- Grund: Perfekt fÃ¼r Web-Recherche, ignoriert Training Data

**Automatik-Model (Entscheidungen):**
- Von: llama3.2:3b (unzuverlÃ¤ssig!)
- Zu: **qwen3:4b** (beste 4B!)
- Grund: Rivalisiert groÃŸe Modelle in Benchmarks

**FÃ¼r komplexe Aufgaben:**
- Zu: **qwen3:32b** (beste Reasoning)
- Achtung: AMD iGPU â†’ CPU-only (langsam, aber beste QualitÃ¤t)

---

## ğŸ’¾ Speicherplatz Management

**Aktuell installiert:** 15 Modelle (~110 GB total)

**Empfohlenes Minimal-Set (3 Modelle):**
1. `qwen2.5:14b` - Haupt-Model (9 GB)
2. `qwen3:4b` - Automatik-Model (2.5 GB)
3. `qwen3:32b` - QualitÃ¤ts-Model (20 GB)
**Total:** ~32 GB

**Empfohlenes Standard-Set (5 Modelle):**
1. `qwen2.5:14b` - Web-Recherche (9 GB)
2. `qwen3:8b` - Balance (5.2 GB)
3. `qwen3:4b` - Automatik (2.5 GB)
4. `qwen3:1.7b` - Schnell (1.4 GB)
5. `qwen3:32b` - QualitÃ¤t (20 GB)
**Total:** ~38 GB

**Zum LÃ¶schen empfohlen (falls Platz knapp):**
- `qwen2.5:0.5b` - Zu klein, kaum nÃ¼tzlich
- `llama3.2:3b` - UnzuverlÃ¤ssig in Benchmarks
- Doppelte: qwen2.5:32b UND qwen3:32b (einer reicht)

---

## ğŸ†˜ Troubleshooting

### GPU-Fehler bei llama2:13b
**Problem:** "GPU error" oder langsame Inferenz
**LÃ¶sung:**
- GPU Toggle ausschalten (CPU-only Modus)
- Oder: Wechsel zu qwen2.5:14b (besser optimiert)

### "Model requires more system memory"
**Problem:** num_ctx zu groÃŸ
**LÃ¶sung:** Automatisch gefixt! Hardware-Erkennung passt Context an.

### "Model not found"
**Problem:** Model nicht installiert
**LÃ¶sung:**
```bash
ollama pull <model-name>
# Beispiel: ollama pull qwen3:4b
```

---

**Erstellt:** 2025-10-17
**Author:** Claude Code
**Version:** 2.0 - VollstÃ¤ndige Ãœbersicht aller 15 Modelle + Hardware-Erkennung
