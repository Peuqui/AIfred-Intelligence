# LLM-Parameter Guide

**Autor:** AIfred Intelligence Team
**Datum:** 2025-10-17
**Version:** 1.0

## √úbersicht

Ollama unterst√ºtzt zahlreiche Parameter zur Feinsteuerung der LLM-Ausgaben. Dieses Dokument erkl√§rt alle verf√ºgbaren Parameter und gibt Empfehlungen f√ºr verschiedene Anwendungsf√§lle.

---

## Sampling-Parameter (Kreativit√§t)

### temperature (0.0 - 2.0, default: 0.8)

**Was ist das?**
- Steuert die "Kreativit√§t" bzw. Zuf√§lligkeit der Antworten
- Niedrig = vorhersehbar, hoch = kreativ

**Technisch:**
- Dividiert die Logits (Modell-Scores) vor Softmax
- H√∂here Werte "gl√§tten" die Wahrscheinlichkeitsverteilung

**Werte:**
```
0.0   ‚Üí Deterministisch (immer gleiche Antwort)
0.1   ‚Üí Sehr pr√§zise, fast keine Variation
0.3   ‚Üí Fokussiert, gut f√ºr Fakten
0.5   ‚Üí Ausgewogen pr√§zise
0.8   ‚Üí Standard (Ollama Default)
1.0   ‚Üí Ausgewogen kreativ
1.2   ‚Üí Kreativ, interessante Formulierungen
1.5   ‚Üí Sehr kreativ, manchmal √ºberraschend
2.0   ‚Üí Extrem kreativ, oft wirr
```

**Anwendungsf√§lle:**
- **Code-Generierung:** 0.2-0.3 (pr√§zise Syntax)
- **Fakten/Recherche:** 0.3-0.5 (fokussiert)
- **Chat/Q&A:** 0.7-0.9 (nat√ºrlich)
- **Geschichten:** 1.0-1.3 (kreativ)
- **Gedichte:** 1.2-1.5 (sehr kreativ)
- **Brainstorming:** 1.3-1.7 (unkonventionell)

**Beispiel:**
```
Prompt: "Nenne 3 Farben"

temp=0.0: "Rot, Blau, Gr√ºn" (immer gleich)
temp=0.8: "Blau, Gr√ºn, Gelb"
temp=1.5: "T√ºrkis, Magenta, Safrangelb"
```

---

### top_p (0.0 - 1.0, default: 0.9)

**Was ist das?**
- "Nucleus Sampling" - w√§hlt aus den wahrscheinlichsten Tokens
- Je niedriger, desto fokussierter die Auswahl

**Technisch:**
- W√§hlt Tokens bis kumulative Wahrscheinlichkeit ‚â• top_p
- Dynamische Auswahl-Gr√∂√üe (im Gegensatz zu top_k)

**Werte:**
```
0.1   ‚Üí Nur die allerbesten Tokens (~10%)
0.5   ‚Üí Mittlere Auswahl (~50%)
0.9   ‚Üí Breite Auswahl (Standard)
0.95  ‚Üí Sehr breite Auswahl
1.0   ‚Üí Alle m√∂glichen Tokens
```

**Interaktion mit temperature:**
```
temp=0.3, top_p=0.5  ‚Üí Sehr fokussiert (Fakten)
temp=0.8, top_p=0.9  ‚Üí Ausgewogen (Standard)
temp=1.2, top_p=0.95 ‚Üí Sehr kreativ
```

---

### top_k (1-100, default: 40)

**Was ist das?**
- Limitiert Auswahl auf Top-K wahrscheinlichste Tokens
- Feste Anzahl (im Gegensatz zu top_p)

**Werte:**
```
1    ‚Üí Greedy (immer bestes Token) = temp=0
10   ‚Üí Sehr eng
40   ‚Üí Standard
100  ‚Üí Breit
```

**top_k vs. top_p:**
- top_k = feste Anzahl
- top_p = dynamisch basierend auf Wahrscheinlichkeit
- Meist nur EINES verwenden (top_p empfohlen)

---

### min_p (0.0 - 1.0, default: 0.0)

**Was ist das?**
- Minimum-Probability-Threshold
- Filtert sehr unwahrscheinliche Tokens aus

**Verwendung:**
```
0.0   ‚Üí Kein Filter (Standard)
0.05  ‚Üí Entfernt die letzten 5%
0.1   ‚Üí Entfernt die letzten 10%
```

---

### typical_p (0.0 - 1.0, default: 0.7)

**Was ist das?**
- "Typical Sampling" - w√§hlt "typische" Tokens
- Alternative zu top_p/top_k

**Technisch:**
- Basiert auf Information Content statt Wahrscheinlichkeit
- Filtert sowohl sehr wahrscheinliche als auch sehr unwahrscheinliche Tokens

**Verwendung:** Meist deaktiviert (1.0) wenn top_p verwendet wird

---

## Output-Kontrolle

### num_predict (-1 oder 1-999999, default: -1)

**Was ist das?**
- Maximale Anzahl zu generierender Tokens
- `-1` = unbegrenzt (bis Modell stoppt)

**Werte:**
```
50    ‚Üí Sehr kurze Antwort (~40 W√∂rter)
200   ‚Üí Kurze Antwort (~150 W√∂rter)
500   ‚Üí Mittlere Antwort (~375 W√∂rter)
1000  ‚Üí Lange Antwort (~750 W√∂rter)
2000  ‚Üí Sehr lange Antwort (~1500 W√∂rter)
-1    ‚Üí Unbegrenzt (Standard)
```

**Wichtig:**
- Ist ein **Maximum**, kein Minimum!
- Modell kann fr√ºher stoppen wenn "fertig"
- N√ºtzlich f√ºr Benchmarks (fixe L√§nge)

**Anwendungsf√§lle:**
- **Kurze Antworten:** 100-200
- **Chat:** 500
- **Artikel:** 1000-2000
- **Keine Limits:** -1

---

### stop (array, default: [])

**Was ist das?**
- Liste von Sequenzen, bei denen Generation stoppt
- N√ºtzlich f√ºr strukturierte Outputs

**Beispiele:**
```json
"stop": ["\n\n"]        // Stop bei Doppel-Zeilenumbruch
"stop": ["END", "###"]  // Stop bei Keywords
"stop": ["\n---\n"]     // Stop bei Separator
```

**Anwendung:**
```
Prompt: "Schreibe ein Gedicht √ºber Winter.\n\n"
stop: ["\n\n\n"]  // Stop nach Gedicht (vor Erkl√§rung)
```

---

### penalize_newline (boolean, default: false)

**Was ist das?**
- Bestraft Zeilenumbr√ºche
- Erzwingt kompakte Antworten

**Verwendung:**
```
false  ‚Üí Normal (Standard)
true   ‚Üí Vermeidet Abs√§tze (kompakter Text)
```

---

## Repetition-Kontrolle

### repeat_penalty (1.0 - 2.0, default: 1.1)

**Was ist das?**
- Bestraft Wiederholung von Tokens
- Verhindert langweilige, sich wiederholende Antworten

**Werte:**
```
1.0   ‚Üí Keine Strafe (reine Modell-Ausgabe)
1.1   ‚Üí Leichte Strafe (Standard)
1.2   ‚Üí Mittlere Strafe
1.5   ‚Üí Starke Strafe (vermeidet Wiederholungen stark)
2.0   ‚Üí Sehr starke Strafe (kann zu unnat√ºrlich wirken)
```

**Technisch:**
- Reduziert Wahrscheinlichkeit bereits verwendeter Tokens
- Wirkt auf die letzten N Tokens (siehe repeat_last_n)

**Anwendungsf√§lle:**
- **Code:** 1.0-1.1 (Wiederholungen OK)
- **Chat:** 1.1-1.2 (leicht variierend)
- **Kreatives Schreiben:** 1.2-1.3 (abwechslungsreich)
- **Vermeidung von Loops:** 1.5+ (bei Problemen)

---

### repeat_last_n (1-999999, default: 64)

**Was ist das?**
- Gr√∂√üe des Fensters f√ºr repeat_penalty
- Wie weit "zur√ºckschauen" f√ºr Wiederholungen?

**Werte:**
```
32   ‚Üí Kurzes Fenster (~24 W√∂rter)
64   ‚Üí Standard
128  ‚Üí Langes Fenster (~96 W√∂rter)
256  ‚Üí Sehr langes Fenster
```

**Kombination:**
```
repeat_penalty=1.2, repeat_last_n=64
‚Üí Bestraft Wiederholungen der letzten 64 Tokens
```

---

### presence_penalty (0.0 - 2.0, default: 0.0)

**Was ist das?**
- Bestraft bereits vorhandene Tokens (unabh√§ngig von H√§ufigkeit)
- OpenAI-Style Parameter

**Unterschied zu repeat_penalty:**
- `repeat_penalty`: Bestraft h√§ufige Wiederholungen
- `presence_penalty`: Bestraft bereits verwendete Tokens einmalig

**Werte:**
```
0.0   ‚Üí Keine Strafe (Standard)
0.5   ‚Üí Leichte Strafe
1.0   ‚Üí Mittlere Strafe
1.5   ‚Üí Starke Strafe (Standard in Ollama)
```

---

### frequency_penalty (0.0 - 2.0, default: 0.0)

**Was ist das?**
- Bestraft Tokens proportional zu ihrer H√§ufigkeit
- OpenAI-Style Parameter

**Werte:**
```
0.0   ‚Üí Keine Strafe (Standard)
0.5   ‚Üí Leichte Strafe
1.0   ‚Üí Mittlere Strafe (Standard in Ollama)
1.5   ‚Üí Starke Strafe
```

---

## Mirostat Sampling (Fortgeschritten)

### mirostat (0, 1, oder 2, default: 0)

**Was ist das?**
- Alternativer Sampling-Algorithmus
- Steuert "Perplexity" (Vorhersagbarkeit)

**Werte:**
```
0  ‚Üí Deaktiviert (Standard) - nutze temperature/top_p
1  ‚Üí Mirostat v1
2  ‚Üí Mirostat v2 (empfohlen wenn Mirostat)
```

**Wann verwenden?**
- Bei Problemen mit zu langweiligen/repetitiven Antworten
- Alternative zu temperature-tuning
- **Meist nicht n√∂tig!**

---

### mirostat_tau (0.0 - 10.0, default: 5.0)

**Was ist das?**
- Target Entropy (Ziel-Perplexity)
- Wie "√ºberraschend" sollen Antworten sein?

**Werte:**
```
0.0   ‚Üí Sehr vorhersehbar
5.0   ‚Üí Standard
10.0  ‚Üí Sehr √ºberraschend
```

---

### mirostat_eta (0.0 - 1.0, default: 0.1)

**Was ist das?**
- Learning Rate f√ºr Mirostat
- Wie schnell anpassen an Ziel-Perplexity?

**Werte:**
```
0.05  ‚Üí Langsame Anpassung
0.1   ‚Üí Standard
0.2   ‚Üí Schnelle Anpassung
```

---

## Context & Memory

### num_ctx (128 - 131072, default: 2048)

**Was ist das?**
- Context Window Gr√∂√üe
- Wie viel Text kann das Modell "sehen"?

**Siehe:** [HARDWARE_DETECTION.md](HARDWARE_DETECTION.md) f√ºr Details

**Wichtig:**
- Gr√∂√üerer Context = mehr RAM/VRAM
- Qwen3 unterst√ºtzt bis 128K nativ
- AIfred setzt automatisch basierend auf Hardware

---

### num_keep (1-999999, default: 0)

**Was ist das?**
- Anzahl Tokens aus Prompt zu behalten
- Bei Multi-Turn Chats wichtig

**Verwendung:**
```
0    ‚Üí Standard (behalte alle)
-1   ‚Üí Behalte alle Tokens
4    ‚Üí Behalte erste 4 Tokens (z.B. System-Prompt)
```

---

### seed (integer, default: random)

**Was ist das?**
- Random Seed f√ºr Reproduzierbarkeit
- Gleicher Seed = gleiche Antwort (bei temp>0)

**Verwendung:**
```
-1         ‚Üí Zuf√§llig (Standard)
42         ‚Üí Fester Seed (reproduzierbar)
123456789  ‚Üí Beliebige Zahl
```

**Anwendungsf√§lle:**
- **Testing:** Fester Seed f√ºr identische Ergebnisse
- **Debugging:** Reproduzierbare Fehler
- **Produktion:** -1 (zuf√§llig)

---

## Hardware & Performance

### num_gpu (0 - 999, default: -1)

**Was ist das?**
- Anzahl Modell-Layer auf GPU
- **NICHT Anzahl GPUs!**

**Siehe:** [HARDWARE_DETECTION.md](HARDWARE_DETECTION.md)

**AIfred setzt automatisch:**
- Kleine Modelle: Auto-Detect
- Gro√üe Modelle: Konservatives Limit
- AMD iGPU + 32B: Force CPU (0)

---

### num_thread (1-999, default: auto)

**Was ist das?**
- Anzahl CPU-Threads f√ºr Inferenz
- Nur relevant bei CPU-Modus

**Werte:**
```
Auto  ‚Üí Ollama w√§hlt optimal
4     ‚Üí 4 Threads
8     ‚Üí 8 Threads (gut f√ºr 8-Core CPU)
```

---

### num_batch (1-999, default: 512)

**Was ist das?**
- Batch Size f√ºr Prompt-Processing
- H√∂her = schneller, aber mehr RAM

---

## Presets f√ºr verschiedene Aufgaben

### üéØ Fakten & Recherche (Pr√§zise)
```json
{
  "temperature": 0.3,
  "top_p": 0.5,
  "repeat_penalty": 1.2,
  "num_predict": 500
}
```

### üíª Code-Generierung (Sehr pr√§zise)
```json
{
  "temperature": 0.2,
  "top_p": 0.5,
  "repeat_penalty": 1.1,
  "num_predict": 1000
}
```

### üí¨ Chat / Q&A (Ausgewogen)
```json
{
  "temperature": 0.8,
  "top_p": 0.9,
  "repeat_penalty": 1.1,
  "num_predict": -1
}
```

### üé® Kreatives Schreiben (Kreativ)
```json
{
  "temperature": 1.2,
  "top_p": 0.95,
  "repeat_penalty": 1.0,
  "num_predict": 2000
}
```

### üìù Gedichte (Sehr kreativ)
```json
{
  "temperature": 1.4,
  "top_p": 0.95,
  "repeat_penalty": 0.9,
  "num_predict": 300
}
```

### üî¨ Benchmarks (Reproduzierbar)
```json
{
  "temperature": 0.3,
  "seed": 42,
  "num_predict": 200
}
```

---

## API-Verwendung

### Python (ollama-python)
```python
import ollama

response = ollama.chat(
    model="qwen3:8b",
    messages=[{"role": "user", "content": "Hallo"}],
    options={
        "temperature": 0.8,
        "top_p": 0.9,
        "num_predict": 500,
        "repeat_penalty": 1.1
    }
)
```

### REST API
```bash
curl http://localhost:11434/api/chat -d '{
  "model": "qwen3:8b",
  "messages": [{"role": "user", "content": "Hallo"}],
  "options": {
    "temperature": 0.8,
    "num_predict": 500
  }
}'
```

### CLI
```bash
ollama run qwen3:8b \
  --temperature 0.8 \
  --num-predict 500 \
  "Dein Prompt hier"
```

---

## Best Practices

### DO ‚úÖ
- Starte mit Defaults (temp=0.8, top_p=0.9)
- Passe temperature f√ºr Aufgabe an
- Nutze num_predict f√ºr Benchmarks
- Setze seed f√ºr reproduzierbare Tests
- Kombiniere temperature + top_p sinnvoll

### DON'T ‚ùå
- Extreme Werte (temp>1.8, top_p<0.3)
- Zu viele Parameter gleichzeitig √§ndern
- Mirostat + temperature/top_p mischen
- num_ctx zu gro√ü setzen (RAM!)
- repeat_penalty zu hoch (>1.5)

---

## Troubleshooting

### Problem: Langweilige, repetitive Antworten
**L√∂sung:**
- Erh√∂he `temperature` (0.8 ‚Üí 1.0)
- Erh√∂he `repeat_penalty` (1.1 ‚Üí 1.3)
- Pr√ºfe `top_p` (sollte ~0.9 sein)

### Problem: Wirre, unverst√§ndliche Antworten
**L√∂sung:**
- Senke `temperature` (1.5 ‚Üí 0.8)
- Senke `top_p` (0.95 ‚Üí 0.85)
- Pr√ºfe ob Modell zu klein f√ºr Aufgabe

### Problem: Antworten zu kurz
**L√∂sung:**
- Erh√∂he `num_predict` (-1 oder 1000+)
- Pr√ºfe Prompt (fordere l√§ngere Antwort)
- Checke `stop` Sequenzen

### Problem: Antworten zu lang
**L√∂sung:**
- Setze `num_predict` (z.B. 300)
- F√ºge `stop` Sequenzen hinzu
- Pr√§zisiere Prompt ("in 3 S√§tzen")

### Problem: Nicht reproduzierbar
**L√∂sung:**
- Setze `seed` auf feste Zahl
- Verwende `temperature=0` f√ºr deterministisch
- Pr√ºfe ob andere Parameter auch gleich

---

## Weitere Dokumentation

- [HARDWARE_DETECTION.md](HARDWARE_DETECTION.md) - Hardware & Context Limits
- [API_SETUP.md](API_SETUP.md) - API Keys & Konfiguration
- [INDEX.md](INDEX.md) - Hauptindex

---

**Stand:** 2025-10-17
**Referenz:** [Ollama API Docs](https://docs.ollama.com/api)
