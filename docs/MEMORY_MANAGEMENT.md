# üß† Smart Model Loading & Memory Management

## √úberblick

AIfred Intelligence verf√ºgt √ºber ein intelligentes Memory-Management-System, das **automatisch** entscheidet, wann Modelle aus dem RAM entladen werden m√ºssen, um Swapping zu vermeiden.

---

## ‚öôÔ∏è Konfiguration

### Ollama Keep-Alive

**Einstellung:** `15 Minuten` (Standard: 5 Minuten)

**Was es bedeutet:**
- Modelle bleiben 15 Minuten im RAM nach dem letzten Request
- Bei Agent-Recherche (3 KI-Calls in ~2 Min): **Kein Re-Loading!**
- Spart ~0.5s pro Call = **1.5s pro Recherche**

**Konfiguration:**
```bash
# /etc/systemd/system/ollama.service
Environment="OLLAMA_KEEP_ALIVE=15m"
```

---

## üß© Komponenten

### 1. `get_available_memory()`

**Was es tut:**
- Liest `/proc/meminfo`
- Gibt **MemAvailable** zur√ºck (nicht MemFree!)
- Ber√ºcksichtigt Kernel-Buffer & Caches

**Beispiel-Output:**
```
Verf√ºgbar: 23.5 GB
```

---

### 2. `get_loaded_models_size()`

**Was es tut:**
- Fragt Ollama API: `GET /api/ps`
- Gibt Gr√∂√üe aller aktuell geladenen Modelle zur√ºck
- Zeigt Liste der geladenen Modelle im Log

**Beispiel-Output:**
```
üìä Geladene Modelle: qwen3:1.7b (2.0 GB), qwen3:8b (5.2 GB)
Gesamt: 7.2 GB
```

---

### 3. `unload_all_models()`

**Was es tut:**
- Holt Liste aller geladenen Modelle via `GET /api/ps`
- Entl√§dt **jedes Modell einzeln** via `POST /api/generate` mit `keep_alive: 0`
- Gibt RAM frei f√ºr gro√üe Modelle

**Beispiel-Output:**
```
üßπ Entlade aktuell geladene Modelle...
   üóëÔ∏è qwen3:1.7b entladen
   üóëÔ∏è qwen3:8b entladen
üßπ Alle Modelle aus RAM entladen
```

---

### 4. `smart_model_load(model_name)`

**Das Herzst√ºck!** Intelligente Entscheidungslogik.

#### **Ablauf:**

```
1. Hole Modellgr√∂√üe aus LARGE_MODELS Liste
   ‚îú‚îÄ Wenn nicht in Liste ‚Üí Kleines Modell ‚Üí ‚úÖ Kein Entladen
   ‚îî‚îÄ Wenn in Liste ‚Üí Weiter zu Schritt 2

2. RAM-Check durchf√ºhren
   ‚îú‚îÄ Verf√ºgbarer RAM
   ‚îú‚îÄ Geladene Modelle
   ‚îî‚îÄ Ben√∂tigter RAM = Modellgr√∂√üe √ó 1.20 (Safety Margin)

3. Entscheidung
   ‚îú‚îÄ Wenn: Verf√ºgbar >= Ben√∂tigt
   ‚îÇ   ‚îî‚îÄ ‚úÖ Kein Entladen - Modell passt rein!
   ‚îî‚îÄ Wenn: Verf√ºgbar < Ben√∂tigt
       ‚îî‚îÄ ‚ö†Ô∏è Entlade geladene Modelle
```

#### **Safety Margin (20%):**

Ollama braucht **zus√§tzlichen RAM** w√§hrend der Inferenz:

```
Modell: 19 GB (Weights)
+ Context Buffer: 0.5 GB
+ KV Cache: 1.5 GB
+ Tempor√§re Tensoren: 1.0 GB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
= ~22.8 GB (19 √ó 1.20)
```

---

## üìä LARGE_MODELS Liste

Modelle ‚â• 5 GB werden als "gro√ü" betrachtet:

```python
LARGE_MODELS = {
    "mixtral:8x7b": 26 GB,      # Gr√∂√ütes Modell
    "qwen2.5:32b": 19 GB,
    "command-r": 18 GB,
    "qwen2.5:14b": 9 GB,
    "qwen3:8b": 5.2 GB,
    "llama3.1:8b": 5 GB
}
```

**Hinweis:** Kleine Modelle (< 5 GB) werden **nie** entladen, sondern bleiben parallel im RAM.

---

## üéØ Beispiele

### **Szenario 1: Kleines Modell**

```
Verf√ºgbar: 25.5 GB
Geladen: 0 GB
Neues Modell: qwen3:1.7b (1.4 GB)

Berechnung:
  Required: 1.4 GB √ó 1.20 = 1.68 GB
  Available: 25.5 GB > 1.68 GB ‚úÖ

Entscheidung: ‚úÖ Kein Entladen n√∂tig!
```

---

### **Szenario 2: Gro√ües Modell, genug RAM**

```
Verf√ºgbar: 25.5 GB
Geladen: qwen3:1.7b (2.0 GB)
Neues Modell: qwen2.5:14b (9.0 GB)

Berechnung:
  Required: 9.0 GB √ó 1.20 = 10.8 GB
  Available: 25.5 GB > 10.8 GB ‚úÖ

Entscheidung: ‚úÖ Kein Entladen n√∂tig!
  ‚Üí qwen3:1.7b BLEIBT im RAM
  ‚Üí qwen2.5:14b wird zus√§tzlich geladen
  ‚Üí Beide k√∂nnen parallel laufen!
```

---

### **Szenario 3: Sehr gro√ües Modell, RAM knapp**

```
Verf√ºgbar: 18.0 GB
Geladen: qwen3:8b (5.7 GB)
Neues Modell: qwen2.5:32b (19.0 GB)

Berechnung:
  Required: 19.0 GB √ó 1.20 = 22.8 GB
  Available: 18.0 GB < 22.8 GB ‚ùå

Entscheidung: ‚ö†Ô∏è Zu wenig RAM!
  ‚Üí qwen3:8b wird ENTLADEN (5.7 GB frei)
  ‚Üí RAM nach Entladen: ~23.6 GB
  ‚Üí qwen2.5:32b wird geladen (19 GB)
  ‚Üí Genug Platz! ‚úÖ Kein Swapping!
```

---

## üìù Log-Output

### **Genug RAM (kein Entladen):**

```
üìä Memory Check:
   Verf√ºgbar: 25.5 GB
   Geladen: 2.0 GB
   Neues Modell: qwen2.5:14b (9.0 GB)
‚úÖ Genug RAM! 25.5 GB > 10.8 GB (mit 20% Reserve)
   Kein Entladen n√∂tig - Modell passt rein!
```

### **Zu wenig RAM (Entladen):**

```
üìä Memory Check:
   Verf√ºgbar: 18.0 GB
   Geladen: 5.7 GB
   Neues Modell: qwen2.5:32b (19.0 GB)
‚ö†Ô∏è Zu wenig RAM! 18.0 GB < 22.8 GB (mit 20% Reserve)
üîÑ Gro√ües Modell: qwen2.5:32b (19.0 GB)
üßπ Entlade aktuell geladene Modelle (5.7 GB)...
   üóëÔ∏è qwen3:8b entladen
üßπ Alle Modelle aus RAM entladen
‚úÖ RAM nach Entladen: 23.6 GB verf√ºgbar
```

---

## üîß Wo wird es verwendet?

### **1. Normale Fragen (ohne Agent)**

`chat_audio_step2_ai()` - [aifred_intelligence.py:579](../aifred_intelligence.py#L579)

```python
# Smart Model Loading: Entlade kleine Modelle wenn gro√ües Modell kommt
smart_model_load(model_choice)

# Zeit messen
start_time = time.time()
response = ollama.chat(model=model_choice, messages=messages)
```

### **2. Agent-Recherche**

`perform_agent_research()` - [aifred_intelligence.py:1259](../aifred_intelligence.py#L1259)

```python
# Smart Model Loading: Entlade kleine Modelle wenn gro√ües Modell kommt
smart_model_load(model_choice)

inference_start = time.time()
response = ollama.chat(model=model_choice, messages=messages)
```

---

## ‚ö†Ô∏è Wichtig

### **Warum 20% Safety Margin?**

Ollama ben√∂tigt w√§hrend der Inferenz **mehr RAM** als nur die Model-Weights:
- Context Buffer (Prompt + History)
- KV Cache (Key-Value Cache f√ºr Attention)
- Tempor√§re Tensoren (Zwischenrechnungen)

**Ohne Safety Margin ‚Üí Swapping ‚Üí 10x langsamer!**

### **Warum nicht mehr als 20%?**

- 20% ist ein guter Kompromiss
- Zu viel Reserve ‚Üí Unn√∂tiges Entladen
- Zu wenig Reserve ‚Üí Risiko von Swapping

---

## üß™ Testing

**Test-Script:** `test_memory_logic.py` (wird nach Commit gel√∂scht)

**Manuelle Tests:**

```bash
# RAM-Status pr√ºfen
free -h

# Geladene Modelle pr√ºfen
ollama ps

# Modell manuell entladen
curl -X POST http://localhost:11434/api/generate \
  -d '{"model": "qwen3:8b", "keep_alive": 0}'
```

---

## üìå Hardware-Anforderungen

**Minimum:**
- 16 GB RAM f√ºr kleine Modelle (< 5 GB)
- 32 GB RAM f√ºr gro√üe Modelle (> 10 GB)

**Empfohlen:**
- 32 GB RAM (wie Aoostar GEM10)
- Erm√∂glicht paralleles Laden mehrerer Modelle

**Swap:**
- 8 GB Swap als Backup
- **Sollte normalerweise LEER sein!**
- Wenn Swap benutzt wird ‚Üí Performance-Problem!

---

## üîç Troubleshooting

### **Problem: Swap wird benutzt**

```bash
# Swap-Status pr√ºfen
free -h | grep Swap

# Swap leeren (tempor√§r)
sudo swapoff -a && sudo swapon -a
```

### **Problem: Modelle werden nicht entladen**

```bash
# Pr√ºfe Ollama-Logs
journalctl -u ollama.service -n 50

# Pr√ºfe AIfred-Logs
journalctl -u aifred-intelligence.service -n 100 | grep "Memory Check"
```

### **Problem: Performance schlecht trotz genug RAM**

```bash
# Pr√ºfe CPU-Auslastung
htop

# Pr√ºfe Disk I/O (sollte niedrig sein!)
iostat -x 1

# Wenn Disk I/O hoch ‚Üí Swapping aktiv!
```

---

## üìö Weiterf√ºhrende Links

- [Ollama API Dokumentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Linux Memory Management](https://www.kernel.org/doc/html/latest/admin-guide/mm/concepts.html)
- [Model Benchmarks](../benchmarks/BENCHMARK_QUALITY_ANALYSIS.md)

---

**Letzte Aktualisierung:** 2025-10-15
**Version:** 1.0
**Autor:** Claude Code
