# ğŸ¤– Detaillierter Modell-Vergleich fÃ¼r AIfred Intelligence

**Letzte Aktualisierung:** 2025-10-15 (basierend auf Web-Recherche + eigenen Benchmarks)

---

## ğŸ“Š SCHNELLÃœBERSICHT

| Modell | GrÃ¶ÃŸe | Geschwindigkeit | QualitÃ¤t | Beste fÃ¼r | RAM |
|--------|-------|----------------|----------|-----------|-----|
| **qwen3:1.7b** | 1.7B | ğŸ¥‡ Sehr schnell | â­â­â­â­ Gut | Automatik-Entscheidungen, Lightweight Tasks | 2-3 GB |
| **qwen3:4b** | 4B | ğŸ¥ˆ Schnell | â­â­â­â­ Gut | MittelgroÃŸe Tasks, Balance | 4-5 GB |
| **qwen3:8b** | 8B | ğŸ¥‰ Mittel | â­â­â­â­â­ Sehr gut | Finale Antworten, Reasoning | 8-10 GB |
| **qwen2.5:14b** | 14B | â±ï¸ Langsam | â­â­â­â­â­ Sehr gut | RAG, lange Dokumente | 14-16 GB |
| **qwen2.5:32b** | 32B | âŒ Sehr langsam | â­â­â­â­â­ Exzellent | Beste QualitÃ¤t, komplexe Aufgaben | 21-24 GB |
| **llama3.2:3b** | 3B | ğŸ¥‡ Sehr schnell | â­â­â­ OK | Edge Devices, Tool Use | 3-4 GB |
| **command-r** | 35B | â±ï¸ Langsam | â­â­â­â­â­ Exzellent | Enterprise RAG, Citations | 20-24 GB |
| **mixtral:8x7b** | 47B | ğŸ¥ˆ Mittel | â­â­â­â­ Gut | Multilingual, MoE | 26-30 GB |
| **mistral** | 7B | ğŸ¥‡ Schnell | â­â­â­â­ Gut | Coding, Effizienz | 7-8 GB |

---

## ğŸ† QWEN3 MODELLE (Empfohlen fÃ¼r AIfred!)

### **qwen3:1.7b** â­ **BESTE WAHL fÃ¼r Automatik**

**StÃ¤rken:**
- âœ… **Extrem schnell**: 7s pro URL-Bewertung (105s fÃ¼r 15 URLs)
- âœ… **ZuverlÃ¤ssig**: Alle Tests bestanden (Benchmark 2025-10-15)
- âœ… **Content-basiert**: Bewertet URLs nach Inhalt, nicht Domain
- âœ… **Edge-fÃ¤hig**: LÃ¤uft auf Standard-PCs und Apple Silicon
- âœ… **Thinking Mode**: Kann zwischen Quick & Deep Reasoning wechseln
- âœ… **32K Context**: Ausreichend fÃ¼r die meisten Tasks

**SchwÃ¤chen:**
- âš ï¸ Etwas weniger QualitÃ¤t als 8B bei komplexen Aufgaben
- âš ï¸ Kleinerer Context (32K) als 8B (128K)
- âš ï¸ Kann bei sehr komplexen Reasoning-Tasks kÃ¤mpfen

**Beste fÃ¼r:**
- ğŸ¯ Automatik-Entscheidung (Web-Recherche JA/NEIN?)
- ğŸ” Query-Optimierung (Keyword-Extraktion)
- ğŸ“Š URL-Bewertung (15 URLs in ~2 Min)
- ğŸ’¬ Einfache Chat-Antworten

**Benchmark-Ergebnisse (eigene Tests):**
- Automatik-Entscheidung: 3/3 korrekt âœ…
- Query-Optimierung: 9/10 Punkte (prÃ¤zise Keywords)
- URL-Bewertung: 8.5/10 Punkte (zu streng bei Wikipedia)
- Finale Antworten: 10/10 Punkte (keine Halluzinationen)

---

### **qwen3:4b**

**StÃ¤rken:**
- âœ… **Ãœberraschend stark**: Ãœbertrifft manche 72B-Modelle bei Programming
- âœ… **Balance**: Guter Kompromiss zwischen Speed & QualitÃ¤t
- âœ… **MultiIF**: 66.3 Punkte (respektabel fÃ¼r 4B Dense-Model)
- âœ… **32K Context**: FÃ¼r die meisten Tasks ausreichend

**SchwÃ¤chen:**
- âŒ **SEHR langsam in unserem Benchmark** (18 Min fÃ¼r 4 Tasks!)
- âš ï¸ Thinking Mode verursacht extreme Latenz (300+ Zeilen Reasoning)
- âš ï¸ Versagt bei Trump/Hamas Test (eigene Benchmarks)
- âš ï¸ Kleinerer Context als 8B

**Beste fÃ¼r:**
- âš ï¸ **NICHT empfohlen fÃ¼r AIfred** (zu langsam + unzuverlÃ¤ssig)
- Programming-Tasks (wenn Zeit keine Rolle spielt)

---

### **qwen3:8b** â­ **BESTE WAHL fÃ¼r Finale Antworten**

**StÃ¤rken:**
- âœ… **Beste Balance**: Speed & QualitÃ¤t optimal
- âœ… **128K Context**: Doppelt so viel wie 1.7b/4b
- âœ… **Sehr gutes Reasoning**: Ãœbertrifft qwen2.5-14b in STEM & Coding
- âœ… **Alle Tests bestanden**: 10/10 bei Automatik-Entscheidungen
- âœ… **Thinking Mode**: Optional fÃ¼r komplexe Probleme
- âœ… **16-24GB VRAM**: LÃ¤uft auf Standard-GPUs

**SchwÃ¤chen:**
- âš ï¸ 2.6x langsamer als 1.7b (~275s vs. 106s fÃ¼r 15 URLs)
- âš ï¸ Zu streng bei URL-Bewertung (Wikipedia = 1/10)
- âš ï¸ HÃ¶herer RAM-Bedarf (8-10 GB)

**Beste fÃ¼r:**
- ğŸ¯ **Finale Antwort-Generierung** (nach Web-Recherche)
- ğŸ§  Komplexe Reasoning-Tasks
- ğŸ“ Lange Texte mit viel Kontext
- ğŸ’» STEM & Coding

**Benchmark-Ergebnisse (eigene Tests):**
- Automatik-Entscheidung: 3/3 korrekt âœ…
- Query-Optimierung: 6/10 Punkte (zu verbose)
- URL-Bewertung: 8/10 Punkte (Tier 1 perfekt, Tier 2 zu streng)
- Finale Antworten: 10/10 Punkte (beste Formulierung)

---

## ğŸ“š QWEN2.5 MODELLE (RAG-Spezialist)

### **qwen2.5:14b**

**StÃ¤rken:**
- âœ… **RAG-Optimiert**: 100% Retrieval, 0% Training Recall
- âœ… **Coding**: Sehr stark bei Programming-Tasks
- âœ… **128K Context**: Lange Dokumente kein Problem
- âœ… **Balance**: Sweet Spot zwischen 8B und 32B

**SchwÃ¤chen:**
- âš ï¸ Langsamer als qwen3-Modelle
- âš ï¸ HÃ¶herer RAM-Bedarf (14-16 GB)
- âš ï¸ qwen3-14b ist mittlerweile besser

**Beste fÃ¼r:**
- ğŸ“„ RAG mit langen Dokumenten
- ğŸ’» Coding-Tasks
- ğŸ” Retrieval-intensive Anwendungen

---

### **qwen2.5:32b**

**StÃ¤rken:**
- âœ… **HÃ¶chste QualitÃ¤t**: Beste Nuancen-Erkennung
- âœ… **Kein Thinking Mode**: Direkte Antworten
- âœ… **Beste URL-Bewertung**: Nutzt volles 1-10 Spektrum
- âœ… **RAG-Performance**: Exzellent bei Retrieval

**SchwÃ¤chen:**
- âŒ **6.8x langsamer als 1.7b** (718s vs. 106s)
- âŒ **KRITISCHER FEHLER**: Versagt bei Trump/Hamas Test (eigene Benchmarks!)
- âŒ **21-24 GB RAM**: Nicht fÃ¼r alle Systeme geeignet
- âš ï¸ Unbrauchbar fÃ¼r Echtzeit-Aufgaben (12 Min fÃ¼r 15 URLs!)

**Beste fÃ¼r:**
- ğŸ¯ Offline-Analyse (wenn Zeit keine Rolle spielt)
- ğŸ“Š Batch-Verarbeitung
- âš ï¸ **NICHT fÃ¼r AIfred Automatik** (zu langsam!)

**Benchmark-Ergebnisse (eigene Tests):**
- Automatik-Entscheidung: 2/3 korrekt âŒ (Trump/Hamas failed!)
- Query-Optimierung: 9.3/10 Punkte (beste Keywords)
- URL-Bewertung: 9.5/10 Punkte (beste Nuancen)
- Finale Antworten: 9/10 Punkte (formal, kein Emoji)

---

## ğŸ¦™ LLAMA3.2 MODELLE

### **llama3.2:3b**

**StÃ¤rken:**
- âœ… **Extrem schnell**: 1.1s fÃ¼r Speed-Test (schnellster!)
- âœ… **Instruction Following**: 77.4 Punkte (Ã¼bertrifft Gemma 2B & Phi-3.5)
- âœ… **Tool Use**: 67.0 Punkte (BFCL V2)
- âœ… **Long Context**: 84.7 Punkte (NIH/Multi-needle)
- âœ… **Edge-optimiert**: LÃ¤uft auf Smartphones & Edge Devices
- âœ… **128K Context**: Trotz kleiner GrÃ¶ÃŸe

**SchwÃ¤chen:**
- âŒ **UnzuverlÃ¤ssig bei News**: Versagt bei Trump/Hamas UND Wetter!
- âŒ **0/3 kritische Tests** (eigene Benchmarks)
- âš ï¸ SchwÃ¤cher in MMLU (63.4) vs. Phi-3.5 (69.0)
- âš ï¸ Math: Deutlich schwÃ¤cher als Phi-3.5 (86.2)
- âš ï¸ Weniger KapazitÃ¤t fÃ¼r Allgemeinwissen (durch Pruning/Distillation)

**Beste fÃ¼r:**
- ğŸ“± Edge Devices / Mobile Apps
- ğŸ› ï¸ Tool Use & Function Calling
- ğŸ“ Instruction Following
- âš ï¸ **NICHT fÃ¼r Automatik-Entscheidungen!** (versteht News/Wetter-Trigger nicht)

**Benchmark-Ergebnisse (eigene Tests):**
- Automatik-Entscheidung: 1/3 korrekt âŒâŒ
- Trump/Hamas: FAILED âŒ
- Wetter: FAILED âŒ
- **Fazit**: Nicht zuverlÃ¤ssig fÃ¼r AIfred!

---

## ğŸ¢ COMMAND-R (Enterprise RAG)

### **command-r** (35B, Cohere)

**StÃ¤rken:**
- âœ… **RAG-Spezialist**: Beste RAG-Performance mit Grounding
- âœ… **Citations**: Automatische Quellenangaben in Antworten
- âœ… **Hallucination-Mitigation**: Durch Grounding & Citations
- âœ… **128K Context**: Lange Dokumente kein Problem
- âœ… **50% hÃ¶herer Throughput** (vs. alte Version)
- âœ… **20% niedrigere Latenz** (vs. alte Version)
- âœ… **Multilingual**: 10 Sprachen (inkl. Deutsch)

**Command A (2025, neueste Version):**
- ğŸ†• 111B Parameter
- ğŸ†• 256K Context!
- ğŸ†• 150% hÃ¶herer Throughput

**SchwÃ¤chen:**
- âš ï¸ Sehr groÃŸ (35B bzw. 111B)
- âš ï¸ Hoher RAM-Bedarf (20-24 GB bzw. mehr)
- âš ï¸ Langsamer als kleinere Modelle

**Beste fÃ¼r:**
- ğŸ¢ **Enterprise RAG** (lange Dokumente mit Citations)
- ğŸ“š Dokumenten-Analyse mit Quellenangaben
- ğŸŒ Multilinguales RAG (10 Sprachen)
- âœ… Wenn Genauigkeit & Citations wichtiger sind als Speed

---

## ğŸ­ MIXTRAL:8X7B (Mixture-of-Experts)

### **mixtral:8x7b** (47B Parameter, 12B aktiv)

**StÃ¤rken:**
- âœ… **MoE-Architektur**: 8 Experten, nur 2 aktiv pro Token
- âœ… **Schneller als 70B**: 6x faster als Llama2-70B
- âœ… **Multilingual**: Deutsch, FranzÃ¶sisch, Spanisch, Italienisch, Englisch
- âœ… **Code Generation**: Stark bei Programming
- âœ… **Weniger Bias**: Als Llama2 (BBQ Benchmark)
- âœ… **Kosteneffizient**: $0.70 pro 1M Tokens
- âœ… **Low Latency**: 0.36s TTFT (Time To First Token)

**SchwÃ¤chen:**
- âš ï¸ **Kleinerer Context**: Unter Durchschnitt
- âš ï¸ **Lower Intelligence**: Index 3 (Artificial Analysis)
- âš ï¸ **Output Speed**: 38.9 tokens/s (langsamer als Durchschnitt)
- âš ï¸ **GPU-Anforderungen**: Linux (NVIDIA/AMD), Windows (NVIDIA), nicht macOS

**Beste fÃ¼r:**
- ğŸŒ **Multilinguales RAG** (5 Sprachen)
- ğŸ’» **Code Generation**
- ğŸ’° **Cost-Performance Balance**
- âš ï¸ Nicht fÃ¼r macOS

---

## âš¡ MISTRAL (7B, Effizienz-Champion)

### **mistral** (7B)

**StÃ¤rken:**
- âœ… **Coding**: Fast auf Niveau von CodeLlama 7B
- âœ… **Effizienz**: Stark ohne massive Rechenpower
- âœ… **Speed**: Schnelle Responses, kosteneffektiv
- âœ… **Reasoning**: Ãœbertrifft Llama2-13B
- âœ… **Kreativ**: Hash-Map AnsÃ¤tze bei Problem-Solving
- âœ… **Local-fÃ¤hig**: LÃ¤uft smooth auf lokalen Maschinen

**SchwÃ¤chen:**
- âš ï¸ **Kleinere GrÃ¶ÃŸe**: 7B = weniger Tiefe bei komplexem Reasoning
- âš ï¸ **Knowledge Cutoff**: Limitiertes Wissen durch Training Cutoff
- âš ï¸ **Response Complexity**: Manchmal zu komplex formuliert
- âš ï¸ **Nuance**: Weniger Detail als grÃ¶ÃŸere Modelle
- âš ï¸ **Code vs. Gemma**: Gemma 7B besser bei Code & Math

**Beste fÃ¼r:**
- ğŸ’» **Coding Companion** (wenn Effizienz wichtig ist)
- ğŸ“ **Summarization**
- ğŸš€ **Edge Devices** (lokal, schnell)
- âš ï¸ Nicht fÃ¼r hochkomplexe Reasoning-Tasks

---

## ğŸ¯ EMPFEHLUNGEN FÃœR AIFRED INTELLIGENCE

### **ğŸ¥‡ AUTOMATIK-MODELL (Entscheidungen, Query-Opt, URL-Rating):**

**1. Wahl: qwen3:1.7b** â­
- Schnellste (7s/URL)
- ZuverlÃ¤ssig (alle Tests bestanden)
- Content-basierte Bewertung

**2. Wahl: qwen3:8b**
- Beste QualitÃ¤t
- 2.6x langsamer
- Nur wenn Zeit keine Rolle spielt

**âŒ NICHT empfohlen:**
- llama3.2:3b (versagt bei News/Wetter)
- qwen3:4b (zu langsam)
- qwen2.5:32b (viel zu langsam fÃ¼r Echtzeit)

---

### **ğŸ¥‡ HAUPT-LLM (Finale Antwort):**

**1. Wahl: qwen3:8b** â­
- Beste Balance Speed & QualitÃ¤t
- 128K Context
- Sehr gutes Reasoning

**2. Wahl: qwen2.5:14b**
- RAG-Spezialist
- Lange Dokumente
- Coding

**3. Wahl: command-r**
- Enterprise RAG
- Citations
- Wenn Genauigkeit > Speed

**FÃ¼r Speed: qwen3:1.7b**
- Wenn Geschwindigkeit wichtiger als QualitÃ¤t

**FÃ¼r maximale QualitÃ¤t: qwen2.5:32b**
- Nur wenn RAM verfÃ¼gbar (21+ GB)
- Nur wenn Zeit keine Rolle spielt

---

## ğŸ“Š ZUSAMMENFASSUNG: QWEN vs. LLAMA

**QWEN-Modelle (Empfohlen!):**
- âœ… Besser bei Automatik-Entscheidungen
- âœ… ZuverlÃ¤ssiger bei News/Wetter-Erkennung
- âœ… Thinking Mode (optional)
- âœ… qwen3 Ã¼bertrifft qwen2.5 trotz kleinerer GrÃ¶ÃŸe
- âœ… Content-basierte URL-Bewertung

**LLAMA-Modelle:**
- âœ… Gut fÃ¼r Edge Devices
- âœ… Tool Use & Instruction Following
- âŒ UnzuverlÃ¤ssig bei komplexen Entscheidungen
- âŒ Versagt bei News/Wetter-Trigger

**Fazit:** FÃ¼r AIfred sind **Qwen-Modelle** die bessere Wahl! ğŸ¯
