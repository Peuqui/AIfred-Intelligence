#!/bin/bash

echo "üöÄ AIfred Intelligence - vLLM Model Download (AWQ Quantization)"
echo "================================================================"
echo ""
echo "‚ö†Ô∏è  Diese Modelle werden von HuggingFace heruntergeladen"
echo "‚úÖ Optimiert f√ºr P40 24GB VRAM mit YaRN Context Extension Support"
echo ""

# ============================================================
# üéØ QWEN3 AWQ MODELS (Neueste Generation, Optional Thinking)
# ============================================================
echo "üéØ Qwen3 AWQ Models (Recommended)"
echo "----------------------------"
echo "‚úÖ Native 32K-40K context, erweiterbar bis 128K mit YaRN"
echo "‚úÖ Optional Thinking Mode (enable_thinking=True/False)"
echo "‚úÖ Beste Performance mit AWQ Marlin kernel"
echo ""

# Model 1: Qwen3-4B-AWQ
echo ""
echo "üì¶ Qwen/Qwen3-4B-AWQ"
echo "   Gr√∂√üe: ~2.5 GB"
echo "   Context: 40K native (YaRN‚Üí128K)"
echo "   Use Case: Testing/Experiments"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "‚¨áÔ∏è  Downloading: Qwen/Qwen3-4B-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen3-4B-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'‚úÖ Downloaded to: {path}')
"
fi

# Model 2: Qwen3-8B-AWQ
echo ""
echo "üì¶ Qwen/Qwen3-8B-AWQ"
echo "   Gr√∂√üe: ~5 GB"
echo "   Context: 40K native (YaRN‚Üí128K)"
echo "   Use Case: Main LLM (empfohlen)"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "‚¨áÔ∏è  Downloading: Qwen/Qwen3-8B-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen3-8B-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'‚úÖ Downloaded to: {path}')
"
fi

# Model 3: Qwen3-14B-AWQ
echo ""
echo "üì¶ Qwen/Qwen3-14B-AWQ"
echo "   Gr√∂√üe: ~8 GB"
echo "   Context: 32K native (YaRN‚Üí128K)"
echo "   Use Case: High Quality (beste Balance)"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "‚¨áÔ∏è  Downloading: Qwen/Qwen3-14B-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen3-14B-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'‚úÖ Downloaded to: {path}')
"
fi

# Model 4: Qwen3-32B-AWQ (optional)
echo ""
echo "üì¶ Qwen/Qwen3-32B-AWQ (Optional)"
echo "   Gr√∂√üe: ~18 GB"
echo "   Context: 32K native (YaRN‚Üí128K)"
echo "   Use Case: Maximum Performance"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "‚¨áÔ∏è  Downloading: Qwen/Qwen3-32B-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen3-32B-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'‚úÖ Downloaded to: {path}')
"
fi

# ============================================================
# üì¶ QWEN2.5 AWQ MODELS (128K Native Context)
# ============================================================
echo ""
echo "üì¶ Qwen2.5 AWQ Models (Alternative mit 128K native)"
echo "----------------------------"
echo "‚úÖ Native 128K context ohne YaRN"
echo "‚úÖ Optional Thinking Mode"
echo "‚ö†Ô∏è  √Ñltere Generation als Qwen3"
echo ""

# Model 1: Qwen2.5-7B-Instruct-AWQ
echo ""
echo "üì¶ Qwen/Qwen2.5-7B-Instruct-AWQ"
echo "   Gr√∂√üe: ~4 GB"
echo "   Context: 128K native (kein YaRN n√∂tig)"
echo "   Use Case: Balanced (√§ltere Generation)"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "‚¨áÔ∏è  Downloading: Qwen/Qwen2.5-7B-Instruct-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen2.5-7B-Instruct-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'‚úÖ Downloaded to: {path}')
"
fi

# Model 2: Qwen2.5-14B-Instruct-AWQ
echo ""
echo "üì¶ Qwen/Qwen2.5-14B-Instruct-AWQ"
echo "   Gr√∂√üe: ~8 GB"
echo "   Context: 128K native (kein YaRN n√∂tig)"
echo "   Use Case: High Quality (√§ltere Generation)"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "‚¨áÔ∏è  Downloading: Qwen/Qwen2.5-14B-Instruct-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen2.5-14B-Instruct-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'‚úÖ Downloaded to: {path}')
"
fi

# Model 3: Qwen2.5-32B-Instruct-AWQ (optional)
echo ""
echo "üì¶ Qwen/Qwen2.5-32B-Instruct-AWQ (Optional)"
echo "   Gr√∂√üe: ~18 GB"
echo "   Context: 128K native (kein YaRN n√∂tig)"
echo "   Use Case: Maximum Performance (√§ltere Generation)"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "‚¨áÔ∏è  Downloading: Qwen/Qwen2.5-32B-Instruct-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen2.5-32B-Instruct-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'‚úÖ Downloaded to: {path}')
"
fi

# ============================================================
# üìù SUMMARY & YARN CONFIGURATION
# ============================================================
echo ""
echo "================================================================"
echo "üéâ Download abgeschlossen!"
echo ""
echo "üìä vLLM Model Configuration:"
echo ""
echo "üîπ Qwen3 AWQ Series (Empfohlen):"
echo "   - Qwen3-4B-AWQ (~2.5GB, 40K native, YaRN‚Üí128K)"
echo "   - Qwen3-8B-AWQ (~5GB, 40K native, YaRN‚Üí128K)"
echo "   - Qwen3-14B-AWQ (~8GB, 32K native, YaRN‚Üí128K)"
echo "   - Optional Thinking Mode: enable_thinking=True/False"
echo ""
echo "üîπ Qwen2.5 Instruct-AWQ Series (Alternative):"
echo "   - Qwen2.5-7B-Instruct-AWQ (~4GB, 128K native)"
echo "   - Qwen2.5-14B-Instruct-AWQ (~8GB, 128K native)"
echo "   - Kein YaRN n√∂tig (bereits 128K)"
echo ""
echo "üßÆ YaRN Context Extension (f√ºr Qwen3):"
echo "   - Native: 32K-40K tokens"
echo "   - Mit YaRN factor=2.0: 64K tokens (empfohlen f√ºr Chat-Historie)"
echo "   - Mit YaRN factor=4.0: 128K tokens (f√ºr lange Dokumente)"
echo ""
echo "üìù vLLM Startup mit YaRN (64K Beispiel):"
echo "   ./venv/bin/vllm serve Qwen/Qwen3-14B-AWQ \\"
echo "     --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":2.0,\"original_max_position_embeddings\":32768}' \\"
echo "     --max-model-len 65536"
echo ""
echo "üí° P40 (24GB VRAM) Empfehlung:"
echo "   - Qwen3-8B-AWQ + YaRN factor=2.0 (64K): ~5GB VRAM + Schnell"
echo "   - Qwen3-14B-AWQ + YaRN factor=2.0 (64K): ~8GB VRAM + Beste Qualit√§t"
echo "   - Qwen2.5-14B-Instruct-AWQ (128K native): ~8GB VRAM + Kein YaRN n√∂tig"
echo ""
echo "‚úÖ vLLM Models bereit!"
echo "================================================================"
