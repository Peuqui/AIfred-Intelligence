#!/bin/bash

echo "ðŸš€ AIfred Intelligence - vLLM Model Download (AWQ Quantization)"
echo "================================================================"
echo ""

# ============================================================
# ðŸ” GPU COMPATIBILITY CHECK
# ============================================================
echo "ðŸ” GPU Compatibility Check"
echo "----------------------------"

# Check if nvidia-smi is available
if ! command -v nvidia-smi &> /dev/null; then
    echo "âš ï¸  WARNING: nvidia-smi not found"
    echo "   Cannot detect GPU - proceeding without check"
    echo ""
else
    # Get GPU name and compute capability
    GPU_INFO=$(nvidia-smi --query-gpu=name,compute_cap --format=csv,noheader,nounits 2>/dev/null | head -1)

    if [ -n "$GPU_INFO" ]; then
        GPU_NAME=$(echo "$GPU_INFO" | cut -d',' -f1 | xargs)
        COMPUTE_CAP=$(echo "$GPU_INFO" | cut -d',' -f2 | xargs)

        echo "âœ… Detected GPU: $GPU_NAME"
        echo "   Compute Capability: $COMPUTE_CAP"
        echo ""

        # Check for known incompatible GPUs
        if [[ "$GPU_NAME" == *"P40"* ]] || [[ "$GPU_NAME" == *"P4 "* ]] || [[ "$COMPUTE_CAP" < "7.0" ]]; then
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo "âŒ INCOMPATIBLE GPU DETECTED!"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo ""
            echo "Your GPU: $GPU_NAME (Compute Capability $COMPUTE_CAP)"
            echo ""
            echo "âš ï¸  vLLM/AWQ REQUIREMENTS:"
            echo "   â€¢ Minimum Compute Capability: 7.5 (Turing)"
            echo "   â€¢ Your GPU has: $COMPUTE_CAP (Pascal/Volta)"
            echo "   â€¢ AWQ requires fast FP16 (unavailable on Pascal)"
            echo ""
            echo "ðŸ“Š KNOWN ISSUES:"
            if [[ "$GPU_NAME" == *"P40"* ]]; then
                echo "   â€¢ Tesla P40: FP16 ratio 1:64 (extremely slow)"
                echo "   â€¢ ExLlamaV2/vLLM: ~1-5 tok/s (unusable)"
                echo "   â€¢ Triton compiler: Not supported on Pascal"
            elif [[ "$GPU_NAME" == *"P100"* ]]; then
                echo "   â€¢ Tesla P100: Moderate FP16, but still slow"
                echo "   â€¢ vLLM performance: Suboptimal"
            fi
            echo ""
            echo "âœ… RECOMMENDED ALTERNATIVE:"
            echo "   Use Ollama with GGUF models instead!"
            echo "   â€¢ Better performance on Pascal GPUs"
            echo "   â€¢ INT8/Q4/Q8 quantization (no FP16 bottleneck)"
            echo "   â€¢ Script: ./download_ollama_models.sh"
            echo ""
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo ""
            read -p "Continue anyway? (NOT RECOMMENDED) (y/n) " -n 1 -r
            echo
            if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                echo "Aborted. Please use ./download_ollama_models.sh instead."
                exit 0
            fi
            echo ""
            echo "âš ï¸  Proceeding at your own risk..."
            echo ""
        else
            echo "âœ… GPU is compatible with vLLM/AWQ"
            echo ""
        fi
    else
        echo "âš ï¸  Could not detect GPU information"
        echo ""
    fi
fi

echo "âš ï¸  Diese Modelle werden von HuggingFace heruntergeladen"
echo "âœ… Optimiert fÃ¼r Ampere/Ada GPUs (RTX 30/40 series, A100, etc.)"
echo ""

# ============================================================
# ðŸŽ¯ QWEN3 AWQ MODELS (Neueste Generation, Optional Thinking)
# ============================================================
echo "ðŸŽ¯ Qwen3 AWQ Models (Recommended)"
echo "----------------------------"
echo "âœ… Native 32K-40K context, erweiterbar bis 128K mit YaRN"
echo "âœ… Optional Thinking Mode (enable_thinking=True/False)"
echo "âœ… Beste Performance mit AWQ Marlin kernel"
echo ""

# Model 1: Qwen3-4B-AWQ
echo ""
echo "ðŸ“¦ Qwen/Qwen3-4B-AWQ"
echo "   GrÃ¶ÃŸe: ~2.5 GB"
echo "   Context: 40K native (YaRNâ†’128K)"
echo "   Use Case: Testing/Experiments"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "â¬‡ï¸  Downloading: Qwen/Qwen3-4B-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen3-4B-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'âœ… Downloaded to: {path}')
"
fi

# Model 2: Qwen3-8B-AWQ
echo ""
echo "ðŸ“¦ Qwen/Qwen3-8B-AWQ"
echo "   GrÃ¶ÃŸe: ~5 GB"
echo "   Context: 40K native (YaRNâ†’128K)"
echo "   Use Case: Main LLM (empfohlen)"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "â¬‡ï¸  Downloading: Qwen/Qwen3-8B-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen3-8B-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'âœ… Downloaded to: {path}')
"
fi

# Model 3: Qwen3-14B-AWQ
echo ""
echo "ðŸ“¦ Qwen/Qwen3-14B-AWQ"
echo "   GrÃ¶ÃŸe: ~8 GB"
echo "   Context: 32K native (YaRNâ†’128K)"
echo "   Use Case: High Quality (beste Balance)"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "â¬‡ï¸  Downloading: Qwen/Qwen3-14B-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen3-14B-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'âœ… Downloaded to: {path}')
"
fi

# Model 4: Qwen3-32B-AWQ (optional)
echo ""
echo "ðŸ“¦ Qwen/Qwen3-32B-AWQ (Optional)"
echo "   GrÃ¶ÃŸe: ~18 GB"
echo "   Context: 32K native (YaRNâ†’128K)"
echo "   Use Case: Maximum Performance"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "â¬‡ï¸  Downloading: Qwen/Qwen3-32B-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen3-32B-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'âœ… Downloaded to: {path}')
"
fi

# ============================================================
# ðŸ“¦ QWEN2.5 AWQ MODELS (128K Native Context)
# ============================================================
echo ""
echo "ðŸ“¦ Qwen2.5 AWQ Models (Alternative mit 128K native)"
echo "----------------------------"
echo "âœ… Native 128K context ohne YaRN"
echo "âœ… Optional Thinking Mode"
echo "âš ï¸  Ã„ltere Generation als Qwen3"
echo ""

# Model 1: Qwen2.5-7B-Instruct-AWQ
echo ""
echo "ðŸ“¦ Qwen/Qwen2.5-7B-Instruct-AWQ"
echo "   GrÃ¶ÃŸe: ~4 GB"
echo "   Context: 128K native (kein YaRN nÃ¶tig)"
echo "   Use Case: Balanced (Ã¤ltere Generation)"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "â¬‡ï¸  Downloading: Qwen/Qwen2.5-7B-Instruct-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen2.5-7B-Instruct-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'âœ… Downloaded to: {path}')
"
fi

# Model 2: Qwen2.5-14B-Instruct-AWQ
echo ""
echo "ðŸ“¦ Qwen/Qwen2.5-14B-Instruct-AWQ"
echo "   GrÃ¶ÃŸe: ~8 GB"
echo "   Context: 128K native (kein YaRN nÃ¶tig)"
echo "   Use Case: High Quality (Ã¤ltere Generation)"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "â¬‡ï¸  Downloading: Qwen/Qwen2.5-14B-Instruct-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen2.5-14B-Instruct-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'âœ… Downloaded to: {path}')
"
fi

# Model 3: Qwen2.5-32B-Instruct-AWQ (optional)
echo ""
echo "ðŸ“¦ Qwen/Qwen2.5-32B-Instruct-AWQ (Optional)"
echo "   GrÃ¶ÃŸe: ~18 GB"
echo "   Context: 128K native (kein YaRN nÃ¶tig)"
echo "   Use Case: Maximum Performance (Ã¤ltere Generation)"
read -p "Herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "â¬‡ï¸  Downloading: Qwen/Qwen2.5-32B-Instruct-AWQ"
    echo "----------------------------------------"
    ./venv/bin/python3 -c "
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
path = snapshot_download(
    repo_id='Qwen/Qwen2.5-32B-Instruct-AWQ',
    cache_dir=cache_dir,
    resume_download=True,
    local_files_only=False
)
print(f'âœ… Downloaded to: {path}')
"
fi

# ============================================================
# ðŸ“ SUMMARY & YARN CONFIGURATION
# ============================================================
echo ""
echo "================================================================"
echo "ðŸŽ‰ Download abgeschlossen!"
echo ""
echo "ðŸ“Š vLLM Model Configuration:"
echo ""
echo "ðŸ”¹ Qwen3 AWQ Series (Empfohlen):"
echo "   - Qwen3-4B-AWQ (~2.5GB, 40K native, YaRNâ†’128K)"
echo "   - Qwen3-8B-AWQ (~5GB, 40K native, YaRNâ†’128K)"
echo "   - Qwen3-14B-AWQ (~8GB, 32K native, YaRNâ†’128K)"
echo "   - Optional Thinking Mode: enable_thinking=True/False"
echo ""
echo "ðŸ”¹ Qwen2.5 Instruct-AWQ Series (Alternative):"
echo "   - Qwen2.5-7B-Instruct-AWQ (~4GB, 128K native)"
echo "   - Qwen2.5-14B-Instruct-AWQ (~8GB, 128K native)"
echo "   - Kein YaRN nÃ¶tig (bereits 128K)"
echo ""
echo "ðŸ§® YaRN Context Extension (fÃ¼r Qwen3):"
echo "   - Native: 32K-40K tokens"
echo "   - Mit YaRN factor=2.0: 64K tokens (empfohlen fÃ¼r Chat-Historie)"
echo "   - Mit YaRN factor=4.0: 128K tokens (fÃ¼r lange Dokumente)"
echo ""
echo "ðŸ“ vLLM Startup mit YaRN (64K Beispiel):"
echo "   ./venv/bin/vllm serve Qwen/Qwen3-14B-AWQ \\"
echo "     --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":2.0,\"original_max_position_embeddings\":32768}' \\"
echo "     --max-model-len 65536"
echo ""
echo "ðŸ’¡ P40 (24GB VRAM) Empfehlung:"
echo "   - Qwen3-8B-AWQ + YaRN factor=2.0 (64K): ~5GB VRAM + Schnell"
echo "   - Qwen3-14B-AWQ + YaRN factor=2.0 (64K): ~8GB VRAM + Beste QualitÃ¤t"
echo "   - Qwen2.5-14B-Instruct-AWQ (128K native): ~8GB VRAM + Kein YaRN nÃ¶tig"
echo ""
echo "âœ… vLLM Models bereit!"
echo "================================================================"
