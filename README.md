# ğŸ¤– AIfred Intelligence - Advanced AI Assistant

**Production-Ready AI Assistant with Multi-LLM Support, Web Research & Voice Interface**

AIfred Intelligence ist ein fortschrittlicher KI-Assistent mit automatischer Web-Recherche, Multi-Model-Support und History-Kompression fÃ¼r unbegrenzte Konversationen.

---

## âœ¨ Features

### ğŸ¯ Core Features
- **Multi-LLM Support**: Ollama Backend mit verschiedenen Modellen (Qwen, Phi3, etc.)
- **Automatische Web-Recherche**: KI entscheidet selbst wann Recherche nÃ¶tig ist
- **History Compression**: Intelligente Kompression bei 70% Context-Auslastung
- **Voice Interface**: Speech-to-Text und Text-to-Speech Integration
- **Vector Cache**: ChromaDB-basierter Semantic Cache fÃ¼r Web-Recherchen (Docker)

### ğŸ”§ Technical Highlights
- **Reflex Framework**: React-Frontend aus Python generiert
- **WebSocket Streaming**: Echtzeit-Updates ohne Polling
- **Adaptive Temperature**: KI wÃ¤hlt Temperature basierend auf Fragetyp
- **Token Management**: Dynamische Context-Window-Berechnung
- **Debug Console**: Umfangreiches Logging und Monitoring
- **ChromaDB Server Mode**: Thread-safe Vector DB via Docker (0.0 distance fÃ¼r exakte Matches)

---

## ğŸš€ Installation

### Voraussetzungen
- Python 3.10+
- Ollama (fÃ¼r LLM Backend)
- 8GB+ RAM empfohlen

### Setup

1. **Repository klonen**:
```bash
git clone https://github.com/yourusername/AIfred-Intelligence.git
cd AIfred-Intelligence
```

2. **Virtual Environment erstellen**:
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate     # Windows
```

3. **Dependencies installieren**:
```bash
pip install -r requirements.txt
```

4. **Umgebungsvariablen** (.env):
```env
# API Keys fÃ¼r Web-Recherche
BRAVE_API_KEY=your_key_here
TAVILY_API_KEY=your_key_here

# Ollama Konfiguration
OLLAMA_BASE_URL=http://localhost:11434
```

5. **Ollama Models installieren**:
```bash
ollama pull qwen3:8b        # Haupt-LLM
ollama pull qwen2.5:3b      # Automatik-LLM
ollama pull phi3:mini       # Backup/Test
```

6. **ChromaDB Vector Cache starten** (Docker):
```bash
cd docker
docker-compose up -d chromadb
cd ..
```

**Optional: SearXNG auch starten** (lokale Suchmaschine):
```bash
cd docker
docker-compose --profile full up -d
cd ..
```

**ChromaDB Cache zurÃ¼cksetzen** (bei Bedarf):

*Option 1: Kompletter Neustart (lÃ¶scht alle Daten)*
```bash
cd docker
docker-compose stop chromadb
cd ..
rm -rf aifred_vector_cache/
cd docker
docker-compose up -d chromadb
cd ..
```

*Option 2: Nur Collection lÃ¶schen (wÃ¤hrend Container lÃ¤uft)*
```bash
./venv/bin/python -c "
import chromadb
from chromadb.config import Settings

client = chromadb.HttpClient(
    host='localhost',
    port=8000,
    settings=Settings(anonymized_telemetry=False)
)

try:
    client.delete_collection('research_cache')
    print('âœ… Collection gelÃ¶scht')
except Exception as e:
    print(f'âš ï¸ Fehler: {e}')
"
```

7. **Starten**:
```bash
reflex run
```

Die App lÃ¤uft dann unter: http://localhost:3002

---

## ğŸ—ï¸ Architektur

### Directory Structure
```
AIfred-Intelligence/
â”œâ”€â”€ aifred/
â”‚   â”œâ”€â”€ backends/          # LLM Backend Adapters
â”‚   â”œâ”€â”€ components/        # Reflex UI Components
â”‚   â”œâ”€â”€ lib/              # Core Libraries
â”‚   â”‚   â”œâ”€â”€ agent_core.py     # Haupt-Agent-Logik
â”‚   â”‚   â”œâ”€â”€ context_manager.py # History-Kompression
â”‚   â”‚   â”œâ”€â”€ config.py         # Konfiguration
â”‚   â”‚   â””â”€â”€ cache.py         # Cache-System
â”‚   â””â”€â”€ state.py          # Reflex State Management
â”œâ”€â”€ prompts/              # System Prompts
â”œâ”€â”€ logs/                 # Debug Logs
â””â”€â”€ docs/                # Dokumentation
```

### History Compression System

Bei 70% Context-Auslastung werden automatisch Ã¤ltere Konversationen komprimiert:

- **Trigger**: 70% des Context Windows belegt
- **Kompression**: 3 Frage-Antwort-Paare â†’ 1 Summary
- **Effizienz**: ~6:1 Kompressionsrate
- **FIFO**: Maximal 10 Summaries (Ã¤lteste werden gelÃ¶scht)
- **Safety**: Mindestens 1 aktuelle Konversation bleibt sichtbar

### Vector Cache & RAG System

AIfred nutzt ein mehrstufiges Cache-System basierend auf semantischer Ã„hnlichkeit (Cosine Distance):

#### Cache-Entscheidungs-Logik

**Phase 1a: Direct Cache Hit Check**
```
User Query â†’ ChromaDB Similarity Search
â”œâ”€ Distance < 0.5 (HIGH Confidence)
â”‚  â”œâ”€ Cache Age < 5min â†’ âœ… Use Cached Answer (Session Cache)
â”‚  â””â”€ Cache Age â‰¥ 5min â†’ âŒ Cache Outdated â†’ Web Research
â”œâ”€ Distance 0.5-1.2 (MEDIUM Confidence) â†’ Continue to Phase 1b (RAG)
â””â”€ Distance > 1.2 (LOW Confidence) â†’ Continue to Phase 2 (Research Decision)
```

**Phase 1b: RAG Context Check** (NEW in v1.2.0)
```
Cache Miss (d â‰¥ 0.5) â†’ Query for RAG Candidates (0.5 â‰¤ d < 1.2)
â”œâ”€ Found RAG Candidates?
â”‚  â”œâ”€ YES â†’ Automatik-LLM checks relevance for each candidate
â”‚  â”‚   â”œâ”€ Relevant (semantic match) â†’ Inject as System Message Context
â”‚  â”‚   â”‚   Example: "Python" â†’ "FastAPI" âœ… (FastAPI is Python framework)
â”‚  â”‚   â””â”€ Not Relevant â†’ Skip
â”‚  â”‚       Example: "Python" â†’ "Weather" âŒ (no connection)
â”‚  â””â”€ NO â†’ Continue to Phase 2
â””â”€ LLM Answer with RAG Context (Source: "Cache+LLM (RAG)")
```

**Phase 2: Research Decision**
```
No Direct Cache Hit & No RAG Context
â””â”€ Automatik-LLM decides: Web Research needed?
   â”œâ”€ YES â†’ Web Research + Cache Result
   â””â”€ NO  â†’ Pure LLM Answer (Source: "LLM-Trainingsdaten")
```

#### Cache Distance Thresholds

| Distance | Confidence | Behavior | Example |
|----------|-----------|----------|---------|
| `0.0 - 0.1` | VERY HIGH | Exact match (if age < 5min) | Identical query |
| `0.1 - 0.5` | HIGH | Direct cache hit (if age < 5min) | "Python tutorial" vs "Python Anleitung" |
| `0.5 - 1.2` | MEDIUM | RAG candidate (relevance check via LLM) | "Python" vs "FastAPI" |
| `1.2+` | LOW | Cache miss â†’ Research decision | "Python" vs "Weather" |

#### Cache Freshness (TTL Logic)

**Duplicate Detection**:
- Cache entries with `distance < 0.5` and `age < 5min` are considered recent duplicates
- **Recent duplicates**: Answer from cache, skip new save
- **Old duplicates** (age â‰¥ 5min): Perform web research, save new answer

**Rationale**:
- 5-minute threshold prevents stale data for volatile queries
- Old cache entries are refreshed automatically on re-query
- RAG mode provides context from older related searches

#### RAG (Retrieval-Augmented Generation) Mode

**How it works**:
1. Query finds related cache entries (distance 0.5-1.2)
2. Automatik-LLM checks if cached content is relevant to current question
3. Relevant entries are injected as system message: "Previous research shows..."
4. Main LLM combines cached context + training knowledge for enhanced answer

**Example Flow**:
```
User: "Was ist Python?" â†’ Web Research â†’ Cache Entry 1 (d=0.0)
User: "Was ist FastAPI?" â†’ RAG finds Entry 1 (d=0.7)
  â†’ LLM checks: "Python" relevant for "FastAPI"? YES (FastAPI uses Python)
  â†’ Inject Entry 1 as context â†’ Enhanced LLM answer
  â†’ Source: "Cache+LLM (RAG)"
```

**Benefits**:
- Leverages related past research without exact cache hits
- Avoids false context (LLM filters irrelevant entries)
- Multi-level context awareness (cache + conversation history)

#### Configuration

Cache behavior in `aifred/lib/config.py`:

```python
# Cache Distance Thresholds
CACHE_DISTANCE_DUPLICATE = 0.5   # < 0.5 = potential cache hit
CACHE_DISTANCE_MEDIUM = 0.5      # 0.5-1.2 = RAG range
CACHE_DISTANCE_RAG = 1.2         # < 1.2 = similar enough for RAG context

# Time Thresholds
CACHE_TIME_THRESHOLD = 300       # 5 minutes (in seconds)
```

**RAG Relevance Check**: Uses Automatik-LLM with dedicated prompt (`prompts/de/rag_relevance_check.txt`)

---

## ğŸ”§ Konfiguration

Alle wichtigen Parameter in `aifred/lib/config.py`:

```python
# Deployment Mode (Production vs Development)
USE_SYSTEMD_RESTART = True  # True fÃ¼r Production, False fÃ¼r Development

# History Compression
HISTORY_COMPRESSION_THRESHOLD = 0.7  # 70% Context
HISTORY_MESSAGES_TO_COMPRESS = 6     # 3 Q&A Paare
HISTORY_MIN_MESSAGES_BEFORE_COMPRESSION = 10

# LLM Settings
LLM_MAIN_MODEL = "qwen3:8b"
LLM_AUTOMATIK_MODEL = "qwen2.5:3b"

# Temperature Presets
TEMPERATURE_PRESETS = {
    "faktisch": 0.2,
    "gemischt": 0.5,
    "kreativ": 0.8
}
```

### HTTP Timeout Konfiguration

In `aifred/backends/ollama.py`:
- **HTTP Client Timeout**: 300 Sekunden (5 Minuten)
- ErhÃ¶ht von 60s fÃ¼r groÃŸe Research-Anfragen mit 30KB+ Context
- Verhindert Timeout-Fehler bei erster Token-Generation

### Restart-Button Verhalten

Der AIfred Restart-Button kann in zwei Modi arbeiten:

- **Production Mode** (`USE_SYSTEMD_RESTART = True`):
  - Startet den kompletten systemd-Service neu
  - BenÃ¶tigt Polkit-Regel fÃ¼r sudo-lose AusfÃ¼hrung
  - FÃ¼r produktive Systeme mit systemd

- **Development Mode** (`USE_SYSTEMD_RESTART = False`):
  - Soft-Restart: LÃ¶scht nur Caches und History
  - BehÃ¤lt laufende Instanz fÃ¼r Hot-Reload
  - FÃ¼r lokale Entwicklung ohne Service

---

## ğŸ“¦ Deployment

### Systemd Service

FÃ¼r produktiven Betrieb als Service sind vorkonfigurierte Service-Dateien im `systemd/` Verzeichnis verfÃ¼gbar.

**âš ï¸ WICHTIG**: Die Umgebungsvariable `AIFRED_ENV=prod` **MUSS** gesetzt sein, damit AIfred auf dem MiniPC lÃ¤uft und nicht auf den Entwicklungsrechner weiterleitet!

#### Schnellinstallation

```bash
# 1. Service-Dateien kopieren
sudo cp systemd/aifred-chromadb.service /etc/systemd/system/
sudo cp systemd/aifred-intelligence.service /etc/systemd/system/

# 2. Services aktivieren und starten
sudo systemctl daemon-reload
sudo systemctl enable aifred-chromadb.service aifred-intelligence.service
sudo systemctl start aifred-chromadb.service aifred-intelligence.service

# 3. Status prÃ¼fen
systemctl status aifred-chromadb.service
systemctl status aifred-intelligence.service
```

Siehe [systemd/README.md](systemd/README.md) fÃ¼r Details, Troubleshooting und Monitoring.

#### Service-Dateien (Referenz)

**1. ChromaDB Service** (`systemd/aifred-chromadb.service`):
```ini
[Unit]
Description=AIfred ChromaDB Vector Cache (Docker)
After=docker.service
Requires=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
WorkingDirectory=/home/mp/Projekte/AIfred-Intelligence/docker
ExecStart=/usr/bin/docker compose up -d chromadb
ExecStop=/usr/bin/docker compose stop chromadb
```

**2. AIfred Intelligence Service** (`systemd/aifred-intelligence.service`):
```ini
[Unit]
Description=AIfred Intelligence Voice Assistant
After=network.target ollama.service aifred-chromadb.service
Requires=ollama.service aifred-chromadb.service

[Service]
Type=simple
User=mp
Group=mp
WorkingDirectory=/home/mp/Projekte/AIfred-Intelligence
Environment="PATH=/home/mp/Projekte/AIfred-Intelligence/venv/bin:/usr/local/bin:/usr/bin:/bin"
Environment="PYTHONUNBUFFERED=1"
Environment="AIFRED_ENV=prod"
ExecStart=/home/mp/Projekte/AIfred-Intelligence/venv/bin/python -m reflex run --frontend-port 3002 --backend-port 8002 --backend-host 0.0.0.0
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

**Umgebungsvariable `AIFRED_ENV` erklÃ¤rt**:
- `AIFRED_ENV=dev` (Standard): API-URL = `http://172.30.8.72:8002` (Hauptrechner/WSL mit RTX 3060)
- `AIFRED_ENV=prod`: API-URL = `https://narnia.spdns.de:8443` (MiniPC mit Tesla P40)

Ohne `AIFRED_ENV=prod` werden alle API-Requests an den Entwicklungsrechner weitergeleitet, auch wenn Nginx korrekt konfiguriert ist!

2. Service aktivieren:
```bash
sudo systemctl daemon-reload
sudo systemctl enable aifred-intelligence
sudo systemctl start aifred-intelligence
```

3. **Optional: Polkit-Regel fÃ¼r Restart ohne sudo**

FÃ¼r den Restart-Button in der Web-UI ohne Passwort-Abfrage:

`/etc/polkit-1/rules.d/50-aifred-restart.rules`:
```javascript
polkit.addRule(function(action, subject) {
    if ((action.id == "org.freedesktop.systemd1.manage-units") &&
        (action.lookup("unit") == "aifred-intelligence.service" ||
         action.lookup("unit") == "ollama.service") &&
        (action.lookup("verb") == "restart") &&
        (subject.user == "mp")) {
        return polkit.Result.YES;
    }
});
```

---

## ğŸ› ï¸ Development

### Debug Logs
```bash
tail -f logs/aifred_debug.log
```

### Tests ausfÃ¼hren
```bash
pytest tests/
```

## ğŸ”¨ Troubleshooting

### HÃ¤ufige Probleme

#### HTTP ReadTimeout bei Research-Anfragen
**Problem**: `httpx.ReadTimeout` nach 60 Sekunden bei groÃŸen Recherchen
**LÃ¶sung**: Timeout ist bereits auf 300s erhÃ¶ht in `aifred/backends/ollama.py`
**Falls weiterhin Probleme**: Ollama Service neustarten mit `systemctl restart ollama`

#### Service startet nicht
**Problem**: AIfred Service startet nicht oder stoppt sofort
**LÃ¶sung**:
```bash
# Logs prÃ¼fen
journalctl -u aifred-intelligence -n 50
# Ollama Status prÃ¼fen
systemctl status ollama
```

#### Restart-Button funktioniert nicht
**Problem**: Restart-Button in Web-UI ohne Funktion
**LÃ¶sung**: Polkit-Regel prÃ¼fen in `/etc/polkit-1/rules.d/50-aifred-restart.rules`

---

## ğŸ“š Dokumentation

Weitere Dokumentation im `docs/` Verzeichnis:
- [Architecture Overview](docs/architecture/)
- [API Documentation](docs/api/)
- [Migration Guide](docs/infrastructure/MIGRATION.md)

---

## ğŸ¤ Contributing

Pull Requests sind willkommen! FÃ¼r grÃ¶ÃŸere Ã„nderungen bitte erst ein Issue Ã¶ffnen.

---

## ğŸ“ Session Notes - 03. November 2025

### Internationalisierung (i18n) Implementierung
- VollstÃ¤ndige Ãœbersetzungstabelle fÃ¼r UI-Strings
- Automatische Spracherkennung fÃ¼r Prompts (de/en basierend auf Nutzereingabe)
- Manueller UI-Sprachumschalter in den Einstellungen hinzugefÃ¼gt
- Englische Prompt-Dateien vervollstÃ¤ndigt (waren unvollstÃ¤ndig)

### Netzwerk- und Konfigurationsanpassungen
- `api_url` in `rxconfig.py` auf lokale IP fÃ¼r Entwicklungsumgebung korrigiert
- UmgebungsabhÃ¤ngige Konfiguration: `AIFRED_ENV=dev` vs `AIFRED_ENV=prod`
- Problem behoben: Anfragen wurden zu Mini-PC weitergeleitet statt lokal verarbeitet
- Entwicklung: `http://172.30.8.72:3002` (mit RTX 3060), Produktion: `https://narnia.spdns.de:8443`

### Bugfixes
- Parameterfehler behoben: `cache_metadata` â†’ `cache_info` in `get_decision_making_prompt()` Aufrufen
- Funktioniert jetzt korrekt mit der definierten Funktionssignatur

---

## ğŸ“„ License

MIT License - siehe [LICENSE](LICENSE) file

---

**Version**: 2.0.0 (November 2025)
**Status**: Production-Ready ğŸš€