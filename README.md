# ü§ñ AIfred Intelligence - Advanced AI Assistant

**Production-Ready AI Assistant with Multi-LLM Support, Web Research & Voice Interface**

AIfred Intelligence ist ein fortschrittlicher KI-Assistent mit automatischer Web-Recherche, Multi-Model-Support und History-Kompression f√ºr unbegrenzte Konversationen.

---

## ‚ú® Features

### üéØ Core Features
- **Multi-Backend Support**: Ollama (GGUF), vLLM (AWQ), TabbyAPI (EXL2)
- **Qwen3 Thinking Mode**: Chain-of-Thought Reasoning f√ºr komplexe Aufgaben
- **Automatische Web-Recherche**: KI entscheidet selbst wann Recherche n√∂tig ist
- **History Compression**: Intelligente Kompression bei 70% Context-Auslastung
- **Voice Interface**: Speech-to-Text und Text-to-Speech Integration
- **Vector Cache**: ChromaDB-basierter Semantic Cache f√ºr Web-Recherchen (Docker)
- **Per-Backend Settings**: Jedes Backend merkt sich seine bevorzugten Modelle

### üîß Technical Highlights
- **Reflex Framework**: React-Frontend aus Python generiert
- **WebSocket Streaming**: Echtzeit-Updates ohne Polling
- **Adaptive Temperature**: KI w√§hlt Temperature basierend auf Fragetyp
- **Token Management**: Dynamische Context-Window-Berechnung
- **Debug Console**: Umfangreiches Logging und Monitoring
- **ChromaDB Server Mode**: Thread-safe Vector DB via Docker (0.0 distance f√ºr exakte Matches)
- **GPU Detection**: Automatische Erkennung und Warnung bei inkompatiblen Backend-GPU-Kombinationen ([docs/GPU_COMPATIBILITY.md](docs/GPU_COMPATIBILITY.md))

---

## üöÄ Installation

### Voraussetzungen
- Python 3.10+
- **LLM Backend** (w√§hle eins):
  - **Ollama** (einfach, GGUF-Modelle) - empfohlen f√ºr Start
  - **vLLM** (schnell, AWQ-Modelle) - beste Performance (requires Compute Capability 7.5+)
  - **TabbyAPI** (ExLlamaV2/V3, EXL2-Modelle) - experimentell
- 8GB+ RAM (12GB+ empfohlen f√ºr gr√∂√üere Modelle)
- Docker (f√ºr ChromaDB Vector Cache)
- **GPU**: NVIDIA GPU empfohlen (siehe [GPU Compatibility Guide](docs/GPU_COMPATIBILITY.md))

### Setup

1. **Repository klonen**:
```bash
git clone https://github.com/yourusername/AIfred-Intelligence.git
cd AIfred-Intelligence
```

2. **Virtual Environment erstellen**:
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate     # Windows
```

3. **Dependencies installieren**:
```bash
pip install -r requirements.txt
```

4. **Umgebungsvariablen** (.env):
```env
# API Keys f√ºr Web-Recherche
BRAVE_API_KEY=your_key_here
TAVILY_API_KEY=your_key_here

# Ollama Konfiguration
OLLAMA_BASE_URL=http://localhost:11434
```

5. **LLM Models installieren**:

**Option A: Alle Models (Empfohlen)**
```bash
# Master-Script f√ºr beide Backends
./scripts/download_all_models.sh
```

**Option B: Nur Ollama (GGUF) - Einfachste Installation**
```bash
# Ollama Models (GGUF Q4/Q8)
./scripts/download_ollama_models.sh

# Empfohlene Core-Modelle:
# - qwen3:30b-instruct (18GB) - Haupt-LLM, 256K context
# - qwen3:8b (5.2GB) - Automatik, optional thinking
# - qwen2.5:3b (1.9GB) - Ultra-schnelle Automatik
```

**Option C: Nur vLLM (AWQ) - Beste Performance**
```bash
# vLLM installieren (falls noch nicht geschehen)
pip install vllm

# vLLM Models (AWQ Quantization)
./scripts/download_vllm_models.sh

# Empfohlene Modelle:
# - Qwen3-8B-AWQ (~5GB, 40K‚Üí128K mit YaRN)
# - Qwen3-14B-AWQ (~8GB, 32K‚Üí128K mit YaRN)
# - Qwen2.5-14B-Instruct-AWQ (~8GB, 128K native)

# vLLM Server starten mit YaRN (64K context)
./venv/bin/vllm serve Qwen/Qwen3-14B-AWQ \
  --quantization awq_marlin \
  --port 8001 \
  --rope-scaling '{"rope_type":"yarn","factor":2.0,"original_max_position_embeddings":32768}' \
  --max-model-len 65536 \
  --gpu-memory-utilization 0.85

# Oder als systemd Service
sudo cp vllm_qwen3_awq.service /etc/systemd/system/
sudo systemctl enable vllm_qwen3_awq
sudo systemctl start vllm_qwen3_awq
```

**Option C: TabbyAPI (EXL2) - Experimentell**
```bash
# Noch nicht vollst√§ndig implementiert
# Siehe: https://github.com/theroyallab/tabbyAPI
```

6. **ChromaDB Vector Cache starten** (Docker):
```bash
cd docker
docker-compose up -d chromadb
cd ..
```

**Optional: SearXNG auch starten** (lokale Suchmaschine):
```bash
cd docker
docker-compose --profile full up -d
cd ..
```

**ChromaDB Cache zur√ºcksetzen** (bei Bedarf):

*Option 1: Kompletter Neustart (l√∂scht alle Daten)*
```bash
cd docker
docker-compose stop chromadb
cd ..
rm -rf aifred_vector_cache/
cd docker
docker-compose up -d chromadb
cd ..
```

*Option 2: Nur Collection l√∂schen (w√§hrend Container l√§uft)*
```bash
./venv/bin/python -c "
import chromadb
from chromadb.config import Settings

client = chromadb.HttpClient(
    host='localhost',
    port=8000,
    settings=Settings(anonymized_telemetry=False)
)

try:
    client.delete_collection('research_cache')
    print('‚úÖ Collection gel√∂scht')
except Exception as e:
    print(f'‚ö†Ô∏è Fehler: {e}')
"
```

7. **Starten**:
```bash
reflex run
```

Die App l√§uft dann unter: http://localhost:3002

---

## ‚öôÔ∏è Backend-Wechsel & Settings

### Multi-Backend Support

AIfred unterst√ºtzt verschiedene LLM-Backends, die in der UI dynamisch gewechselt werden k√∂nnen:

- **Ollama**: GGUF-Modelle (Q4/Q8), einfachste Installation
- **vLLM**: AWQ-Modelle (4-bit), beste Performance mit AWQ Marlin Kernel
- **TabbyAPI**: EXL2-Modelle (ExLlamaV2/V3), experimentell

### GPU Compatibility Detection

AIfred erkennt automatisch beim Start deine GPU und warnt vor inkompatiblen Backend-Konfigurationen:

- **Tesla P40 / GTX 10 Series** (Pascal): Nutze Ollama (GGUF) - vLLM/AWQ wird nicht unterst√ºtzt
- **RTX 20+ Series** (Turing/Ampere/Ada): vLLM (AWQ) empfohlen f√ºr beste Performance

Detaillierte Informationen: [GPU_COMPATIBILITY.md](docs/GPU_COMPATIBILITY.md)

### Settings-Persistenz

Settings werden in `~/.config/aifred/settings.json` gespeichert:

**Per-Backend Modell-Speicherung:**
- Jedes Backend merkt sich seine zuletzt verwendeten Modelle
- Beim Backend-Wechsel werden automatisch die richtigen Modelle wiederhergestellt
- Beim ersten Start werden Defaults aus `aifred/lib/config.py` verwendet

**Beispiel Settings-Struktur:**
```json
{
  "backend_type": "vllm",
  "enable_thinking": true,
  "backend_models": {
    "ollama": {
      "selected_model": "qwen3:8b",
      "automatik_model": "qwen2.5:3b"
    },
    "vllm": {
      "selected_model": "Qwen/Qwen3-8B-AWQ",
      "automatik_model": "Qwen/Qwen3-4B-AWQ"
    }
  }
}
```

### Qwen3 Thinking Mode

**Chain-of-Thought Reasoning f√ºr komplexe Aufgaben:**

- **Thinking Mode ON**: Temperature 0.6, generiert `<think>...</think>` Blocks
- **Thinking Mode OFF**: Temperature 0.7, direkte Antworten ohne CoT
- Toggle erscheint nur bei Qwen3/QwQ-Modellen in der UI
- Funktioniert mit allen Backends (Ollama, vLLM, TabbyAPI)

**Empfohlene Modelle f√ºr Thinking Mode:**
- `qwen3:8b`, `qwen3:14b`, `qwen3:30b` (Ollama)
- `Qwen/Qwen3-8B-AWQ`, `Qwen/Qwen3-4B-AWQ` (vLLM)
- `qwq:32b` (dediziertes Reasoning-Modell, nur Ollama)

---

## üèóÔ∏è Architektur

### Directory Structure
```
AIfred-Intelligence/
‚îú‚îÄ‚îÄ aifred/
‚îÇ   ‚îú‚îÄ‚îÄ backends/          # LLM Backend Adapters
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py           # Abstract Base Class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama.py         # Ollama Backend (GGUF)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vllm.py           # vLLM Backend (AWQ)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tabbyapi.py       # TabbyAPI Backend (EXL2)
‚îÇ   ‚îú‚îÄ‚îÄ components/        # Reflex UI Components
‚îÇ   ‚îú‚îÄ‚îÄ lib/              # Core Libraries
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent_core.py     # Haupt-Agent-Logik
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ context_manager.py # History-Kompression
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py         # Default Settings
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py       # Settings Persistence
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector_cache.py   # ChromaDB Vector Cache
‚îÇ   ‚îî‚îÄ‚îÄ state.py          # Reflex State Management
‚îú‚îÄ‚îÄ prompts/              # System Prompts
‚îú‚îÄ‚îÄ scripts/              # Utility Scripts
‚îÇ   ‚îú‚îÄ‚îÄ download_all_models.sh     # Multi-Backend Model Downloader
‚îÇ   ‚îú‚îÄ‚îÄ download_ollama_models.sh  # Ollama GGUF Models
‚îÇ   ‚îú‚îÄ‚îÄ download_vllm_models.sh    # vLLM AWQ Models
‚îÇ   ‚îú‚îÄ‚îÄ run_aifred.sh              # Development Runner
‚îÇ   ‚îî‚îÄ‚îÄ chroma_maintenance.py      # Vector Cache Maintenance
‚îú‚îÄ‚îÄ docs/                 # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ vllm/                      # vLLM-specific docs
‚îÇ   ‚îî‚îÄ‚îÄ GPU_COMPATIBILITY.md       # GPU compatibility matrix
‚îî‚îÄ‚îÄ CHANGELOG.md          # Project Changelog
```

### History Compression System

Bei 70% Context-Auslastung werden automatisch √§ltere Konversationen komprimiert:

- **Trigger**: 70% des Context Windows belegt
- **Kompression**: 3 Frage-Antwort-Paare ‚Üí 1 Summary
- **Effizienz**: ~6:1 Kompressionsrate
- **FIFO**: Maximal 10 Summaries (√§lteste werden gel√∂scht)
- **Safety**: Mindestens 1 aktuelle Konversation bleibt sichtbar

### Vector Cache & RAG System

AIfred nutzt ein mehrstufiges Cache-System basierend auf **semantischer √Ñhnlichkeit** (Cosine Distance). **Neu in v1.3.0**: Pure Semantic Deduplication ohne Zeit-Abh√§ngigkeit + intelligente Cache-Nutzung bei expliziten Recherche-Keywords.

#### Cache-Entscheidungs-Logik

**Phase 0: Explizite Recherche-Keywords** (NEW in v1.3.0)
```
User Query: "recherchiere Python" / "google Python" / "suche im internet Python"
‚îî‚îÄ Explizites Keyword erkannt ‚Üí Cache-Check ZUERST
   ‚îú‚îÄ Distance < 0.05 (praktisch identisch)
   ‚îÇ  ‚îî‚îÄ ‚úÖ Cache-Hit (0.15s statt 100s) - Zeigt Alter transparent an
   ‚îî‚îÄ Distance ‚â• 0.05 (nicht identisch)
      ‚îî‚îÄ Neue Web-Recherche (User will neue Daten)
```

**Phase 1a: Direct Cache Hit Check**
```
User Query ‚Üí ChromaDB Similarity Search
‚îú‚îÄ Distance < 0.5 (HIGH Confidence)
‚îÇ  ‚îî‚îÄ ‚úÖ Use Cached Answer (sofort, keine Zeit-Checks mehr!)
‚îú‚îÄ Distance 0.5-1.2 (MEDIUM Confidence) ‚Üí Continue to Phase 1b (RAG)
‚îî‚îÄ Distance > 1.2 (LOW Confidence) ‚Üí Continue to Phase 2 (Research Decision)
```

**Phase 1b: RAG Context Check**
```
Cache Miss (d ‚â• 0.5) ‚Üí Query for RAG Candidates (0.5 ‚â§ d < 1.2)
‚îú‚îÄ Found RAG Candidates?
‚îÇ  ‚îú‚îÄ YES ‚Üí Automatik-LLM checks relevance for each candidate
‚îÇ  ‚îÇ   ‚îú‚îÄ Relevant (semantic match) ‚Üí Inject as System Message Context
‚îÇ  ‚îÇ   ‚îÇ   Example: "Python" ‚Üí "FastAPI" ‚úÖ (FastAPI is Python framework)
‚îÇ  ‚îÇ   ‚îî‚îÄ Not Relevant ‚Üí Skip
‚îÇ  ‚îÇ       Example: "Python" ‚Üí "Weather" ‚ùå (no connection)
‚îÇ  ‚îî‚îÄ NO ‚Üí Continue to Phase 2
‚îî‚îÄ LLM Answer with RAG Context (Source: "Cache+LLM (RAG)")
```

**Phase 2: Research Decision**
```
No Direct Cache Hit & No RAG Context
‚îî‚îÄ Automatik-LLM decides: Web Research needed?
   ‚îú‚îÄ YES ‚Üí Web Research + Cache Result
   ‚îî‚îÄ NO  ‚Üí Pure LLM Answer (Source: "LLM-Trainingsdaten")
```

#### Semantic Deduplication (v1.3.0)

**Beim Speichern in Vector Cache:**
```
New Research Result ‚Üí Check for Semantic Duplicates
‚îî‚îÄ Distance < 0.3 (semantisch √§hnlich)
   ‚îî‚îÄ ‚úÖ IMMER Update (zeitunabh√§ngig!)
      - L√∂scht alten Eintrag
      - Speichert neuen Eintrag
      - Garantiert: Neueste Daten werden verwendet
```

**Vorher (v1.2.0):**
- Zeit-basierte Logik: < 5min = Skip, ‚â• 5min = Update
- F√ºhrte zu Race Conditions und Duplikaten

**Jetzt (v1.3.0):**
- Rein semantisch: Distance < 0.3 = IMMER Update
- Keine Zeit-Checks mehr ‚Üí Konsistentes Verhalten

#### Cache Distance Thresholds

| Distance | Confidence | Behavior | Example |
|----------|-----------|----------|---------|
| `0.0 - 0.05` | EXACT | Explizite Recherche nutzt Cache | Identische Query |
| `0.05 - 0.5` | HIGH | Direct cache hit | "Python tutorial" vs "Python Anleitung" |
| `0.5 - 1.2` | MEDIUM | RAG candidate (relevance check via LLM) | "Python" vs "FastAPI" |
| `1.2+` | LOW | Cache miss ‚Üí Research decision | "Python" vs "Weather" |

#### ChromaDB Maintenance Tool (v1.3.0)

**Neues Wartungstool** f√ºr Vector Cache:
```bash
# Stats anzeigen
python3 chroma_maintenance.py --stats

# Duplikate finden
python3 chroma_maintenance.py --find-duplicates

# Duplikate entfernen (Dry-Run)
python3 chroma_maintenance.py --remove-duplicates

# Duplikate entfernen (Execute)
python3 chroma_maintenance.py --remove-duplicates --execute

# Alte Eintr√§ge l√∂schen (> 30 Tage)
python3 chroma_maintenance.py --remove-old 30 --execute
```

#### RAG (Retrieval-Augmented Generation) Mode

**How it works**:
1. Query finds related cache entries (distance 0.5-1.2)
2. Automatik-LLM checks if cached content is relevant to current question
3. Relevant entries are injected as system message: "Previous research shows..."
4. Main LLM combines cached context + training knowledge for enhanced answer

**Example Flow**:
```
User: "Was ist Python?" ‚Üí Web Research ‚Üí Cache Entry 1 (d=0.0)
User: "Was ist FastAPI?" ‚Üí RAG finds Entry 1 (d=0.7)
  ‚Üí LLM checks: "Python" relevant for "FastAPI"? YES (FastAPI uses Python)
  ‚Üí Inject Entry 1 as context ‚Üí Enhanced LLM answer
  ‚Üí Source: "Cache+LLM (RAG)"
```

**Benefits**:
- Leverages related past research without exact cache hits
- Avoids false context (LLM filters irrelevant entries)
- Multi-level context awareness (cache + conversation history)

#### Configuration

Cache behavior in `aifred/lib/config.py`:

```python
# Cache Distance Thresholds
CACHE_DISTANCE_DUPLICATE = 0.5   # < 0.5 = potential cache hit
CACHE_DISTANCE_MEDIUM = 0.5      # 0.5-1.2 = RAG range
CACHE_DISTANCE_RAG = 1.2         # < 1.2 = similar enough for RAG context

# Time Thresholds
CACHE_TIME_THRESHOLD = 300       # 5 minutes (in seconds)
```

**RAG Relevance Check**: Uses Automatik-LLM with dedicated prompt (`prompts/de/rag_relevance_check.txt`)

---

## üîß Konfiguration

Alle wichtigen Parameter in `aifred/lib/config.py`:

```python
# Deployment Mode (Production vs Development)
USE_SYSTEMD_RESTART = True  # True f√ºr Production, False f√ºr Development

# History Compression
HISTORY_COMPRESSION_THRESHOLD = 0.7  # 70% Context
HISTORY_MESSAGES_TO_COMPRESS = 6     # 3 Q&A Paare
HISTORY_MIN_MESSAGES_BEFORE_COMPRESSION = 10

# LLM Settings (Default Models)
DEFAULT_SETTINGS = {
    "model": "qwen3:30b-instruct",      # Haupt-LLM (Tesla P40 optimiert)
    "automatik_model": "qwen3:8b",      # Automatik-Entscheidungen
}

# Temperature Presets
TEMPERATURE_PRESETS = {
    "faktisch": 0.2,
    "gemischt": 0.5,
    "kreativ": 0.8
}
```

### HTTP Timeout Konfiguration

In `aifred/backends/ollama.py`:
- **HTTP Client Timeout**: 300 Sekunden (5 Minuten)
- Erh√∂ht von 60s f√ºr gro√üe Research-Anfragen mit 30KB+ Context
- Verhindert Timeout-Fehler bei erster Token-Generation

### Restart-Button Verhalten

Der AIfred Restart-Button kann in zwei Modi arbeiten:

- **Production Mode** (`USE_SYSTEMD_RESTART = True`):
  - Startet den kompletten systemd-Service neu
  - Ben√∂tigt Polkit-Regel f√ºr sudo-lose Ausf√ºhrung
  - F√ºr produktive Systeme mit systemd

- **Development Mode** (`USE_SYSTEMD_RESTART = False`):
  - Soft-Restart: L√∂scht nur Caches und History
  - Beh√§lt laufende Instanz f√ºr Hot-Reload
  - F√ºr lokale Entwicklung ohne Service

---

## üì¶ Deployment

### Systemd Service

F√ºr produktiven Betrieb als Service sind vorkonfigurierte Service-Dateien im `systemd/` Verzeichnis verf√ºgbar.

**‚ö†Ô∏è WICHTIG**: Die Umgebungsvariable `AIFRED_ENV=prod` **MUSS** gesetzt sein, damit AIfred auf dem MiniPC l√§uft und nicht auf den Entwicklungsrechner weiterleitet!

#### Schnellinstallation

```bash
# 1. Service-Dateien kopieren
sudo cp systemd/aifred-chromadb.service /etc/systemd/system/
sudo cp systemd/aifred-intelligence.service /etc/systemd/system/

# 2. Services aktivieren und starten
sudo systemctl daemon-reload
sudo systemctl enable aifred-chromadb.service aifred-intelligence.service
sudo systemctl start aifred-chromadb.service aifred-intelligence.service

# 3. Status pr√ºfen
systemctl status aifred-chromadb.service
systemctl status aifred-intelligence.service
```

Siehe [systemd/README.md](systemd/README.md) f√ºr Details, Troubleshooting und Monitoring.

#### Service-Dateien (Referenz)

**1. ChromaDB Service** (`systemd/aifred-chromadb.service`):
```ini
[Unit]
Description=AIfred ChromaDB Vector Cache (Docker)
After=docker.service
Requires=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
WorkingDirectory=/home/mp/Projekte/AIfred-Intelligence/docker
ExecStart=/usr/bin/docker compose up -d chromadb
ExecStop=/usr/bin/docker compose stop chromadb
```

**2. AIfred Intelligence Service** (`systemd/aifred-intelligence.service`):
```ini
[Unit]
Description=AIfred Intelligence Voice Assistant
After=network.target ollama.service aifred-chromadb.service
Requires=ollama.service aifred-chromadb.service

[Service]
Type=simple
User=mp
Group=mp
WorkingDirectory=/home/mp/Projekte/AIfred-Intelligence
Environment="PATH=/home/mp/Projekte/AIfred-Intelligence/venv/bin:/usr/local/bin:/usr/bin:/bin"
Environment="PYTHONUNBUFFERED=1"
Environment="AIFRED_ENV=prod"
ExecStart=/home/mp/Projekte/AIfred-Intelligence/venv/bin/python -m reflex run --frontend-port 3002 --backend-port 8002 --backend-host 0.0.0.0
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

**Umgebungsvariable `AIFRED_ENV` erkl√§rt**:
- `AIFRED_ENV=dev` (Standard): API-URL = `http://172.30.8.72:8002` (Hauptrechner/WSL mit RTX 3060)
- `AIFRED_ENV=prod`: API-URL = `https://narnia.spdns.de:8443` (MiniPC mit Tesla P40)

Ohne `AIFRED_ENV=prod` werden alle API-Requests an den Entwicklungsrechner weitergeleitet, auch wenn Nginx korrekt konfiguriert ist!

2. Service aktivieren:
```bash
sudo systemctl daemon-reload
sudo systemctl enable aifred-intelligence
sudo systemctl start aifred-intelligence
```

3. **Optional: Polkit-Regel f√ºr Restart ohne sudo**

F√ºr den Restart-Button in der Web-UI ohne Passwort-Abfrage:

`/etc/polkit-1/rules.d/50-aifred-restart.rules`:
```javascript
polkit.addRule(function(action, subject) {
    if ((action.id == "org.freedesktop.systemd1.manage-units") &&
        (action.lookup("unit") == "aifred-intelligence.service" ||
         action.lookup("unit") == "ollama.service") &&
        (action.lookup("verb") == "restart") &&
        (subject.user == "mp")) {
        return polkit.Result.YES;
    }
});
```

---

## üõ†Ô∏è Development

### Debug Logs
```bash
tail -f logs/aifred_debug.log
```

### Tests ausf√ºhren
```bash
pytest tests/
```

## üî® Troubleshooting

### H√§ufige Probleme

#### HTTP ReadTimeout bei Research-Anfragen
**Problem**: `httpx.ReadTimeout` nach 60 Sekunden bei gro√üen Recherchen
**L√∂sung**: Timeout ist bereits auf 300s erh√∂ht in `aifred/backends/ollama.py`
**Falls weiterhin Probleme**: Ollama Service neustarten mit `systemctl restart ollama`

#### Service startet nicht
**Problem**: AIfred Service startet nicht oder stoppt sofort
**L√∂sung**:
```bash
# Logs pr√ºfen
journalctl -u aifred-intelligence -n 50
# Ollama Status pr√ºfen
systemctl status ollama
```

#### Restart-Button funktioniert nicht
**Problem**: Restart-Button in Web-UI ohne Funktion
**L√∂sung**: Polkit-Regel pr√ºfen in `/etc/polkit-1/rules.d/50-aifred-restart.rules`

---

## üìö Dokumentation

Weitere Dokumentation im `docs/` Verzeichnis:
- [Architecture Overview](docs/architecture/)
- [API Documentation](docs/api/)
- [Migration Guide](docs/infrastructure/MIGRATION.md)

---

## ü§ù Contributing

Pull Requests sind willkommen! F√ºr gr√∂√üere √Ñnderungen bitte erst ein Issue √∂ffnen.

---

## üìù Session Notes - 03. November 2025

### Internationalisierung (i18n) Implementierung
- Vollst√§ndige √úbersetzungstabelle f√ºr UI-Strings
- Automatische Spracherkennung f√ºr Prompts (de/en basierend auf Nutzereingabe)
- Manueller UI-Sprachumschalter in den Einstellungen hinzugef√ºgt
- Englische Prompt-Dateien vervollst√§ndigt (waren unvollst√§ndig)

### Netzwerk- und Konfigurationsanpassungen
- `api_url` in `rxconfig.py` auf lokale IP f√ºr Entwicklungsumgebung korrigiert
- Umgebungsabh√§ngige Konfiguration: `AIFRED_ENV=dev` vs `AIFRED_ENV=prod`
- Problem behoben: Anfragen wurden zu Mini-PC weitergeleitet statt lokal verarbeitet
- Entwicklung: `http://172.30.8.72:3002` (mit RTX 3060), Produktion: `https://narnia.spdns.de:8443`

### Bugfixes
- Parameterfehler behoben: `cache_metadata` ‚Üí `cache_info` in `get_decision_making_prompt()` Aufrufen
- Funktioniert jetzt korrekt mit der definierten Funktionssignatur

---

## üìÑ License

MIT License - siehe [LICENSE](LICENSE) file

---

**Version**: 2.0.0 (November 2025)
**Status**: Production-Ready üöÄ