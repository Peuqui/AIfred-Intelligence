# ğŸ¤– AIfred Intelligence - Advanced AI Assistant

**Production-Ready AI Assistant with Multi-LLM Support, Web Research & Voice Interface**

AIfred Intelligence ist ein fortschrittlicher KI-Assistent mit automatischer Web-Recherche, Multi-Model-Support und History-Kompression fÃ¼r unbegrenzte Konversationen.

---

## âœ¨ Features

### ğŸ¯ Core Features
- **Multi-LLM Support**: Ollama Backend mit verschiedenen Modellen (Qwen, Phi3, etc.)
- **Automatische Web-Recherche**: KI entscheidet selbst wann Recherche nÃ¶tig ist
- **History Compression**: Intelligente Kompression bei 70% Context-Auslastung
- **Voice Interface**: Speech-to-Text und Text-to-Speech Integration
- **Cache-System**: Intelligentes Caching von Recherche-Ergebnissen

### ğŸ”§ Technical Highlights
- **Reflex Framework**: React-Frontend aus Python generiert
- **WebSocket Streaming**: Echtzeit-Updates ohne Polling
- **Adaptive Temperature**: KI wÃ¤hlt Temperature basierend auf Fragetyp
- **Token Management**: Dynamische Context-Window-Berechnung
- **Debug Console**: Umfangreiches Logging und Monitoring

---

## ğŸš€ Installation

### Voraussetzungen
- Python 3.10+
- Ollama (fÃ¼r LLM Backend)
- 8GB+ RAM empfohlen

### Setup

1. **Repository klonen**:
```bash
git clone https://github.com/yourusername/AIfred-Intelligence.git
cd AIfred-Intelligence
```

2. **Virtual Environment erstellen**:
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate     # Windows
```

3. **Dependencies installieren**:
```bash
pip install -r requirements.txt
```

4. **Umgebungsvariablen** (.env):
```env
# API Keys fÃ¼r Web-Recherche
BRAVE_API_KEY=your_key_here
TAVILY_API_KEY=your_key_here

# Ollama Konfiguration
OLLAMA_BASE_URL=http://localhost:11434
```

5. **Ollama Models installieren**:
```bash
ollama pull qwen3:8b        # Haupt-LLM
ollama pull qwen2.5:3b      # Automatik-LLM
ollama pull phi3:mini       # Backup/Test
```

6. **Starten**:
```bash
reflex run
```

Die App lÃ¤uft dann unter: http://localhost:3002

---

## ğŸ—ï¸ Architektur

### Directory Structure
```
AIfred-Intelligence/
â”œâ”€â”€ aifred/
â”‚   â”œâ”€â”€ backends/          # LLM Backend Adapters
â”‚   â”œâ”€â”€ components/        # Reflex UI Components
â”‚   â”œâ”€â”€ lib/              # Core Libraries
â”‚   â”‚   â”œâ”€â”€ agent_core.py     # Haupt-Agent-Logik
â”‚   â”‚   â”œâ”€â”€ context_manager.py # History-Kompression
â”‚   â”‚   â”œâ”€â”€ config.py         # Konfiguration
â”‚   â”‚   â””â”€â”€ cache.py         # Cache-System
â”‚   â””â”€â”€ state.py          # Reflex State Management
â”œâ”€â”€ prompts/              # System Prompts
â”œâ”€â”€ logs/                 # Debug Logs
â””â”€â”€ docs/                # Dokumentation
```

### History Compression System

Bei 70% Context-Auslastung werden automatisch Ã¤ltere Konversationen komprimiert:

- **Trigger**: 70% des Context Windows belegt
- **Kompression**: 3 Frage-Antwort-Paare â†’ 1 Summary
- **Effizienz**: ~6:1 Kompressionsrate
- **FIFO**: Maximal 10 Summaries (Ã¤lteste werden gelÃ¶scht)
- **Safety**: Mindestens 1 aktuelle Konversation bleibt sichtbar

---

## ğŸ”§ Konfiguration

Alle wichtigen Parameter in `aifred/lib/config.py`:

```python
# History Compression
HISTORY_COMPRESSION_THRESHOLD = 0.7  # 70% Context
HISTORY_MESSAGES_TO_COMPRESS = 6     # 3 Q&A Paare
HISTORY_MIN_MESSAGES_BEFORE_COMPRESSION = 10

# LLM Settings
LLM_MAIN_MODEL = "qwen3:8b"
LLM_AUTOMATIK_MODEL = "qwen2.5:3b"

# Temperature Presets
TEMPERATURE_PRESETS = {
    "faktisch": 0.2,
    "gemischt": 0.5,
    "kreativ": 0.8
}
```

---

## ğŸ“¦ Deployment

### Systemd Service

FÃ¼r produktiven Betrieb als Service:

1. Service-File erstellen: `/etc/systemd/system/aifred.service`
```ini
[Unit]
Description=AIfred Intelligence
After=network.target ollama.service

[Service]
Type=simple
User=aifred
WorkingDirectory=/opt/aifred
Environment="PATH=/opt/aifred/venv/bin"
ExecStart=/opt/aifred/venv/bin/python -m reflex run --frontend-port 3002 --backend-port 8001
Restart=always

[Install]
WantedBy=multi-user.target
```

2. Service aktivieren:
```bash
sudo systemctl daemon-reload
sudo systemctl enable aifred
sudo systemctl start aifred
```

---

## ğŸ› ï¸ Development

### Debug Logs
```bash
tail -f logs/aifred_debug.log
```

### Tests ausfÃ¼hren
```bash
pytest tests/
```

---

## ğŸ“š Dokumentation

Weitere Dokumentation im `docs/` Verzeichnis:
- [Architecture Overview](docs/architecture/)
- [API Documentation](docs/api/)
- [Migration Guide](docs/infrastructure/MIGRATION.md)

---

## ğŸ¤ Contributing

Pull Requests sind willkommen! FÃ¼r grÃ¶ÃŸere Ã„nderungen bitte erst ein Issue Ã¶ffnen.

---

## ğŸ“„ License

MIT License - siehe [LICENSE](LICENSE) file

---

**Version**: 2.0.0 (November 2025)
**Status**: Production-Ready ğŸš€