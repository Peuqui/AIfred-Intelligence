# ğŸ¤– AIfred Intelligence - Advanced AI Assistant

**Production-Ready AI Assistant with Multi-LLM Support, Web Research & Voice Interface**

AIfred Intelligence ist ein fortschrittlicher KI-Assistent mit automatischer Web-Recherche, Multi-Model-Support und History-Kompression fÃ¼r unbegrenzte Konversationen.

---

## ğŸ‰ Latest Updates (2025-11-15)

### ğŸ” Enhanced Debug Logging & Query Visibility
- âœ… **Consistent Debug Output** across all research modes (Eigenes Wissen, Automatik, Quick, Deep)
- âœ… **Precise Preload Timing** with `âœ… Haupt-LLM vorgeladen (X.Xs)` in all modes
- âœ… **Optimized Query Display** shows LLM-generated search terms: `ğŸ” Optimierte Query: [terms]`
- âœ… **Backend-Aware Timing**: Ollama (actual load time) vs vLLM/TabbyAPI (prep time)
- ğŸ”§ Added comprehensive debug messages: Token stats, Temperature, TTFT, Tokens/s
- **Impact**: Professional debug output, easier performance optimization, better web search quality assessment

### ğŸ”§ Research Mode Persistence (2025-11-14)
- âœ… **Research Mode now persists** across application restarts
- ğŸ”§ Fixed missing `_save_settings()` call in `set_research_mode_display()`
- **Impact**: Your preferred research mode (Automatik/Quick/Deep/None) is remembered

### ğŸ¯ Progress UI System Complete - MILESTONE (2025-11-14)
- âœ… **Full Progress Feedback** across all 4 research modes (Automatik, Quick, Deep, None)
- âœ… **Pulsing Animation** for "Generiere Antwort" in all modes (including "Eigenes Wissen")
- âœ… **Web-Scraping Progress Bar** now visible (1/3, 2/3, 3/3) with orange fill
- âœ… **Dynamic Status Text** reflects system activity in real-time
- **Impact**: Professional, consistent UI feedback - users always know what the system is doing

See [CHANGELOG.md](CHANGELOG.md) for detailed changes.

---

## âœ¨ Features

### ğŸ¯ Core Features
- **Multi-Backend Support**: Ollama (GGUF), vLLM (AWQ), TabbyAPI (EXL2)
- **Qwen3 Thinking Mode**: Chain-of-Thought Reasoning fÃ¼r komplexe Aufgaben (Ollama + vLLM)
- **Automatische Web-Recherche**: KI entscheidet selbst wann Recherche nÃ¶tig ist
- **History Compression**: Intelligente Kompression bei 70% Context-Auslastung
- **Voice Interface**: Speech-to-Text und Text-to-Speech Integration
- **Vector Cache**: ChromaDB-basierter Semantic Cache fÃ¼r Web-Recherchen (Docker)
- **Per-Backend Settings**: Jedes Backend merkt sich seine bevorzugten Modelle

### ğŸ”§ Technical Highlights
- **Reflex Framework**: React-Frontend aus Python generiert
- **WebSocket Streaming**: Echtzeit-Updates ohne Polling
- **Adaptive Temperature**: KI wÃ¤hlt Temperature basierend auf Fragetyp
- **Token Management**: Dynamische Context-Window-Berechnung
- **Debug Console**: Umfangreiches Logging und Monitoring
- **ChromaDB Server Mode**: Thread-safe Vector DB via Docker (0.0 distance fÃ¼r exakte Matches)
- **GPU Detection**: Automatische Erkennung und Warnung bei inkompatiblen Backend-GPU-Kombinationen ([docs/GPU_COMPATIBILITY.md](docs/GPU_COMPATIBILITY.md))

---

## ğŸš€ Installation

### Voraussetzungen
- Python 3.10+
- **LLM Backend** (wÃ¤hle eins):
  - **Ollama** (einfach, GGUF-Modelle) - empfohlen fÃ¼r Start
  - **vLLM** (schnell, AWQ-Modelle) - beste Performance (requires Compute Capability 7.5+)
  - **TabbyAPI** (ExLlamaV2/V3, EXL2-Modelle) - experimentell
- 8GB+ RAM (12GB+ empfohlen fÃ¼r grÃ¶ÃŸere Modelle)
- Docker (fÃ¼r ChromaDB Vector Cache)
- **GPU**: NVIDIA GPU empfohlen (siehe [GPU Compatibility Guide](docs/GPU_COMPATIBILITY.md))

### Setup

1. **Repository klonen**:
```bash
git clone https://github.com/yourusername/AIfred-Intelligence.git
cd AIfred-Intelligence
```

2. **Virtual Environment erstellen**:
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate     # Windows
```

3. **Dependencies installieren**:
```bash
pip install -r requirements.txt
```

4. **Umgebungsvariablen** (.env):
```env
# API Keys fÃ¼r Web-Recherche
BRAVE_API_KEY=your_key_here
TAVILY_API_KEY=your_key_here

# Ollama Konfiguration
OLLAMA_BASE_URL=http://localhost:11434
```

5. **LLM Models installieren**:

**Option A: Alle Models (Empfohlen)**
```bash
# Master-Script fÃ¼r beide Backends
./scripts/download_all_models.sh
```

**Option B: Nur Ollama (GGUF) - Einfachste Installation**
```bash
# Ollama Models (GGUF Q4/Q8)
./scripts/download_ollama_models.sh

# Empfohlene Core-Modelle:
# - qwen3:30b-instruct (18GB) - Haupt-LLM, 256K context
# - qwen3:8b (5.2GB) - Automatik, optional thinking
# - qwen2.5:3b (1.9GB) - Ultra-schnelle Automatik
```

**Option C: Nur vLLM (AWQ) - Beste Performance**
```bash
# vLLM installieren (falls noch nicht geschehen)
pip install vllm

# vLLM Models (AWQ Quantization)
./scripts/download_vllm_models.sh

# Empfohlene Modelle:
# - Qwen3-8B-AWQ (~5GB, 40Kâ†’128K mit YaRN)
# - Qwen3-14B-AWQ (~8GB, 32Kâ†’128K mit YaRN)
# - Qwen2.5-14B-Instruct-AWQ (~8GB, 128K native)

# vLLM Server starten mit YaRN (64K context)
./venv/bin/vllm serve Qwen/Qwen3-14B-AWQ \
  --quantization awq_marlin \
  --port 8001 \
  --rope-scaling '{"rope_type":"yarn","factor":2.0,"original_max_position_embeddings":32768}' \
  --max-model-len 65536 \
  --gpu-memory-utilization 0.85

# Oder als systemd Service
sudo cp vllm_qwen3_awq.service /etc/systemd/system/
sudo systemctl enable vllm_qwen3_awq
sudo systemctl start vllm_qwen3_awq
```

**Option C: TabbyAPI (EXL2) - Experimentell**
```bash
# Noch nicht vollstÃ¤ndig implementiert
# Siehe: https://github.com/theroyallab/tabbyAPI
```

6. **ChromaDB Vector Cache starten** (Docker):
```bash
cd docker
docker-compose up -d chromadb
cd ..
```

**Optional: SearXNG auch starten** (lokale Suchmaschine):
```bash
cd docker
docker-compose --profile full up -d
cd ..
```

**ChromaDB Cache zurÃ¼cksetzen** (bei Bedarf):

*Option 1: Kompletter Neustart (lÃ¶scht alle Daten)*
```bash
cd docker
docker-compose stop chromadb
cd ..
rm -rf aifred_vector_cache/
cd docker
docker-compose up -d chromadb
cd ..
```

*Option 2: Nur Collection lÃ¶schen (wÃ¤hrend Container lÃ¤uft)*
```bash
./venv/bin/python -c "
import chromadb
from chromadb.config import Settings

client = chromadb.HttpClient(
    host='localhost',
    port=8000,
    settings=Settings(anonymized_telemetry=False)
)

try:
    client.delete_collection('research_cache')
    print('âœ… Collection gelÃ¶scht')
except Exception as e:
    print(f'âš ï¸ Fehler: {e}')
"
```

7. **Starten**:
```bash
reflex run
```

Die App lÃ¤uft dann unter: http://localhost:3002

---

## âš™ï¸ Backend-Wechsel & Settings

### Multi-Backend Support

AIfred unterstÃ¼tzt verschiedene LLM-Backends, die in der UI dynamisch gewechselt werden kÃ¶nnen:

- **Ollama**: GGUF-Modelle (Q4/Q8), einfachste Installation
- **vLLM**: AWQ-Modelle (4-bit), beste Performance mit AWQ Marlin Kernel
- **TabbyAPI**: EXL2-Modelle (ExLlamaV2/V3), experimentell

### GPU Compatibility Detection

AIfred erkennt automatisch beim Start deine GPU und warnt vor inkompatiblen Backend-Konfigurationen:

- **Tesla P40 / GTX 10 Series** (Pascal): Nutze Ollama (GGUF) - vLLM/AWQ wird nicht unterstÃ¼tzt
- **RTX 20+ Series** (Turing/Ampere/Ada): vLLM (AWQ) empfohlen fÃ¼r beste Performance

Detaillierte Informationen: [GPU_COMPATIBILITY.md](docs/GPU_COMPATIBILITY.md)

### Settings-Persistenz

Settings werden in `~/.config/aifred/settings.json` gespeichert:

**Per-Backend Modell-Speicherung:**
- Jedes Backend merkt sich seine zuletzt verwendeten Modelle
- Beim Backend-Wechsel werden automatisch die richtigen Modelle wiederhergestellt
- Beim ersten Start werden Defaults aus `aifred/lib/config.py` verwendet

**Beispiel Settings-Struktur:**
```json
{
  "backend_type": "vllm",
  "enable_thinking": true,
  "backend_models": {
    "ollama": {
      "selected_model": "qwen3:8b",
      "automatik_model": "qwen2.5:3b"
    },
    "vllm": {
      "selected_model": "Qwen/Qwen3-8B-AWQ",
      "automatik_model": "Qwen/Qwen3-4B-AWQ"
    }
  }
}
```

### Qwen3 Thinking Mode

**Chain-of-Thought Reasoning fÃ¼r komplexe Aufgaben:**

AIfred unterstÃ¼tzt Thinking Mode fÃ¼r Qwen3-Modelle in Ollama und vLLM Backends:

- **Thinking Mode ON**: Temperature 0.6, generiert `<think>...</think>` Blocks mit Denkprozess
- **Thinking Mode OFF**: Temperature 0.7, direkte Antworten ohne CoT
- **UI**: Toggle erscheint nur bei Qwen3/QwQ-Modellen
- **Backends**:
  - **Ollama**: Nutzt `"think": true` API-Parameter, liefert separate `thinking` und `content` Felder
  - **vLLM**: Nutzt `chat_template_kwargs: {"enable_thinking": true/false}` in `extra_body`
  - **TabbyAPI**: Noch nicht implementiert
- **Formatierung**: Denkprozess als ausklappbares Collapsible mit Modellname und Inferenzzeit
- **Automatik-LLM**: Thinking Mode fÃ¼r Automatik-Entscheidungen DEAKTIVIERT (8x schneller)

**Empfohlene Modelle fÃ¼r Thinking Mode:**
- `qwen3:8b`, `qwen3:14b`, `qwen3:30b` (Ollama)
- `Qwen/Qwen3-8B-AWQ`, `Qwen/Qwen3-4B-AWQ` (vLLM)
- `qwq:32b` (dediziertes Reasoning-Modell, nur Ollama)

**Technische Details:**
- Ollama sendet `thinking` + `content` in separaten Message-Feldern
- vLLM nutzt Qwen3's Chat-Template mit `enable_thinking` Flag
- Beide Backends wrappen Output in `<think>...</think>` Tags fÃ¼r einheitliche Formatierung
- Formatting-Modul (`aifred/lib/formatting.py`) rendert Collapsibles mit kompakter Absatz-Formatierung

---

## ğŸ—ï¸ Architektur

### Directory Structure
```
AIfred-Intelligence/
â”œâ”€â”€ aifred/
â”‚   â”œâ”€â”€ backends/          # LLM Backend Adapters
â”‚   â”‚   â”œâ”€â”€ base.py           # Abstract Base Class
â”‚   â”‚   â”œâ”€â”€ ollama.py         # Ollama Backend (GGUF)
â”‚   â”‚   â”œâ”€â”€ vllm.py           # vLLM Backend (AWQ)
â”‚   â”‚   â””â”€â”€ tabbyapi.py       # TabbyAPI Backend (EXL2)
â”‚   â”œâ”€â”€ components/        # Reflex UI Components
â”‚   â”œâ”€â”€ lib/              # Core Libraries
â”‚   â”‚   â”œâ”€â”€ agent_core.py     # Haupt-Agent-Logik
â”‚   â”‚   â”œâ”€â”€ context_manager.py # History-Kompression
â”‚   â”‚   â”œâ”€â”€ config.py         # Default Settings
â”‚   â”‚   â”œâ”€â”€ settings.py       # Settings Persistence
â”‚   â”‚   â””â”€â”€ vector_cache.py   # ChromaDB Vector Cache
â”‚   â””â”€â”€ state.py          # Reflex State Management
â”œâ”€â”€ prompts/              # System Prompts
â”œâ”€â”€ scripts/              # Utility Scripts
â”‚   â”œâ”€â”€ download_all_models.sh     # Multi-Backend Model Downloader
â”‚   â”œâ”€â”€ download_ollama_models.sh  # Ollama GGUF Models
â”‚   â”œâ”€â”€ download_vllm_models.sh    # vLLM AWQ Models
â”‚   â”œâ”€â”€ run_aifred.sh              # Development Runner
â”‚   â””â”€â”€ chroma_maintenance.py      # Vector Cache Maintenance
â”œâ”€â”€ docs/                 # Documentation
â”‚   â”œâ”€â”€ vllm/                      # vLLM-specific docs
â”‚   â””â”€â”€ GPU_COMPATIBILITY.md       # GPU compatibility matrix
â””â”€â”€ CHANGELOG.md          # Project Changelog
```

### History Compression System

Bei 70% Context-Auslastung werden automatisch Ã¤ltere Konversationen komprimiert:

- **Trigger**: 70% des Context Windows belegt
- **Kompression**: 3 Frage-Antwort-Paare â†’ 1 Summary
- **Effizienz**: ~6:1 Kompressionsrate
- **FIFO**: Maximal 10 Summaries (Ã¤lteste werden gelÃ¶scht)
- **Safety**: Mindestens 1 aktuelle Konversation bleibt sichtbar

### Vector Cache & RAG System

AIfred nutzt ein mehrstufiges Cache-System basierend auf **semantischer Ã„hnlichkeit** (Cosine Distance). **Neu in v1.3.0**: Pure Semantic Deduplication ohne Zeit-AbhÃ¤ngigkeit + intelligente Cache-Nutzung bei expliziten Recherche-Keywords.

#### Cache-Entscheidungs-Logik

**Phase 0: Explizite Recherche-Keywords** (NEW in v1.3.0)
```
User Query: "recherchiere Python" / "google Python" / "suche im internet Python"
â””â”€ Explizites Keyword erkannt â†’ Cache-Check ZUERST
   â”œâ”€ Distance < 0.05 (praktisch identisch)
   â”‚  â””â”€ âœ… Cache-Hit (0.15s statt 100s) - Zeigt Alter transparent an
   â””â”€ Distance â‰¥ 0.05 (nicht identisch)
      â””â”€ Neue Web-Recherche (User will neue Daten)
```

**Phase 1a: Direct Cache Hit Check**
```
User Query â†’ ChromaDB Similarity Search
â”œâ”€ Distance < 0.5 (HIGH Confidence)
â”‚  â””â”€ âœ… Use Cached Answer (sofort, keine Zeit-Checks mehr!)
â”œâ”€ Distance 0.5-1.2 (MEDIUM Confidence) â†’ Continue to Phase 1b (RAG)
â””â”€ Distance > 1.2 (LOW Confidence) â†’ Continue to Phase 2 (Research Decision)
```

**Phase 1b: RAG Context Check**
```
Cache Miss (d â‰¥ 0.5) â†’ Query for RAG Candidates (0.5 â‰¤ d < 1.2)
â”œâ”€ Found RAG Candidates?
â”‚  â”œâ”€ YES â†’ Automatik-LLM checks relevance for each candidate
â”‚  â”‚   â”œâ”€ Relevant (semantic match) â†’ Inject as System Message Context
â”‚  â”‚   â”‚   Example: "Python" â†’ "FastAPI" âœ… (FastAPI is Python framework)
â”‚  â”‚   â””â”€ Not Relevant â†’ Skip
â”‚  â”‚       Example: "Python" â†’ "Weather" âŒ (no connection)
â”‚  â””â”€ NO â†’ Continue to Phase 2
â””â”€ LLM Answer with RAG Context (Source: "Cache+LLM (RAG)")
```

**Phase 2: Research Decision**
```
No Direct Cache Hit & No RAG Context
â””â”€ Automatik-LLM decides: Web Research needed?
   â”œâ”€ YES â†’ Web Research + Cache Result
   â””â”€ NO  â†’ Pure LLM Answer (Source: "LLM-Trainingsdaten")
```

#### Semantic Deduplication (v1.3.0)

**Beim Speichern in Vector Cache:**
```
New Research Result â†’ Check for Semantic Duplicates
â””â”€ Distance < 0.3 (semantisch Ã¤hnlich)
   â””â”€ âœ… IMMER Update (zeitunabhÃ¤ngig!)
      - LÃ¶scht alten Eintrag
      - Speichert neuen Eintrag
      - Garantiert: Neueste Daten werden verwendet
```

**Vorher (v1.2.0):**
- Zeit-basierte Logik: < 5min = Skip, â‰¥ 5min = Update
- FÃ¼hrte zu Race Conditions und Duplikaten

**Jetzt (v1.3.0):**
- Rein semantisch: Distance < 0.3 = IMMER Update
- Keine Zeit-Checks mehr â†’ Konsistentes Verhalten

#### Cache Distance Thresholds

| Distance | Confidence | Behavior | Example |
|----------|-----------|----------|---------|
| `0.0 - 0.05` | EXACT | Explizite Recherche nutzt Cache | Identische Query |
| `0.05 - 0.5` | HIGH | Direct cache hit | "Python tutorial" vs "Python Anleitung" |
| `0.5 - 1.2` | MEDIUM | RAG candidate (relevance check via LLM) | "Python" vs "FastAPI" |
| `1.2+` | LOW | Cache miss â†’ Research decision | "Python" vs "Weather" |

#### ChromaDB Maintenance Tool (v1.3.0)

**Neues Wartungstool** fÃ¼r Vector Cache:
```bash
# Stats anzeigen
python3 chroma_maintenance.py --stats

# Duplikate finden
python3 chroma_maintenance.py --find-duplicates

# Duplikate entfernen (Dry-Run)
python3 chroma_maintenance.py --remove-duplicates

# Duplikate entfernen (Execute)
python3 chroma_maintenance.py --remove-duplicates --execute

# Alte EintrÃ¤ge lÃ¶schen (> 30 Tage)
python3 chroma_maintenance.py --remove-old 30 --execute
```

#### RAG (Retrieval-Augmented Generation) Mode

**How it works**:
1. Query finds related cache entries (distance 0.5-1.2)
2. Automatik-LLM checks if cached content is relevant to current question
3. Relevant entries are injected as system message: "Previous research shows..."
4. Main LLM combines cached context + training knowledge for enhanced answer

**Example Flow**:
```
User: "Was ist Python?" â†’ Web Research â†’ Cache Entry 1 (d=0.0)
User: "Was ist FastAPI?" â†’ RAG finds Entry 1 (d=0.7)
  â†’ LLM checks: "Python" relevant for "FastAPI"? YES (FastAPI uses Python)
  â†’ Inject Entry 1 as context â†’ Enhanced LLM answer
  â†’ Source: "Cache+LLM (RAG)"
```

**Benefits**:
- Leverages related past research without exact cache hits
- Avoids false context (LLM filters irrelevant entries)
- Multi-level context awareness (cache + conversation history)

#### Configuration

Cache behavior in `aifred/lib/config.py`:

```python
# Cache Distance Thresholds
CACHE_DISTANCE_DUPLICATE = 0.5   # < 0.5 = potential cache hit
CACHE_DISTANCE_MEDIUM = 0.5      # 0.5-1.2 = RAG range
CACHE_DISTANCE_RAG = 1.2         # < 1.2 = similar enough for RAG context

# Time Thresholds
CACHE_TIME_THRESHOLD = 300       # 5 minutes (in seconds)
```

**RAG Relevance Check**: Uses Automatik-LLM with dedicated prompt (`prompts/de/rag_relevance_check.txt`)

---

## ğŸ”§ Konfiguration

Alle wichtigen Parameter in `aifred/lib/config.py`:

```python
# Deployment Mode (Production vs Development)
USE_SYSTEMD_RESTART = True  # True fÃ¼r Production, False fÃ¼r Development

# History Compression
HISTORY_COMPRESSION_THRESHOLD = 0.7  # 70% Context
HISTORY_MESSAGES_TO_COMPRESS = 6     # 3 Q&A Paare
HISTORY_MIN_MESSAGES_BEFORE_COMPRESSION = 10

# LLM Settings (Default Models)
DEFAULT_SETTINGS = {
    "model": "qwen3:30b-instruct",      # Haupt-LLM (Tesla P40 optimiert)
    "automatik_model": "qwen3:8b",      # Automatik-Entscheidungen
}

# Temperature Presets
TEMPERATURE_PRESETS = {
    "faktisch": 0.2,
    "gemischt": 0.5,
    "kreativ": 0.8
}
```

### HTTP Timeout Konfiguration

In `aifred/backends/ollama.py`:
- **HTTP Client Timeout**: 300 Sekunden (5 Minuten)
- ErhÃ¶ht von 60s fÃ¼r groÃŸe Research-Anfragen mit 30KB+ Context
- Verhindert Timeout-Fehler bei erster Token-Generation

### Restart-Button Verhalten

Der AIfred Restart-Button kann in zwei Modi arbeiten:

- **Production Mode** (`USE_SYSTEMD_RESTART = True`):
  - Startet den kompletten systemd-Service neu
  - BenÃ¶tigt Polkit-Regel fÃ¼r sudo-lose AusfÃ¼hrung
  - FÃ¼r produktive Systeme mit systemd

- **Development Mode** (`USE_SYSTEMD_RESTART = False`):
  - Soft-Restart: LÃ¶scht nur Caches und History
  - BehÃ¤lt laufende Instanz fÃ¼r Hot-Reload
  - FÃ¼r lokale Entwicklung ohne Service

---

## ğŸ“¦ Deployment

### Systemd Service

FÃ¼r produktiven Betrieb als Service sind vorkonfigurierte Service-Dateien im `systemd/` Verzeichnis verfÃ¼gbar.

**âš ï¸ WICHTIG**: Die Umgebungsvariable `AIFRED_ENV=prod` **MUSS** gesetzt sein, damit AIfred auf dem MiniPC lÃ¤uft und nicht auf den Entwicklungsrechner weiterleitet!

#### Schnellinstallation

```bash
# 1. Service-Dateien kopieren
sudo cp systemd/aifred-chromadb.service /etc/systemd/system/
sudo cp systemd/aifred-intelligence.service /etc/systemd/system/

# 2. Services aktivieren und starten
sudo systemctl daemon-reload
sudo systemctl enable aifred-chromadb.service aifred-intelligence.service
sudo systemctl start aifred-chromadb.service aifred-intelligence.service

# 3. Status prÃ¼fen
systemctl status aifred-chromadb.service
systemctl status aifred-intelligence.service
```

Siehe [systemd/README.md](systemd/README.md) fÃ¼r Details, Troubleshooting und Monitoring.

#### Service-Dateien (Referenz)

**1. ChromaDB Service** (`systemd/aifred-chromadb.service`):
```ini
[Unit]
Description=AIfred ChromaDB Vector Cache (Docker)
After=docker.service
Requires=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
WorkingDirectory=/home/mp/Projekte/AIfred-Intelligence/docker
ExecStart=/usr/bin/docker compose up -d chromadb
ExecStop=/usr/bin/docker compose stop chromadb
```

**2. AIfred Intelligence Service** (`systemd/aifred-intelligence.service`):
```ini
[Unit]
Description=AIfred Intelligence Voice Assistant
After=network.target ollama.service aifred-chromadb.service
Requires=ollama.service aifred-chromadb.service

[Service]
Type=simple
User=mp
Group=mp
WorkingDirectory=/home/mp/Projekte/AIfred-Intelligence
Environment="PATH=/home/mp/Projekte/AIfred-Intelligence/venv/bin:/usr/local/bin:/usr/bin:/bin"
Environment="PYTHONUNBUFFERED=1"
Environment="AIFRED_ENV=prod"
ExecStart=/home/mp/Projekte/AIfred-Intelligence/venv/bin/python -m reflex run --frontend-port 3002 --backend-port 8002 --backend-host 0.0.0.0
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

**Umgebungsvariable `AIFRED_ENV` erklÃ¤rt**:
- `AIFRED_ENV=dev` (Standard): API-URL = `http://172.30.8.72:8002` (Hauptrechner/WSL mit RTX 3060)
- `AIFRED_ENV=prod`: API-URL = `https://narnia.spdns.de:8443` (MiniPC mit Tesla P40)

Ohne `AIFRED_ENV=prod` werden alle API-Requests an den Entwicklungsrechner weitergeleitet, auch wenn Nginx korrekt konfiguriert ist!

2. Service aktivieren:
```bash
sudo systemctl daemon-reload
sudo systemctl enable aifred-intelligence
sudo systemctl start aifred-intelligence
```

3. **Optional: Polkit-Regel fÃ¼r Restart ohne sudo**

FÃ¼r den Restart-Button in der Web-UI ohne Passwort-Abfrage:

`/etc/polkit-1/rules.d/50-aifred-restart.rules`:
```javascript
polkit.addRule(function(action, subject) {
    if ((action.id == "org.freedesktop.systemd1.manage-units") &&
        (action.lookup("unit") == "aifred-intelligence.service" ||
         action.lookup("unit") == "ollama.service") &&
        (action.lookup("verb") == "restart") &&
        (subject.user == "mp")) {
        return polkit.Result.YES;
    }
});
```

---

## ğŸ› ï¸ Development

### Debug Logs
```bash
tail -f logs/aifred_debug.log
```

### Tests ausfÃ¼hren
```bash
pytest tests/
```

## ğŸ”¨ Troubleshooting

### HÃ¤ufige Probleme

#### HTTP ReadTimeout bei Research-Anfragen
**Problem**: `httpx.ReadTimeout` nach 60 Sekunden bei groÃŸen Recherchen
**LÃ¶sung**: Timeout ist bereits auf 300s erhÃ¶ht in `aifred/backends/ollama.py`
**Falls weiterhin Probleme**: Ollama Service neustarten mit `systemctl restart ollama`

#### Service startet nicht
**Problem**: AIfred Service startet nicht oder stoppt sofort
**LÃ¶sung**:
```bash
# Logs prÃ¼fen
journalctl -u aifred-intelligence -n 50
# Ollama Status prÃ¼fen
systemctl status ollama
```

#### Restart-Button funktioniert nicht
**Problem**: Restart-Button in Web-UI ohne Funktion
**LÃ¶sung**: Polkit-Regel prÃ¼fen in `/etc/polkit-1/rules.d/50-aifred-restart.rules`

---

## ğŸ“š Dokumentation

Weitere Dokumentation im `docs/` Verzeichnis:
- [Architecture Overview](docs/architecture/)
- [API Documentation](docs/api/)
- [Migration Guide](docs/infrastructure/MIGRATION.md)

---

## ğŸ¤ Contributing

Pull Requests sind willkommen! FÃ¼r grÃ¶ÃŸere Ã„nderungen bitte erst ein Issue Ã¶ffnen.

---

## ğŸ“ Session Notes - 03. November 2025

### Internationalisierung (i18n) Implementierung
- VollstÃ¤ndige Ãœbersetzungstabelle fÃ¼r UI-Strings
- Automatische Spracherkennung fÃ¼r Prompts (de/en basierend auf Nutzereingabe)
- Manueller UI-Sprachumschalter in den Einstellungen hinzugefÃ¼gt
- Englische Prompt-Dateien vervollstÃ¤ndigt (waren unvollstÃ¤ndig)

### Netzwerk- und Konfigurationsanpassungen
- `api_url` in `rxconfig.py` auf lokale IP fÃ¼r Entwicklungsumgebung korrigiert
- UmgebungsabhÃ¤ngige Konfiguration: `AIFRED_ENV=dev` vs `AIFRED_ENV=prod`
- Problem behoben: Anfragen wurden zu Mini-PC weitergeleitet statt lokal verarbeitet
- Entwicklung: `http://172.30.8.72:3002` (mit RTX 3060), Produktion: `https://narnia.spdns.de:8443`

### Bugfixes
- Parameterfehler behoben: `cache_metadata` â†’ `cache_info` in `get_decision_making_prompt()` Aufrufen
- Funktioniert jetzt korrekt mit der definierten Funktionssignatur

---

## ğŸ“„ License

MIT License - siehe [LICENSE](LICENSE) file

---

**Version**: 2.0.0 (November 2025)
**Status**: Production-Ready ğŸš€