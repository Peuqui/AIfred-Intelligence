# AIfred Intelligence - Gradio ‚Üí Reflex Migration

## Status: ‚úÖ Phase 2.5 Complete - Gradio-Style UI Recreated

Stand: 2025-10-25 (UI Update)

---

## ‚úÖ Was wurde portiert

### 1. Core Libraries (`aifred/lib/`)

Alle essentiellen Module von Gradio-Legacy wurden nach `aifred/lib/` portiert:

| Modul | Status | Beschreibung |
|-------|--------|--------------|
| `logging_utils.py` | ‚úÖ Portiert | Debug-Logging, Console-Output |
| `prompt_loader.py` | ‚úÖ Portiert | L√§dt Prompts aus `/prompts/` |
| `agent_tools.py` | ‚úÖ Portiert | Web Search, Scraping, Context-Building |
| `agent_core.py` | ‚úÖ Portiert | Research Agent, Intent Detection, URL Rating |
| `formatting.py` | ‚úÖ Portiert | Message Formatting |
| `message_builder.py` | ‚úÖ Portiert | Chat History Management |
| `config.py` | ‚úÖ Portiert | Configuration |

### 2. Prompts (`prompts/`)

‚úÖ **Alle Prompts sind bereits vorhanden und kompatibel:**
- `decision_making.txt` - Mit lokalen Aktivit√§ten-Fix
- `url_rating.txt` - Mit generischer lokaler Relevanz
- `system_rag.txt` - Keine URLs in Inline-Zitaten
- `query_optimization.txt`
- `intent_detection.txt`
- `followup_intent_detection.txt`

### 3. Backend-System (`aifred/backends/`)

‚úÖ **Bereits in Reflex vorhanden:**
- `base.py` - Abstract LLMBackend
- `ollama.py` - Ollama Adapter
- `vllm.py` - vLLM Adapter (OpenAI-kompatibel)
- `__init__.py` - BackendFactory

---

## üîÑ Was noch fehlt

### Phase 2: Research Integration ‚úÖ COMPLETE

- [x] **Web Research in Reflex State integriert**
  - ‚úÖ Research Cache Management (class-level Dict mit Lock)
  - ‚úÖ `AIState.send_message()` mit Research-Integration
  - ‚úÖ Decision-Making Logic (Automatik-Modus)
  - ‚úÖ ThreadPoolExecutor f√ºr sync agent_core ‚Üí async Reflex
  - ‚úÖ Debug-Sync zwischen lib console und Reflex State

- [x] **Settings Management UI**
  - ‚úÖ Automatik-LLM Auswahl (Dropdown)
  - ‚úÖ Research Mode (none/quick/deep/automatik)
  - ‚úÖ Temperature Slider
  - ‚úÖ Haupt-LLM vs. Automatik-LLM Trennung

### Phase 2.5: Gradio-Style UI Recreation ‚úÖ COMPLETE

- [x] **2-Column Layout wie Gradio**
  - ‚úÖ Left Column: Audio placeholder, Text input, Research mode radio, LLM parameters accordion
  - ‚úÖ Right Column: User/AI text display, TTS controls placeholder, Chat history
  - ‚úÖ Header mit Titel und Subtitle
  - ‚úÖ Bottom: Debug Console (accordion, 400px height)
  - ‚úÖ Bottom: Settings Accordion (Backend, Haupt-LLM, Automatik-LLM)

- [x] **UI Components Functional**
  - ‚úÖ Text input with disabled state w√§hrend generation
  - ‚úÖ Research mode radio buttons mit emoji icons
  - ‚úÖ Temperature slider mit dynamischer Anzeige
  - ‚úÖ Chat history display mit user/AI bubbles
  - ‚úÖ Debug console mit auto-refresh toggle
  - ‚úÖ Responsive 2-column grid layout

- [x] **Styling wie Gradio**
  - ‚úÖ √Ñhnliche Farben (#2563eb f√ºr User, #e5e7eb f√ºr AI)
  - ‚úÖ Rounded corners, padding, spacing
  - ‚úÖ Background colors (#f9fafb f√ºr readonly fields, #f3f4f6 f√ºr page)
  - ‚úÖ Emoji icons f√ºr bessere UX

### Phase 3: Audio Processing (Optional)

- [ ] **Audio Input (STT)**
  - Whisper STT Integration
  - Microphone recording UI
  - Audio waveform display

- [ ] **Audio Output (TTS)**
  - Edge TTS Integration
  - Audio playback controls
  - Voice selection UI

### Phase 4: Advanced Features

- [ ] Chat History Persistence (SQLite)
- [ ] Multi-Session Support
- [ ] Model Download/Management UI
- [ ] Performance Metrics Dashboard

---

## üìÅ Verzeichnisstruktur

```
AIfred-Intelligence/
‚îú‚îÄ‚îÄ aifred/
‚îÇ   ‚îú‚îÄ‚îÄ backends/           # ‚úÖ Multi-Backend System
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vllm.py
‚îÇ   ‚îú‚îÄ‚îÄ lib/                # ‚úÖ NEU - Portierte Gradio-Module
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging_utils.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_loader.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent_tools.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent_core.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ formatting.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ message_builder.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py
‚îÇ   ‚îú‚îÄ‚îÄ aifred.py           # Reflex UI Components
‚îÇ   ‚îî‚îÄ‚îÄ state.py            # Reflex State Management
‚îú‚îÄ‚îÄ prompts/                # ‚úÖ Alle Prompts vorhanden
‚îÇ   ‚îú‚îÄ‚îÄ decision_making.txt
‚îÇ   ‚îú‚îÄ‚îÄ url_rating.txt
‚îÇ   ‚îú‚îÄ‚îÄ system_rag.txt
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ gradio-legacy/          # ‚úÖ Referenz-Code (alte Version)
‚îÇ   ‚îú‚îÄ‚îÄ aifred_intelligence.py
‚îÇ   ‚îú‚îÄ‚îÄ agent_core.py
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ logs/                   # Debug-Logs
    ‚îî‚îÄ‚îÄ aifred_debug.log
```

---

## üîß Technische Details

### Import-Struktur

**Alte Gradio-Version:**
```python
from lib.agent_core import perform_agent_research
from lib.logging_utils import debug_print
```

**Neue Reflex-Version:**
```python
from aifred.lib import perform_agent_research
from aifred.lib import debug_print
```

### Logging-System

**Console-Output f√ºr Reflex UI:**
```python
from aifred.lib import console_print, get_console_messages

# Schreiben
console_print("üåê Web-Scraping startet...")

# Lesen (in Reflex State)
messages = get_console_messages()  # Liste aller Messages
```

**Debug-Log-File:**
- Pfad: `/home/mp/Projekte/AIfred-Intelligence/logs/aifred_debug.log`
- Automatische Rotation bei >1 MB
- Timestamp-Format: `HH:MM:SS.mmm`

### Prompt-System

**Prompts laden:**
```python
from aifred.lib import get_decision_making_prompt

prompt = get_decision_making_prompt(
    user_text="Aktivit√§ten in Kassel?",
    cache_metadata=""
)
```

### Web Research Integration (Phase 2)

**Research in Reflex State:**
```python
# aifred/state.py
class AIState(rx.State):
    # Research Settings
    research_mode: str = "automatik"  # "quick", "deep", "automatik", "none"
    automatik_model: str = "qwen3:4b"  # F√ºr Decision/Query-Opt/URL-Rating
    session_id: str = ""

    # Research Cache (class-level, shared)
    _research_cache: Dict = {}
    _cache_lock: threading.Lock = threading.Lock()

    async def send_message(self):
        """Send message with optional web research"""
        # Phase 1: Research (wenn aktiviert)
        if self.research_mode != "none":
            # Run agent_core.perform_agent_research() in ThreadPool
            with ThreadPoolExecutor() as executor:
                research_result = executor.submit(
                    perform_agent_research,
                    user_text=user_msg,
                    mode=self.research_mode,
                    model_choice=self.selected_model,
                    automatik_model=self.automatik_model,
                    history=self.chat_history,
                    session_id=self.session_id,
                    ...
                ).result()

            # Sync debug messages from lib console
            self.sync_debug_from_lib()

        # Phase 2: LLM Response (with or without RAG context)
        if research_result and research_result.get('ai_response'):
            # Research lieferte RAG-Antwort
            full_response = research_result['ai_response']
        else:
            # Normaler Chat ohne Research
            full_response = await backend.chat_stream(...)
```

**Web Research Funktionen:**
```python
from aifred.lib import search_web, scrape_webpage, build_context

# Web-Suche (Multi-API Fallback: Brave ‚Üí Tavily ‚Üí SearXNG)
results = search_web("Wetter Berlin")

# URL scrapen (Trafilatura + Playwright Fallback)
content = scrape_webpage("https://wetter.com/berlin")

# Context bauen (f√ºr RAG)
context = build_context(user_text, tool_results)
```

**UI Settings:**
- Research Mode: none / quick (3 URLs) / deep (7 URLs) / automatik (KI entscheidet)
- Automatik-LLM: Separate LLM f√ºr Decision-Making, Query-Opt, URL-Rating
- Haupt-LLM: F√ºr finale Antwort-Generierung

---

## üß™ Testing

### Import-Tests

```bash
# Aktiviere venv
source venv/bin/activate

# Teste Imports
python -c "from aifred.lib import debug_print; print('‚úÖ OK')"
python -c "from aifred.lib import search_web; print('‚úÖ OK')"
python -c "from aifred.lib import perform_agent_research; print('‚úÖ OK')"
```

### Reflex-Server starten

```bash
source venv/bin/activate
reflex run
```

√ñffne: `http://192.168.0.252:3002`

---

## üìù N√§chste Schritte

### ‚úÖ Phase 2 Complete - Bereit zum Testen!

Die Web-Research-Integration ist vollst√§ndig portiert. N√§chste Schritte:

1. **Ollama starten & Testen**
   ```bash
   # Ollama starten
   systemctl start ollama

   # Models pr√ºfen
   ollama list

   # Ben√∂tigte Models pullen
   ollama pull qwen3:8b
   ollama pull qwen3:4b

   # Reflex starten
   source venv/bin/activate
   reflex run
   ```

2. **Test-Szenarien**
   - **Research Mode: none** ‚Üí Normaler Chat (kein Web Search)
   - **Research Mode: quick** ‚Üí 3 URLs scraped
   - **Research Mode: deep** ‚Üí 7 URLs scraped (mit Fallback)
   - **Research Mode: automatik** ‚Üí KI entscheidet (Decision-Making)

3. **Debug-Console beobachten**
   - Web-Scraping-Fortschritt
   - URL-Rating Scores
   - Cache-Hits/Misses
   - LLM Performance Stats

### Phase 3: Optional Features

4. **Audio Processing portieren**
   - Whisper STT Integration (Voice Input)
   - Edge TTS Integration (Voice Output)
   - Audio UI Components

5. **Advanced Features**
   - Chat History Persistence (SQLite/Redis)
   - Multi-User Support
   - Model Download UI
   - Performance Dashboard

---

## üîó Referenzen

- **Gradio-Legacy Branch:** `origin/gradio-legacy`
- **Gradio-Legacy Code:** `/home/mp/Projekte/AIfred-Intelligence/gradio-legacy/`
- **Reflex Docs:** https://reflex.dev/docs/getting-started/introduction/
- **GitHub Repo:** https://github.com/Peuqui/AIfred-Intelligence

---

## ‚ö†Ô∏è Bekannte Warnungen

### Reflex Deprecation Warning
```
DeprecationWarning: rx.Base has been deprecated in version 0.8.15.
<class 'aifred.state.ChatMessage'> is subclassing rx.Base.
```

**Fix:** Migriere `ChatMessage` zu `pydantic.BaseModel` statt `rx.Base`

```python
# Vorher (alt)
class ChatMessage(rx.Base):
    role: str
    content: str

# Nachher (neu)
from pydantic import BaseModel

class ChatMessage(BaseModel):
    role: str
    content: str
```

---

**Erstellt:** 2025-10-25
**Autor:** Claude Code
**Version:** Phase 1 - Library Portierung Complete
