#!/bin/bash

echo "ğŸ¤– AIfred Intelligence - Complete Model Download"
echo "================================================="
echo ""
echo "âš ï¸  Basierend auf Modell-Evaluation vom November 2025"
echo "âœ… Downloads alle empfohlenen Modelle fÃ¼r beide Backends"
echo ""

# ============================================================
# ğŸ¯ OLLAMA MODELS (GGUF)
# ============================================================
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ¤– SCHRITT 1: Ollama Models (GGUF Q4/Q8)"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
read -p "Ollama-Modelle jetzt herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    ./download_ollama_models.sh
else
    echo "â­ï¸  Ollama-Download Ã¼bersprungen"
fi

# ============================================================
# ğŸš€ vLLM MODELS (AWQ)
# ============================================================
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸš€ SCHRITT 2: vLLM Models (AWQ Quantization)"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
read -p "vLLM-Modelle jetzt herunterladen? (y/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    ./download_vllm_models.sh
else
    echo "â­ï¸  vLLM-Download Ã¼bersprungen"
fi

# ============================================================
# ğŸ“ FINAL SUMMARY
# ============================================================
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ‰ Model Download Abgeschlossen!"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“Š Multi-Backend Setup:"
echo ""
echo "ğŸ”¹ Ollama (GGUF Q4/Q8):"
echo "   - Beste KompatibilitÃ¤t"
echo "   - Qwen3:30b-instruct (18GB) - Haupt-LLM, 256K context"
echo "   - Qwen3:8b (5.2GB) - Automatik, optional thinking"
echo "   - Qwen2.5:3b (1.9GB) - Ultra-schnelle Automatik"
echo ""
echo "ğŸ”¹ vLLM (AWQ 4-bit):"
echo "   - Beste Performance (AWQ Marlin kernel)"
echo "   - Qwen3-8B-AWQ (~5GB, 40Kâ†’128K mit YaRN)"
echo "   - Qwen3-14B-AWQ (~8GB, 32Kâ†’128K mit YaRN)"
echo "   - Qwen2.5-14B-Instruct-AWQ (~8GB, 128K native)"
echo ""
echo "ğŸ’¾ Speicherplatz Total:"
echo "   Ollama Core: ~25 GB"
echo "   vLLM AWQ: ~15-20 GB (je nach Auswahl)"
echo "   Total: ~40-60 GB"
echo ""
echo "ğŸ’¡ Empfohlene Konfiguration fÃ¼r P40 24GB:"
echo "   - Backend: vLLM (fÃ¼r maximale Performance)"
echo "   - Main LLM: Qwen3-14B-AWQ + YaRN factor=2.0 (64K context)"
echo "   - Automatik: Qwen2.5:3b (Ollama, ultra-schnell)"
echo ""
echo "ğŸ§® YaRN Context Extension (fÃ¼r Qwen3 + vLLM):"
echo "   - Native: 32K-40K tokens"
echo "   - YaRN factor=2.0: 64K (empfohlen fÃ¼r Chat-Historie)"
echo "   - YaRN factor=4.0: 128K (fÃ¼r lange Dokumente)"
echo ""
echo "ğŸ“ Weitere Infos:"
echo "   - Ollama Details: ./download_ollama_models.sh"
echo "   - vLLM Details: ./download_vllm_models.sh"
echo "   - YaRN Config: Siehe vLLM script summary"
echo ""
echo "âœ… Multi-Backend Setup bereit!"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
