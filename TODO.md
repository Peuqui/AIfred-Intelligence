# AIfred Intelligence - TODO Liste

## üöÄ Priorit√§t: Hoch

### History-Summarization (Automatische Context-Kompression)

**Zweck**: Verhindert Context-Overflow bei langen Konversationen

**Triggerpunkt**:
- Wenn `estimated_tokens(system_prompt + history + user_message) > context_window_limit`
- Berechnung VOR dem LLM-Call

**Strategie**:
1. **Berechne ben√∂tigte Token-Reduktion**:
   ```python
   overflow_tokens = estimated_tokens - context_window_limit
   target_reduction = overflow_tokens + buffer (z.B. 2048 Tokens Reserve)
   ```

2. **Summarize √§lteste History-Eintr√§ge**:
   - Nimm √§lteste N Messages (paarweise: User + AI)
   - Sende an Automatik-LLM: "Fasse diese Konversation zusammen (max X Tokens)"
   - Ersetze diese Messages durch 1 System-Message mit Summary

3. **Iterativ reduzieren bis Platz frei**:
   - Wenn ein Summary nicht reicht: N√§chste N Messages summarizen
   - Stopp wenn: `estimated_tokens <= context_window_limit`

4. **Behalte neueste Messages unver√§ndert**:
   - Letzten 2-4 Messages (1-2 Runden) nie summarizen
   - Wichtig f√ºr unmittelbaren Kontext

**Beispiel**:
```
Vor Summarization (10.000 Tokens):
1. User: "Wetter Berlin?"
2. AI: "15¬∞C, sonnig..."
3. User: "Und M√ºnchen?"
4. AI: "18¬∞C, bew√∂lkt..."
5. User: "Unterschied?"
6. AI: "M√ºnchen 3¬∞C w√§rmer..."
[Context Limit erreicht!]

Nach Summarization (4.000 Tokens):
1. System: "User fragte Wetter Berlin (15¬∞C) und M√ºnchen (18¬∞C), Unterschied besprochen"
2. User: "Unterschied?"
3. AI: "M√ºnchen 3¬∞C w√§rmer..."
[6.000 Tokens gespart!]
```

**Implementation-Details**:
- Funktion: `async def summarize_history_if_needed(messages, context_limit) -> messages`
- Aufruf: In `agent_core.py` VOR dem LLM-Call
- LLM: Automatik-LLM verwenden (schnell + g√ºnstig)
- Prompt: "Fasse folgende Konversation pr√§gnant zusammen (max {target_tokens} Tokens)"

**Vorteile**:
- ‚úÖ Keine FIFO-Verluste mehr (√§ltere Infos bleiben als Summary)
- ‚úÖ Mehr Platz f√ºr neue Research-Daten
- ‚úÖ LLM beh√§lt Kontext √ºber l√§ngere Sessions

**Nachteile**:
- ‚ö†Ô∏è Informationsverlust bei Details
- ‚ö†Ô∏è Extra LLM-Call (~2-3 Sekunden)
- ‚ö†Ô∏è Komplexit√§t erh√∂ht

---

## üîß Weitere TODOs

### Performance

#### LLM Pre-Loading (Paralleles Model-Loading)

**Zweck**: Zeit sparen durch paralleles Laden von Haupt-LLM w√§hrend Automatik-LLM arbeitet

**Problem**:
- Aktuell: Automatik-LLM fertig ‚Üí dann erst Haupt-LLM laden ‚Üí User wartet
- Legacy hatte das besser: Haupt-LLM l√§dt bereits im Hintergrund

**Strategie**:
1. **W√§hrend Intent-Detection l√§uft**: Haupt-LLM vorw√§rmen (dummy call)
2. **W√§hrend Query-Optimizer l√§uft**: Haupt-LLM im Speicher halten
3. **W√§hrend URL-Rating l√§uft**: Haupt-LLM ist bereits geladen
4. **W√§hrend Web-Scraping l√§uft**: Haupt-LLM bereit f√ºr sofortigen Einsatz

**Implementation**:
```python
# Beispiel: Parallel Pre-Loading
import asyncio

async def preload_haupt_llm():
    """Dummy call to load model into VRAM"""
    await llm_client.chat(
        model=haupt_model,
        messages=[{"role": "user", "content": "test"}],
        options={"num_predict": 1}  # Nur 1 Token generieren
    )

# Im Agent-Core:
# Starte Pre-Loading parallel zu Automatik-Aufgaben
asyncio.create_task(preload_haupt_llm())  # Fire & forget
```

**Zeiteinsparung**:
- Model-Loading: ~3-5 Sekunden (bei gro√üen Models wie qwen3:8b)
- Gesamtersparnis: 3-5 Sekunden pro Web-Recherche

**Legacy-Referenz**: `gradio-legacy/agent_core.py` - hatte Model-Warming

---

- [ ] Paralleles Scraping optimieren (Timeout-Handling)
- [ ] Cache-Warming beim App-Start
- [ ] **LLM Pre-Loading implementieren** (siehe oben)

### UI/UX
- [ ] Quellen-Links in LLM-Antwort unterdr√ºcken (redundant)
- [ ] Progress-Indicator f√ºr lange Scraping-Vorg√§nge

### Code Quality
- [ ] Unit-Tests f√ºr Context-Manager
- [ ] Integration-Tests f√ºr Cache-System

---

**Erstellt**: 30.10.2025
**Letztes Update**: 30.10.2025
