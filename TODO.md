# AIfred Intelligence - TODO Liste

## ğŸš€ PrioritÃ¤t: Hoch

### History-Summarization (Automatische Context-Kompression)

**Zweck**: Verhindert Context-Overflow bei langen Konversationen

**Triggerpunkt**:
- Wenn `estimated_tokens(system_prompt + history + user_message) > context_window_limit`
- Berechnung VOR dem LLM-Call

**Strategie**:
1. **Berechne benÃ¶tigte Token-Reduktion**:
   ```python
   overflow_tokens = estimated_tokens - context_window_limit
   target_reduction = overflow_tokens + buffer (z.B. 2048 Tokens Reserve)
   ```

2. **Summarize Ã¤lteste History-EintrÃ¤ge**:
   - Nimm Ã¤lteste N Messages (paarweise: User + AI)
   - Sende an Automatik-LLM: "Fasse diese Konversation zusammen (max X Tokens)"
   - Ersetze diese Messages durch 1 System-Message mit Summary

3. **Iterativ reduzieren bis Platz frei**:
   - Wenn ein Summary nicht reicht: NÃ¤chste N Messages summarizen
   - Stopp wenn: `estimated_tokens <= context_window_limit`

4. **Behalte neueste Messages unverÃ¤ndert**:
   - Letzten 2-4 Messages (1-2 Runden) nie summarizen
   - Wichtig fÃ¼r unmittelbaren Kontext

**Beispiel**:
```
Vor Summarization (10.000 Tokens):
1. User: "Wetter Berlin?"
2. AI: "15Â°C, sonnig..."
3. User: "Und MÃ¼nchen?"
4. AI: "18Â°C, bewÃ¶lkt..."
5. User: "Unterschied?"
6. AI: "MÃ¼nchen 3Â°C wÃ¤rmer..."
[Context Limit erreicht!]

Nach Summarization (4.000 Tokens):
1. System: "User fragte Wetter Berlin (15Â°C) und MÃ¼nchen (18Â°C), Unterschied besprochen"
2. User: "Unterschied?"
3. AI: "MÃ¼nchen 3Â°C wÃ¤rmer..."
[6.000 Tokens gespart!]
```

**Implementation-Details**:
- Funktion: `async def summarize_history_if_needed(messages, context_limit) -> messages`
- Aufruf: In `agent_core.py` VOR dem LLM-Call
- LLM: Automatik-LLM verwenden (schnell + gÃ¼nstig)
- Prompt: "Fasse folgende Konversation prÃ¤gnant zusammen (max {target_tokens} Tokens)"

**Vorteile**:
- âœ… Keine FIFO-Verluste mehr (Ã¤ltere Infos bleiben als Summary)
- âœ… Mehr Platz fÃ¼r neue Research-Daten
- âœ… LLM behÃ¤lt Kontext Ã¼ber lÃ¤ngere Sessions

**Nachteile**:
- âš ï¸ Informationsverlust bei Details
- âš ï¸ Extra LLM-Call (~2-3 Sekunden)
- âš ï¸ KomplexitÃ¤t erhÃ¶ht

---

## ğŸ”§ Weitere TODOs

### Performance

#### âœ… LLM Pre-Loading (IMPLEMENTIERT - 01.11.2025)

**Status**: âœ… Implementiert und getestet

**Ergebnis**:
- Performance-Gewinn: **23.3 Sekunden (35% schneller)** bei Cache-Hits
- Automatik-LLM: Preload beim App-Start
- Haupt-LLM: Preload wÃ¤hrend Web-Scraping (parallel)

**Details**: Siehe Commits vom 01.11.2025

---

- [x] Paralleles Scraping optimieren (Timeout-Handling) âœ… DONE (01.11.2025 - Globaler Timeout 60s, keine API-spezifischen Overrides)
- [ ] Cache-Warming beim App-Start
- [x] **LLM Pre-Loading implementieren** âœ… DONE (01.11.2025)

### UI/UX
- [x] Quellen-Links in LLM-Antwort unterdrÃ¼cken (redundant) âœ… DONE (vorherige Session)
- [x] **Progress-Indicator fÃ¼r lange Scraping-VorgÃ¤nge** âœ… DONE (01.11.2025)
  - **Status**: VollstÃ¤ndig implementiert
  - **Features**:
    - Phase 1: Automatik-Entscheidung (pulsierend)
    - Phase 2: Web-Scraping (Fortschrittsbalken X/Y URLs + Fehleranzahl)
    - Phase 3: Generiere Antwort (pulsierend)
    - Funktioniert auch bei Cache-Hit korrekt
  - **Dateien**: `aifred/aifred.py`, `aifred/state.py`, `aifred/lib/agent_core.py`

### Internationalisierung (i18n)

**Zweck**: AIfred fÃ¼r internationale Nutzer verfÃ¼gbar machen

**Vorteile**:
- âœ… Englische/mehrsprachige User kÃ¶nnen AIfred nutzen
- âœ… Bessere LLM Performance pro Sprache (EN prompts fÃ¼r EN-only models)
- âœ… Open-Source ready fÃ¼r internationale Community
- âœ… FlexibilitÃ¤t bei LLM-Wahl

**Struktur**:
```
prompts/
â”œâ”€â”€ de/
â”‚   â”œâ”€â”€ system_rag.txt
â”‚   â”œâ”€â”€ system_rag_cache_hit.txt
â”‚   â”œâ”€â”€ decision_making.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ en/
â”‚   â”œâ”€â”€ system_rag.txt
â”‚   â”œâ”€â”€ system_rag_cache_hit.txt
â”‚   â”œâ”€â”€ decision_making.txt
â”‚   â””â”€â”€ ...
â””â”€â”€ config.yaml  # Language: de/en/auto
```

**Implementation**:
- [ ] Prompt-Ordnerstruktur nach Sprachen aufteilen
- [ ] Language-Parameter in Config (de/en/auto)
- [ ] Auto-Detection: User-Sprache erkennen (aus erster Frage)
- [ ] UI-Strings internationalisieren (Reflex i18n)
- [ ] Debug-Messages mehrsprachig

**Phasen**:
1. Phase 1: Deutsche + Englische Prompts (Hauptsprachen)
2. Phase 2: UI-Strings mehrsprachig
3. Phase 3: Auto-Detection implementieren
4. Phase 4: Weitere Sprachen (FR, ES, IT, etc.)

### Code Quality
- [ ] Unit-Tests fÃ¼r Context-Manager
- [ ] Integration-Tests fÃ¼r Cache-System

---

**Erstellt**: 30.10.2025
**Letztes Update**: 01.11.2025 (Session 2: Progress-Indicator, UI-Fixes, Cache-Hit Progress-Flow)
