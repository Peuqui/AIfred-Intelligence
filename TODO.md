# AIfred Intelligence - TODO List

## High Priority

### TTL-based Vector Cache System
**Problem**: Semi-volatile queries (news, events) mit Keywords wie "aktuell", "neueste" werden aktuell nicht gecacht, obwohl die Daten fÃ¼r 24h wertvoll wÃ¤ren.

**Example**: "Recherchiere die aktuellen Ereignisse im Israelkrieg" wird als volatile erkannt und nicht gecacht, obwohl die Info fÃ¼r 1 Tag wiederverwendbar wÃ¤re.

**Solution Options**:
- **Option A**: Simple TTL parameter per cache entry (24h expiration for volatile queries)
- **Option B**: LLM-based volatility level detection (hourly/daily/permanent)
- **Option C**: User toggle to override volatile detection and force caching

**Implementation Details**:
- ChromaDB hat kein natives TTL â†’ Manual cleanup job needed
- Add `expires_at` timestamp to cache metadata
- Background task to delete expired entries (daily cleanup)
- Modify `vector_cache.py` to support TTL parameter
- Update `cache_decision` prompt to classify volatility level

**Files to modify**:
- `aifred/lib/vector_cache.py` - Add TTL support
- `aifred/lib/research/context_builder.py` - Pass TTL parameter based on LLM decision
- `prompts/cache_decision_de.txt` / `prompts/cache_decision_en.txt` - Add volatility level classification

**Status**: Not started

---

## Medium Priority

(Future tasks go here)

---

## Low Priority

(Future tasks go here)

---

## Completed

### Fix: Automatic Date/Time Injection in All Prompts (2025-11-15)
**Problem**: LLM hatte kein Bewusstsein fÃ¼r aktuelles Datum/Zeit, konnte nicht unterscheiden ob Trainingsdaten veraltet sind oder welches Jahr/Monat aktuell ist.

**Root Cause**:
- Kein System-Prompt mit Datum/Zeit im "Eigenes Wissen"-Modus
- Manuelle Datum-Injektion in einigen Prompts (system_rag, query_optimization), aber inkonsistent
- Keine zentrale Stelle fÃ¼r Timestamp-Injection

**Solution Implemented**:
**Zentrale Timestamp-Injection in `load_prompt()`** (Option 2 - User-approved):
1. âœ… `load_prompt()` injiziert automatisch Datum/Zeit **vor** jeden geladenen Prompt
2. âœ… Neue minimale System-Prompts fÃ¼r "Eigenes Wissen"-Modus (`system_minimal.txt`)
3. âœ… Alle manuellen `{current_date}` / `{current_year}` Platzhalter aus Prompts entfernt
4. âœ… Convenience-Functions cleanup (get_query_optimization_prompt, get_decision_making_prompt, get_system_rag_prompt)

**Implementation Details**:

**1. Central Timestamp Injection** - [aifred/lib/prompt_loader.py](aifred/lib/prompt_loader.py) Lines 123-152
```python
# Inject current date/time (always, for all prompts)
now = datetime.now()
if lang == "de":
    timestamp_prefix = f"""AKTUELLES DATUM UND UHRZEIT:
- Datum: {weekday_de}, {now.strftime('%d.%m.%Y')}
- Uhrzeit: {now.strftime('%H:%M:%S')} Uhr
- Jahr: {now.year}

"""
# Prepend to every loaded prompt
prompt_template = timestamp_prefix + prompt_template
```

**2. Minimal System Prompts** - [prompts/de/system_minimal.txt](prompts/de/system_minimal.txt), [prompts/en/system_minimal.txt](prompts/en/system_minimal.txt)
```
Du bist ein hilfreicher AI-Assistent.

Beantworte Fragen prÃ¤zise und nutze dein Trainingswissen. Wenn deine Trainingsdaten veraltet sind oder du dir unsicher bist, weise den Nutzer darauf hin.
```
(Timestamp wird automatisch von `load_prompt()` vorangestellt)

**3. Eigenes Wissen Mode Update** - [aifred/lib/conversation_handler.py](aifred/lib/conversation_handler.py) Lines 360-363
```python
# Inject minimal system prompt with timestamp
from .prompt_loader import load_prompt
system_prompt_minimal = load_prompt('system_minimal', lang=detected_user_language)
messages.insert(0, {"role": "system", "content": system_prompt_minimal})
```

**4. Cleanup - Removed Manual Date Injection**:
- [aifred/lib/prompt_loader.py](aifred/lib/prompt_loader.py) Lines 195-230: Removed `current_date`/`current_year` parameters from convenience functions
- [aifred/lib/research/context_builder.py](aifred/lib/research/context_builder.py) Lines 102-108: Removed manual date params
- **All prompt files**: Removed `{current_date}` and `{current_year}` placeholders (10 files updated)

**Files Modified**:
- [aifred/lib/prompt_loader.py](aifred/lib/prompt_loader.py) - Central timestamp injection + cleanup
- [aifred/lib/conversation_handler.py](aifred/lib/conversation_handler.py) - System prompt for "Eigenes Wissen"
- [aifred/lib/research/context_builder.py](aifred/lib/research/context_builder.py) - Removed manual date params
- [prompts/de/system_minimal.txt](prompts/de/system_minimal.txt) - New minimal system prompt (German)
- [prompts/en/system_minimal.txt](prompts/en/system_minimal.txt) - New minimal system prompt (English)
- 10 prompt files updated (system_rag, query_optimization, decision_making, cache_decision, etc.)

**How it works**:
1. **Every `load_prompt()` call** automatically prepends timestamp (Date, Time, Weekday, Year)
2. **"Eigenes Wissen" mode** loads `system_minimal.txt` â†’ gets timestamp automatically
3. **Web-Recherche modes** load `system_rag.txt` â†’ gets timestamp automatically
4. **Decision-Making & Query-Optimization** â†’ get timestamp automatically
5. **All modes** now have consistent date/time awareness

**Result**:
âœ… LLM knows current date/time in ALL modes (Eigenes Wissen, Quick, Deep, Automatik)
âœ… Can determine if training data is outdated
âœ… Can use current year for query expansion ("neueste Ereignisse" â†’ adds current year)
âœ… Central maintenance - only one place to update timestamp format
âœ… No duplicate timestamp injection

**Additional Fixes (2025-11-15 Evening)**:

**Fix 1: Missing Import in state.py**
- **Problem**: "Eigenes Wissen" mode crashed with `ModuleNotFoundError: No module named 'aifred.lib.language_detection'`
- **Root Cause**: [aifred/state.py](aifred/state.py#L990) tried to import `detect_language` from non-existent module
- **Solution**: Changed import to `from .lib.prompt_loader import load_prompt, detect_language` (Lines 990)
- **Files Modified**: [aifred/state.py](aifred/state.py#L989-L993)

**Fix 2: Research Mode Settings Not Persisting**
- **Problem**: Research mode reset to "Automatik" on every restart, even though `settings.json` stored `"research_mode": "none"`
- **Root Cause**: [aifred/state.py](aifred/state.py#L219) loaded `research_mode` from settings, but `research_mode_display` (UI value) was never updated â†’ UI always showed default "ðŸ¤– Automatik"
- **Solution**: Added automatic `research_mode_display` sync when loading settings (Lines 221-223)
- **Implementation**:
```python
# Update research_mode_display to match loaded research_mode
from .lib import TranslationManager
self.research_mode_display = TranslationManager.get_research_mode_display(self.research_mode)
```
- **Files Modified**: [aifred/state.py](aifred/state.py#L219-L223)

**Status**: âœ… Completed & Tested (2025-11-15)
- âœ… Timestamp injection working in all 4 research modes
- âœ… "Eigenes Wissen" mode no longer crashes
- âœ… Research mode settings persist across restarts
- âœ… LLM correctly knows current date (tested: November 15, 2024)

---

### Fix: Research Mode Persistence (2025-11-14)
**Problem**: Research Mode wurde beim Programmstart nicht korrekt wiederhergestellt. Einstellung ging verloren.

**Root Cause**:
- `set_research_mode_display()` setzte die Variable, rief aber `_save_settings()` nicht auf
- Settings-Infrastruktur war komplett da (Laden + Speichern im Dict)
- Nur der **Trigger** zum Speichern fehlte bei Mode-Ã„nderung
- Inkonsistent mit anderen Settern wie `set_automatik_model()` (Zeile 1319), die korrekt speichern

**Solution Implemented**:
- Added `_save_settings()` call in `set_research_mode_display()` (Line 1314)
- Macht Setter konsistent: Jede User-Einstellung ruft `_save_settings()` auf
- Settings werden sofort persistiert wenn Mode geÃ¤ndert wird

**How it works**:
1. **Startup**: LÃ¤dt `research_mode` aus `settings.json` (Zeile 219) oder Fallback zu `"automatik"` aus `config.py`
2. **User changes mode**: UI ruft `set_research_mode_display()` â†’ `_save_settings()` â†’ `settings.json` updated
3. **Next startup**: Gespeicherter Mode wird wiederhergestellt

**Files Modified**:
- [aifred/state.py](aifred/state.py) Line 1314

**Status**: âœ… Completed (2025-11-14)
**Result**: Research Mode wird korrekt gespeichert und beim Programmstart wiederhergestellt

---

### ðŸŽ¯ Milestone: Progress UI System Complete (2025-11-14)
**Achievement**: VollstÃ¤ndig funktionierendes Progress-Feedback-System Ã¼ber alle Research-Modi hinweg

**Problems Solved**:
1. **Progress Bar unsichtbar** - Orange Fill-Bar wurde durch CSS-Override versteckt
2. **Status Text nicht dynamisch** - "Generiere Antwort" wurde im "none"-Modus nicht angezeigt
3. **Quick/Deep Modi ohne Progress-Events** - Web-Scraping und LLM-Phasen wurden nicht angezeigt
4. **Keine Pulsier-Animation** - "Generiere Antwort" pulsierte nicht wÃ¤hrend LLM-Inferenz

**Solutions Implemented**:

#### 1. CSS Fix - Progress Bar Visibility
- **File**: [assets/custom.css](assets/custom.css) Line 90
- **Change**: Removed `.rt-Box` from dark background CSS selector
- **Reason**: `.rt-Box { background-color: #161b22 !important; }` overwrote orange progress bar color
- **Result**: Progress bar orange fill (#e67700) now visible in all modes

#### 2. Progress Event Handling - Quick/Deep Modes
- **File**: [aifred/state.py](aifred/state.py) Lines 942-960
- **Change**: Added complete progress event handling to Quick/Deep research modes
- **Events Added**: `progress`, `history_update`, `thinking_warning`
- **Logic**: Copied identical event handling from Automatik mode
- **Result**: Web-Scraping progress (1/3, 2/3, 3/3) now displayed correctly

#### 3. Dynamic Status Text - None Mode
- **File**: [aifred/aifred.py](aifred/aifred.py) Lines 449-464
- **Change**: Added `is_generating` state check for idle text display
- **Logic**: Shows "Generiere Antwort" when `is_generating=True`, even if `progress_active=False`
- **Result**: Status text updates correctly in "Eigenes Wissen (schnell)" mode

#### 4. Pulsing Animation - All Modes
- **File**: [aifred/aifred.py](aifred/aifred.py) Lines 526, 539, 543
- **Change**: Animation triggers on `progress_active | is_generating`
- **Before**: Only pulsed when `progress_active=True` (research modes only)
- **After**: Pulses during any LLM activity (research + none mode)
- **Result**: Visual feedback during all LLM inference phases

**Investigation Method**:
1. Git bisecting: Tested commits c282fdc (working) â†’ main (broken)
2. Identified breaking commit: d8e4d55 ("Force dark theme")
3. CSS analysis revealed `.rt-Box` override issue
4. Code comparison: Automatik mode vs Quick/Deep mode event handling
5. Systematic testing of each fix in all 4 research modes

**Technical Details**:
- Progress events flow: `scraper_orchestrator.py` â†’ `orchestrator.py` â†’ `state.py` â†’ `aifred.py`
- Event types: `progress` (scraping, llm, compress), `debug`, `content`, `result`, `history_update`
- State variables: `progress_active`, `progress_phase`, `progress_current`, `progress_total`, `is_generating`
- CSS specificity: `!important` overrides component-specific styling
- Reflex reactive rendering: `rx.cond()` for conditional UI updates

**Files Modified**:
- [assets/custom.css](assets/custom.css) Line 90
- [aifred/state.py](aifred/state.py) Lines 942-960
- [aifred/aifred.py](aifred/aifred.py) Lines 449-464, 526, 539, 543

**Testing Results**:
- âœ… **Automatik Mode**: Progress bar + pulsing "Automatik-Entscheidung" â†’ "Web-Scraping 1/7" â†’ "Generiere Antwort"
- âœ… **Quick Mode**: Progress bar + pulsing "Web-Scraping 1/3" â†’ "Generiere Antwort"
- âœ… **Deep Mode**: Progress bar + pulsing "Web-Scraping 1/7" â†’ "Generiere Antwort"
- âœ… **None Mode**: Pulsing "Generiere Antwort" (no web scraping, as expected)

**Status**: âœ… **MILESTONE COMPLETED** (2025-11-14)
**Impact**: Consistent, professional UI feedback across all research modes. User always knows what's happening.

---

### Fix: Broken Progress Bar in Web-Scraping
**Problem**: Progress bar wurde angezeigt (Box-Container sichtbar), aber der orangene FÃ¼llbalken war unsichtbar.

**Root Cause**:
- Commit d8e4d55 ("Force dark theme") fÃ¼gte CSS-Override hinzu: `.rt-Box { background-color: #161b22 !important; }`
- Dies Ã¼berschrieb die orangene Progress Bar Farbe (`#e67700`) mit dunklem Grau
- Der innere `rx.box` mit `background_color=COLORS["primary"]` wurde unsichtbar

**Investigation Method**:
1. Systematisches Git-Bisecting: Commit-fÃ¼r-Commit Testing von c282fdc (working) bis main (broken)
2. Identifiziert: d8e4d55 als ersten kaputten Commit
3. CSS-Analyse zeigte `.rt-Box` Override als Problem

**Solution Implemented**:
- Entfernte `.rt-Box` aus dem globalen dark background CSS-Override
- BehÃ¤lt dark theme fÃ¼r andere Container (Card, Container, Section, etc.)
- Progress Bar kann jetzt seine orangene Farbe behalten

**Files Modified**:
- [assets/custom.css](assets/custom.css) Line 90 - Removed `.rt-Box` from selector

**Status**: âœ… Completed (2025-11-14)
**Result**: Progress Bar funktioniert wieder in allen Modi (Automatik, Quick, Deep)

---

### Fix: Automatik-LLM Double-Loading on Startup
**Problem**: Automatik-LLM wurde beim Start zweimal geladen (Load â†’ Unload â†’ Reload), sichtbar in `nvidia-smi` als VRAM-Thrashing innerhalb von Sekunden.

**Root Cause**:
- `state.py` Line 93 setzte `automatik_model` mit hardcodiertem `config.py` Default bei Class-Init
- `settings.json` wurde erst spÃ¤ter in `on_load()` geladen
- Resultat: Falsches Modell initial geladen â†’ richtiges Modell nachgeladen â†’ unnÃ¶tiger Cycle

**Solution Implemented**:
- Models werden nicht mehr bei Class-Definition initialisiert (leere Strings)
- `settings.json` wird ZUERST geladen in `on_load()`
- `config.py` dient nur als Fallback wenn `settings.json` leer/nicht vorhanden
- Betrifft sowohl `selected_model` als auch `automatik_model`

**Files Modified**:
- [aifred/state.py](aifred/state.py) Lines 89-95, 239-256

**Status**: âœ… Completed (2025-11-14)
**Result**: Keine doppelten Model-Loads mehr beim Start, kein VRAM-Thrashing

---
