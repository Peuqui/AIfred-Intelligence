"""
Ollama Wrapper - Centralized GPU control for all ollama.chat() calls

This wrapper patches ollama.chat() to automatically inject num_gpu parameter
based on the enable_gpu setting, without needing to modify every call site.

Features:
- Automatic hardware detection (GPU type, VRAM, vendor)
- Dynamic configuration based on available resources
- Portable across different systems (AMD iGPU, NVIDIA RTX, etc.)
- Fallback to CPU for problematic model+hardware combinations

Usage:
    # Set GPU mode at request start
    set_gpu_mode(enable_gpu=False)  # CPU only

    # All subsequent ollama.chat() calls will AUTOMATICALLY use this setting
    response = ollama.chat(model="qwen3:8b", messages=messages)
    # ‚Üë Internally becomes: ollama.chat(..., options={"num_gpu": 0})
"""

import ollama
from .logging_utils import debug_print, console_print
from threading import local

# Thread-local storage f√ºr GPU-Einstellung und LLM-Parameter
_thread_local = local()

# Original ollama.chat function (wird beim ersten Import gespeichert)
_original_ollama_chat = ollama.chat


def _log_ollama_performance(response) -> None:
    """
    Loggt Ollama Performance-Metriken aus der Response (zentral f√ºr alle ollama.chat() Aufrufe).

    Output:
    - Journal-Control (debug_print): Vollst√§ndige Metriken (Prompt t/s, Gen t/s, Zeit)
    - Browser Console (console_print): Nur Generation t/s (kompakt)

    Args:
        response: Ollama Response-Dict oder Pydantic-Objekt
    """
    try:
        # Konvertiere Pydantic zu Dict falls n√∂tig
        if hasattr(response, 'model_dump'):
            data = response.model_dump()
        else:
            data = dict(response) if not isinstance(response, dict) else response

        # Extrahiere Metriken
        prompt_tokens = data.get('prompt_eval_count', 0)
        prompt_ns = data.get('prompt_eval_duration', 0)
        gen_tokens = data.get('eval_count', 0)
        gen_ns = data.get('eval_duration', 0)
        total_ns = data.get('total_duration', 0)

        # Berechne Tokens/Sekunde
        if prompt_ns > 0:
            prompt_tps = prompt_tokens / (prompt_ns / 1e9)
        else:
            prompt_tps = 0

        if gen_ns > 0:
            gen_tps = gen_tokens / (gen_ns / 1e9)
        else:
            gen_tps = 0

        total_s = total_ns / 1e9

        # Journal-Control: Vollst√§ndige Metriken
        if prompt_tps > 0 and gen_tps > 0:
            debug_print(f"   ‚ö° {prompt_tps:.0f} t/s Prompt | {gen_tps:.0f} t/s Gen | {total_s:.1f}s")
        elif gen_tps > 0:
            debug_print(f"   ‚ö° {gen_tps:.0f} t/s Gen | {total_s:.1f}s")
        else:
            debug_print(f"   ‚ö° {total_s:.1f}s")

        # Browser Console: Nur Generation t/s (kompakt)
        if gen_tps > 0:
            console_print(f"‚ö° {gen_tps:.0f} t/s")

    except Exception as e:
        debug_print(f"‚ö†Ô∏è Fehler beim Formatieren von Ollama Performance: {e}")



def _patched_ollama_chat(*args, **kwargs):
    """
    Patched ollama.chat() that injects num_gpu based on GPU toggle setting.

    Features:
    - CPU-only mode (num_gpu=0) when GPU toggle disabled
    - GPU Auto-Detect (no num_gpu) when GPU toggle enabled ‚Üí Ollama optimizes automatically
    - Custom LLM parameters (temperature, top_p, top_k, etc.) from UI
    """
    # Hole aktuellen GPU-Modus und custom Parameter aus Thread-Local-Storage
    enable_gpu = getattr(_thread_local, 'enable_gpu', None)
    custom_options = getattr(_thread_local, 'custom_options', {})

    if enable_gpu is not None:
        if 'options' not in kwargs:
            kwargs['options'] = {}

        model_name = kwargs.get('model', '')

        # === VEREINFACHTE LOGIK: Nur CPU-Toggle, sonst Ollama Auto-Detect ===
        # num_gpu wird NUR noch gesetzt wenn User explizit CPU-only will
        # Alle Hardware-Checks (VRAM, Model-Size, Context) macht jetzt Ollama selbst!

        if not enable_gpu:
            # CPU-only: num_gpu=0 explizit setzen (User-Wahl via Toggle)
            if 'num_gpu' not in kwargs['options']:
                kwargs['options']['num_gpu'] = 0
                debug_print(f"üîß [ollama.chat] CPU-only aktiviert (num_gpu=0) f√ºr {model_name}")
        else:
            # GPU aktiviert: Lass Ollama IMMER selbst entscheiden (Auto-Detect)
            # Ollama optimiert basierend auf: VRAM, Model-Size, Context-Gr√∂√üe
            debug_print(f"üîß [ollama.chat] GPU Auto-Detect f√ºr {model_name} (Ollama optimiert Layer-Aufteilung)")

    # Merge custom LLM-Parameter (User-Eingaben √ºberschreiben Hardware-Config!)
    if custom_options:
        if 'options' not in kwargs:
            kwargs['options'] = {}

        # User-Parameter haben PRIORIT√ÑT (√ºberschreiben Hardware-Config)
        for key, value in custom_options.items():
            if value is not None:
                # Spezial-Behandlung f√ºr num_ctx: User kann Hardware-Config √ºberschreiben!
                if key == 'num_ctx' and key in kwargs['options']:
                    old_val = kwargs['options'][key]
                    kwargs['options'][key] = value
                    debug_print(f"üë§ [ollama.chat] num_ctx √ºberschrieben: {old_val} ‚Üí {value} (User-Eingabe)")
                elif key not in kwargs['options']:
                    kwargs['options'][key] = value

        if custom_options:
            # Filtere None-Werte f√ºr sauberes Debug-Log
            relevant = {k: v for k, v in custom_options.items() if v is not None}
            if relevant:
                debug_print(f"üé® [ollama.chat] Custom LLM-Parameter: {relevant}")

    # Rufe originale ollama.chat() Funktion auf
    response = _original_ollama_chat(*args, **kwargs)

    # Performance-Metriken loggen (zentral f√ºr alle ollama.chat() Aufrufe)
    _log_ollama_performance(response)

    return response


# Patche ollama.chat() global
ollama.chat = _patched_ollama_chat


def set_gpu_mode(enable_gpu=True, llm_options=None):
    """
    Setzt GPU-Modus und optionale LLM-Parameter f√ºr den aktuellen Request/Thread

    Args:
        enable_gpu: True = GPU aktiv, False = CPU only
        llm_options: dict mit LLM-Parametern (temperature, top_p, top_k, num_predict, etc.)

    WICHTIG: Muss am Anfang jeder Request-Funktion aufgerufen werden!
    Nach diesem Aufruf werden ALLE ollama.chat() Calls automatisch
    mit den korrekten Parametern versehen.

    Beispiel:
        set_gpu_mode(True, {"temperature": 0.8, "top_p": 0.9, "num_predict": 200})
    """
    _thread_local.enable_gpu = enable_gpu
    _thread_local.custom_options = llm_options or {}

    if enable_gpu:
        debug_print(f"‚úÖ [GPU Mode] GPU-Beschleunigung aktiviert f√ºr diesen Request")
    else:
        debug_print(f"üñ•Ô∏è  [GPU Mode] CPU-only Modus aktiviert f√ºr diesen Request")

    if llm_options:
        # Filtere nur relevante Parameter f√ºr Debug-Ausgabe
        relevant = {k: v for k, v in llm_options.items() if v is not None}
        if relevant:
            debug_print(f"üé® [LLM Options] Custom Parameter: {relevant}")
