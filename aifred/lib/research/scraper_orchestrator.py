"""
Scraper Orchestrator - Parallel web scraping coordination

Handles:
- Scraping strategy based on mode (quick/deep)
- Parallel scraping with ThreadPoolExecutor
- Progress reporting
- LLM preloading during scraping
"""

import asyncio
from typing import Dict, List, AsyncIterator
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..agent_tools import scrape_webpage
from ..logging_utils import log_message


async def orchestrate_scraping(
    related_urls: List[str],
    mode: str,
    llm_client,
    model_choice: str
) -> AsyncIterator[Dict]:
    """
    Orchestrate parallel web scraping

    Args:
        related_urls: List of URLs to scrape
        mode: Scraping mode ('quick' or 'deep')
        llm_client: Main LLM client (for preloading)
        model_choice: Main LLM model name

    Yields:
        Dict: Progress updates, debug messages, scraping results

    Returns (via last yield):
        Tuple[List[Dict], List[Dict]]: (scraped_results, tool_results)
    """
    scraped_results: List[Dict] = []
    tool_results: List[Dict] = []

    if not related_urls:
        log_message("‚ö†Ô∏è Keine URLs gefunden, nur Abstract")
        yield {"type": "scraping_result", "data": (scraped_results, tool_results)}
        return

    log_message(f"üìã {len(related_urls)} URLs von Search-Engine gefunden")

    # Determine scraping strategy
    if mode == "quick":
        target_sources = 3
        initial_scrape_count = 3
        log_message("‚ö° Schnell-Modus: Scrape Top 3 URLs")
    elif mode == "deep":
        target_sources = 5
        initial_scrape_count = 7
        log_message(f"üîç Ausf√ºhrlich-Modus: Scrape Top {initial_scrape_count} URLs (Ziel: {target_sources} erfolgreiche)")
    else:
        target_sources = 3
        initial_scrape_count = 3

    scrape_limit = initial_scrape_count if mode == "deep" else target_sources
    urls_to_scrape = related_urls[:scrape_limit]

    yield {"type": "debug", "message": "üåê Web-Scraping startet (parallel)"}

    # PERFORMANCE: Preload Main LLM during scraping (only for backends that need it)
    # vLLM and TabbyAPI keep models loaded in VRAM, so preloading is unnecessary
    needs_preload = llm_client.backend_type not in ["vllm", "tabbyapi"]
    preload_task = None
    preload_message_sent = False

    if needs_preload:
        preload_task = asyncio.create_task(llm_client.preload_model(model_choice))
        log_message(f"üöÄ Haupt-LLM ({model_choice}) wird parallel vorgeladen...")
        yield {"type": "debug", "message": f"üöÄ Haupt-LLM ({model_choice}) wird vorgeladen..."}
    else:
        log_message(f"‚ÑπÔ∏è Haupt-LLM ({model_choice}) bereits geladen (Backend: {llm_client.backend_type})")
        yield {"type": "debug", "message": f"‚ÑπÔ∏è Haupt-LLM ({model_choice}) bereits geladen"}
        preload_message_sent = True  # Skip preload messages

    # Parallel Scraping
    log_message(f"üöÄ Parallel Scraping: {len(urls_to_scrape)} URLs gleichzeitig")

    # Start scraping progress
    yield {"type": "progress", "phase": "scraping", "current": 0, "total": len(urls_to_scrape), "failed": 0}

    # Parallel execution
    with ThreadPoolExecutor(max_workers=min(5, len(urls_to_scrape))) as executor:
        future_to_url = {
            executor.submit(scrape_webpage, url): url
            for url in urls_to_scrape
        }

        # Collect results as they complete
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            url_short = url[:60] + '...' if len(url) > 60 else url

            try:
                scrape_result = future.result(timeout=10)

                if scrape_result['success']:
                    tool_results.append(scrape_result)
                    scraped_results.append(scrape_result)
                    log_message(f"  ‚úÖ {url_short}: {scrape_result['word_count']} W√∂rter")
                else:
                    log_message(f"  ‚ùå {url_short}: {scrape_result.get('error', 'Unknown')}")

            except Exception as e:
                log_message(f"  ‚ùå {url_short}: Exception: {e}")

            # Check if preload finished and send message immediately
            if not preload_message_sent and preload_task and preload_task.done():
                try:
                    success, load_time = preload_task.result()
                    if success:
                        log_message(f"‚úÖ Haupt-LLM vorgeladen ({load_time:.1f}s)")
                        yield {"type": "debug", "message": f"‚úÖ Haupt-LLM vorgeladen ({load_time:.1f}s)"}
                    else:
                        log_message(f"‚ö†Ô∏è Haupt-LLM Preload fehlgeschlagen ({load_time:.1f}s)")
                        yield {"type": "debug", "message": f"‚ö†Ô∏è Haupt-LLM Preload fehlgeschlagen ({load_time:.1f}s)"}
                    preload_message_sent = True
                except Exception as e:
                    log_message(f"‚ö†Ô∏è Haupt-LLM Preload Exception: {e}")
                    preload_message_sent = True

            # Update progress
            completed = len([f for f in future_to_url if f.done()])
            failed = completed - len(scraped_results)
            yield {"type": "progress", "phase": "scraping", "current": len(scraped_results), "total": len(urls_to_scrape), "failed": failed}

    log_message(f"‚úÖ Parallel Scraping fertig: {len(scraped_results)}/{len(urls_to_scrape)} erfolgreich")

    # Wait for preload task to complete if not done yet
    if not preload_message_sent and preload_task:
        try:
            success, load_time = await preload_task
            if success:
                log_message(f"‚úÖ Haupt-LLM vorgeladen ({load_time:.1f}s)")
                yield {"type": "debug", "message": f"‚úÖ Haupt-LLM vorgeladen ({load_time:.1f}s)"}
            else:
                log_message(f"‚ö†Ô∏è Haupt-LLM Preload fehlgeschlagen ({load_time:.1f}s)")
                yield {"type": "debug", "message": f"‚ö†Ô∏è Haupt-LLM Preload fehlgeschlagen ({load_time:.1f}s)"}
        except Exception as e:
            log_message(f"‚ö†Ô∏è Haupt-LLM Preload Exception: {e}")

    # Return results
    yield {"type": "scraping_result", "data": (scraped_results, tool_results)}
