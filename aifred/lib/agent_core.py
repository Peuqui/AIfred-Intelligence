"""
Agent Core Module - AI Research and Decision Making

This module handles agent-based research workflows including:
- Query optimization
- URL rating with AI
- Multi-mode research (quick/deep/automatic)
- Interactive decision-making
"""

import time
import re
import threading
import ollama
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from .agent_tools import search_web, scrape_webpage, build_context
from .formatting import format_thinking_process, build_debug_accordion
from .logging_utils import debug_print, console_print, console_separator
from .message_builder import build_messages_from_history
from .prompt_loader import (
    get_url_rating_prompt,
    get_query_optimization_prompt,
    get_decision_making_prompt,
    get_intent_detection_prompt,
    get_followup_intent_prompt,
    get_system_rag_prompt
)

# Compiled Regex Patterns (Performance-Optimierung)
THINK_TAG_PATTERN = re.compile(r'<think>(.*?)</think>', re.DOTALL)

# ============================================================
# RESEARCH CACHE (Dependency Injection)
# ============================================================
_research_cache = None
_research_cache_lock = None

# ============================================================
# MODEL CONTEXT LIMITS
# ============================================================
# Speichert die Context-Limits der aktuell genutzten Modelle
# Diese werden beim Service-Start und bei Modellwechsel von Ollama abgefragt
_haupt_llm_context_limit = 4096      # Fallback: 4096 Tokens
_automatik_llm_context_limit = 4096  # Fallback: 4096 Tokens


def set_research_cache(cache_dict: Dict, lock: threading.Lock) -> None:
    """
    Sets the research cache and lock for dependency injection

    Args:
        cache_dict: Dictionary to store research results by session_id
        lock: threading.Lock() for thread-safe access
    """
    global _research_cache, _research_cache_lock
    _research_cache = cache_dict
    _research_cache_lock = lock


def get_cached_research(session_id: Optional[str]) -> Optional[Dict]:
    """
    Gets cached research for a session (thread-safe)

    Args:
        session_id: Session ID to lookup

    Returns:
        Cached research data or None if not found
    """
    # WICHTIG: Pr√ºfe _research_cache_lock und session_id, aber NICHT ob _research_cache leer ist!
    # Ein leeres Dictionary {} ist ein g√ºltiger (aber leerer) Cache-State!
    if _research_cache is None or _research_cache_lock is None or not session_id:
        debug_print("üîç DEBUG Cache-Lookup: _research_cache oder _research_cache_lock ist None, oder keine session_id")
        return None

    with _research_cache_lock:
        # DEBUG: Zeige Cache-Inhalt (Keys) f√ºr Diagnose
        cache_keys = list(_research_cache.keys())
        debug_print(f"üîç DEBUG Cache-Lookup: Suche session_id = {session_id[:8]}...")
        debug_print(f"   Cache enth√§lt {len(cache_keys)} Eintr√§ge: {[k[:8] + '...' for k in cache_keys]}")

        if session_id in _research_cache:
            cache_entry = _research_cache[session_id]
            debug_print(f"   ‚úÖ Cache-Hit! Eintrag gefunden mit {len(cache_entry.get('scraped_sources', []))} Quellen")
            return cache_entry.copy()
        else:
            debug_print(f"   ‚ùå Cache-Miss! session_id '{session_id[:8]}...' nicht in Cache")
    return None


def generate_cache_metadata(session_id: Optional[str], model_choice: str) -> None:
    """
    Generiert KI-basierte Metadata f√ºr gecachte Research-Daten (asynchron nach UI-Update).

    Diese Funktion wird NACH dem UI-Update aufgerufen, damit der User nicht auf die
    Metadata-Generierung warten muss. Sie holt den Cache, generiert eine semantische
    Zusammenfassung der Quellen, und updated den Cache.

    Args:
        session_id: Session ID f√ºr Cache-Lookup
        model_choice: LLM-Modell f√ºr Metadata-Generierung
    """
    if not session_id or not _research_cache or not _research_cache_lock:
        return

    try:
        # Hole Cache-Daten
        cache_entry = get_cached_research(session_id)
        if not cache_entry:
            debug_print("‚ö†Ô∏è Metadata-Generierung: Kein Cache gefunden")
            return

        scraped_sources = cache_entry.get('scraped_sources', [])
        if not scraped_sources:
            debug_print("‚ö†Ô∏è Metadata-Generierung: Keine Quellen im Cache")
            return

        debug_print("üìù Generiere KI-basierte Cache-Metadata...")
        console_print("üìù Erstelle Cache-Zusammenfassung...")

        # Baue Prompt f√ºr Metadata-Generierung
        sources_preview = "\n\n".join([
            f"Quelle {i+1}: {s.get('title', 'N/A')}\nURL: {s.get('url', 'N/A')}\nInhalt: {s.get('content', '')[:500]}..."
            for i, s in enumerate(scraped_sources[:3])  # Erste 3 Quellen f√ºr Kontext
        ])

        metadata_prompt = f"""Analysiere die Recherche-Quellen und erstelle eine KURZE Zusammenfassung (max 150 Zeichen):

QUELLEN:
{sources_preview}

AUFGABE:
Beschreibe in 1-2 S√§tzen:
- Welches Thema/Zeitraum wird abgedeckt?
- Welche Hauptinformationen sind enthalten?

BEISPIELE:
Wetter: "7-Tage-Vorhersage Niestetal (24.10-30.10): Temp 12-18¬∞C, Niederschlag, Wind"
News: "Nobelpreis Physik 2025: Verleihung 7.10, Gewinner-Namen nicht genannt"
Produkt: "iPhone 16 Pro: Display, Prozessor A18, Kamera 48MP, Akku 4685mAh"

Antworte NUR mit der Zusammenfassung (max 150 Zeichen), nichts anderes!"""

        # DEBUG: Zeige Metadata-Generierung Prompt vollst√§ndig
        debug_print("=" * 60)
        debug_print("üìã METADATA GENERATION PROMPT:")
        debug_print("-" * 60)
        debug_print(metadata_prompt)
        debug_print("-" * 60)
        debug_print(f"Prompt-L√§nge: {len(metadata_prompt)} Zeichen, ~{len(metadata_prompt.split())} W√∂rter")
        debug_print("=" * 60)

        messages = [{'role': 'user', 'content': metadata_prompt}]

        # DEBUG: Zeige Messages-Array vollst√§ndig
        debug_print("=" * 60)
        debug_print(f"üì® MESSAGES an {model_choice} (Metadata):")
        debug_print("-" * 60)
        for i, msg in enumerate(messages):
            debug_print(f"Message {i+1} - Role: {msg['role']}")
            debug_print(f"Content: {msg['content']}")
            debug_print("-" * 60)
        # Dynamisches num_ctx basierend auf Haupt-LLM-Limit (50% f√ºr Metadata, kurzer Output)
        metadata_num_ctx = min(2048, _haupt_llm_context_limit // 2)  # Max 2048 oder 50% des Limits

        debug_print(f"Total Messages: {len(messages)}, Temperature: 0.1, num_ctx: {metadata_num_ctx} (Haupt-LLM-Limit: {_haupt_llm_context_limit}), num_predict: 100")
        debug_print("=" * 60)

        metadata_start = time.time()
        metadata_response = ollama.chat(
            model=model_choice,
            messages=messages,
            options={'temperature': 0.1, 'num_ctx': metadata_num_ctx, 'num_predict': 100}
        )
        metadata_summary = metadata_response['message']['content'].strip()
        metadata_time = time.time() - metadata_start

        # K√ºrze auf max 150 Zeichen falls n√∂tig
        if len(metadata_summary) > 150:
            metadata_summary = metadata_summary[:147] + "..."

        # Update Cache mit Metadata
        with _research_cache_lock:
            if session_id in _research_cache:
                _research_cache[session_id]['metadata_summary'] = metadata_summary

        debug_print(f"‚úÖ Cache-Metadata generiert ({metadata_time:.1f}s): {metadata_summary}")
        console_print(f"‚úÖ Zusammenfassung erstellt {metadata_summary[:80]}{'...' if len(metadata_summary) > 80 else ''}")
        console_separator()

    except Exception as e:
        debug_print(f"‚ö†Ô∏è Fehler bei Metadata-Generierung: {e}")
        console_print(f"‚ö†Ô∏è Metadata-Generierung fehlgeschlagen")
        console_separator()


def save_cached_research(session_id: Optional[str], user_text: str, scraped_sources: List[Dict], mode: str, metadata_summary: Optional[str] = None) -> None:
    """
    Saves research to cache (thread-safe)

    Args:
        session_id: Session ID
        user_text: Original user question
        scraped_sources: List of scraped source dictionaries
        mode: Research mode used
        metadata_summary: Optional KI-generated semantic summary of sources
    """
    if _research_cache is None or _research_cache_lock is None or not session_id:
        debug_print("‚ö†Ô∏è DEBUG Cache-Speicherung fehlgeschlagen: Cache nicht initialisiert oder keine session_id")
        return

    with _research_cache_lock:
        _research_cache[session_id] = {
            'timestamp': time.time(),
            'user_text': user_text,
            'scraped_sources': scraped_sources,
            'mode': mode,
            'metadata_summary': metadata_summary  # üÜï KI-generierte Zusammenfassung
        }
        # DEBUG: Zeige Cache-Status nach Speichern
        cache_size = len(_research_cache)
        debug_print(f"üíæ Research-Cache gespeichert f√ºr Session {session_id[:8]}...")
        debug_print(f"   Cache enth√§lt jetzt {cache_size} Eintr√§ge: {[k[:8] + '...' for k in _research_cache.keys()]}")
        debug_print(f"   Gespeichert: {len(scraped_sources)} Quellen, user_text: '{user_text[:50]}...'")

        # DEBUG: Zeige KOMPLETTEN Cache-Inhalt
        debug_print("=" * 80)
        debug_print("üì¶ KOMPLETTER CACHE-INHALT:")
        debug_print("=" * 80)
        for cache_key, cache_value in _research_cache.items():
            source_urls = [s.get('url', 'N/A') for s in cache_value.get('scraped_sources', [])]
            debug_print(f"Session: {cache_key}")
            debug_print(f"  User-Text: {cache_value.get('user_text', 'N/A')}")
            debug_print(f"  Timestamp: {cache_value.get('timestamp', 0)}")
            debug_print(f"  Mode: {cache_value.get('mode', 'N/A')}")
            debug_print(f"  Quellen ({len(source_urls)}):")
            for i, url in enumerate(source_urls, 1):
                debug_print(f"    {i}. {url[:80]}{'...' if len(url) > 80 else ''}")
            debug_print("-" * 80)
        debug_print("=" * 80)


def delete_cached_research(session_id: Optional[str]) -> None:
    """
    Deletes cached research for a session (thread-safe)

    Args:
        session_id: Session ID to delete
    """
    if not _research_cache or not _research_cache_lock or not session_id:
        return

    with _research_cache_lock:
        if session_id in _research_cache:
            del _research_cache[session_id]
            debug_print(f"üóëÔ∏è Cache gel√∂scht f√ºr Session {session_id[:8]}...")


def estimate_tokens(messages):
    """
    Sch√§tzt Token-Anzahl aus Messages

    Args:
        messages: Liste von Message-Dicts mit 'content' Key

    Returns:
        int: Gesch√§tzte Anzahl Tokens (Faustregel: 1 Token ‚âà 4 Zeichen)
    """
    total_size = sum(len(m['content']) for m in messages)
    return total_size // 4


def query_model_context_limit(model_name: str) -> int:
    """
    Fragt das Context-Limit eines Modells von Ollama ab.

    Nutzt die original_context_length (Training Context) als sicheres Limit,
    nicht die erweiterte context_length (RoPE-Scaling).

    Diese Funktion wird NUR beim Service-Start und bei Modellwechsel aufgerufen,
    NICHT bei jedem Request!

    Args:
        model_name: Name des Ollama-Modells (z.B. "phi3:mini", "qwen3:8b")

    Returns:
        int: Original Context Limit in Tokens (z.B. 4096 f√ºr phi3:mini, 32768 f√ºr qwen3:8b)
             Fallback: 4096 wenn Abfrage fehlschl√§gt
    """
    try:
        # Ollama API abfragen
        response = ollama.show(model_name)
        # Konvertiere Pydantic-Objekt zu Dict (model_dump() statt deprecated dict())
        data = response.model_dump() if hasattr(response, 'model_dump') else response.dict()
        # WICHTIG: Key hei√üt 'modelinfo' nicht 'model_info'!
        model_details = data.get('modelinfo', {})

        # Suche nach original_context_length (sicherstes Limit)
        # Beispiel phi3: "phi3.rope.scaling.original_context_length": 4096
        # Beispiel qwen: "qwen2.context_length": 32768

        # PRIORIT√ÑT 1: Suche nach original_context_length (f√ºr Modelle mit RoPE-Scaling)
        for key, value in model_details.items():
            if 'original_context' in key.lower():
                limit = int(value)
                debug_print(f"üìè Model {model_name}: Context Limit = {limit} Tokens (aus {key}, original)")
                return limit

        # PRIORIT√ÑT 2: Suche nach .context_length (f√ºr Modelle ohne RoPE-Scaling)
        for key, value in model_details.items():
            if key.endswith('.context_length'):
                limit = int(value)
                debug_print(f"üìè Model {model_name}: Context Limit = {limit} Tokens (aus {key})")
                return limit

        # Fallback: Wenn nicht gefunden, nutze 4K (konservativ)
        debug_print(f"‚ö†Ô∏è Model {model_name}: Context Limit nicht gefunden, nutze 4096 Fallback")
        return 4096

    except Exception as e:
        debug_print(f"‚ö†Ô∏è Fehler beim Abfragen von Model-Info f√ºr {model_name}: {e}")
        return 4096  # Konservativer Fallback


def set_haupt_llm_context_limit(model_name: str) -> None:
    """
    Setzt das Context-Limit f√ºr das Haupt-LLM.
    Wird beim Service-Start und bei Modellwechsel aufgerufen.

    Args:
        model_name: Name des Haupt-LLM Modells
    """
    global _haupt_llm_context_limit
    _haupt_llm_context_limit = query_model_context_limit(model_name)
    debug_print(f"‚úÖ Haupt-LLM Context-Limit gesetzt: {_haupt_llm_context_limit}")


def set_automatik_llm_context_limit(model_name: str) -> None:
    """
    Setzt das Context-Limit f√ºr das Automatik-LLM.
    Wird beim Service-Start und bei Modellwechsel aufgerufen.

    Args:
        model_name: Name des Automatik-LLM Modells
    """
    global _automatik_llm_context_limit
    _automatik_llm_context_limit = query_model_context_limit(model_name)
    debug_print(f"‚úÖ Automatik-LLM Context-Limit gesetzt: {_automatik_llm_context_limit}")


def calculate_dynamic_num_ctx(messages, llm_options=None, is_automatik_llm=False):
    """
    Berechnet optimales num_ctx basierend auf Message-Gr√∂√üe und Model-Limit.

    Die Berechnung ber√ºcksichtigt:
    1. Message-Gr√∂√üe + 30% Puffer + 2048 f√ºr Antwort
    2. Model-Maximum (Haupt-LLM oder Automatik-LLM Limit)
    3. User-Override (falls in llm_options gesetzt)

    Args:
        messages: Liste von Message-Dicts mit 'content' Key
        llm_options: Dict mit optionalem 'num_ctx' Override
        is_automatik_llm: True wenn f√ºr Automatik-LLM berechnet wird (default: False = Haupt-LLM)

    Returns:
        int: Optimales num_ctx (gerundet auf Standard-Gr√∂√üen, geclippt auf Model-Limit)
    """
    # Check f√ºr manuellen Override
    user_num_ctx = llm_options.get('num_ctx') if llm_options else None
    if user_num_ctx:
        return user_num_ctx

    # Berechne Tokens aus Message-Gr√∂√üe
    estimated_tokens = estimate_tokens(messages)  # 1 Token ‚âà 4 Zeichen

    # Puffer: +30% f√ºr Varianz + 2048 f√ºr Antwort
    needed_tokens = int(estimated_tokens * 1.3) + 2048

    # Runde auf Standard-Gr√∂√üe
    if needed_tokens <= 2048:
        calculated_ctx = 2048
    elif needed_tokens <= 4096:
        calculated_ctx = 4096
    elif needed_tokens <= 8192:
        calculated_ctx = 8192
    elif needed_tokens <= 10240:
        calculated_ctx = 10240
    elif needed_tokens <= 12288:
        calculated_ctx = 12288
    elif needed_tokens <= 16384:
        calculated_ctx = 16384
    elif needed_tokens <= 20480:
        calculated_ctx = 20480  # 20K
    elif needed_tokens <= 24576:
        calculated_ctx = 24576  # 24K
    elif needed_tokens <= 28672:
        calculated_ctx = 28672  # 28K
    elif needed_tokens <= 32768:
        calculated_ctx = 32768  # 32K
    elif needed_tokens <= 49152:
        calculated_ctx = 49152  # 48K
    elif needed_tokens <= 65536:
        calculated_ctx = 65536  # 64K
    elif needed_tokens <= 98304:
        calculated_ctx = 98304  # 96K
    else:
        calculated_ctx = 131072  # 128K

    # WICHTIG: Clippe auf Model-Limit
    model_limit = _automatik_llm_context_limit if is_automatik_llm else _haupt_llm_context_limit
    llm_type = "Automatik-LLM" if is_automatik_llm else "Haupt-LLM"

    if calculated_ctx > model_limit:
        debug_print(f"‚ö†Ô∏è Context {calculated_ctx} > {llm_type}-Limit {model_limit}, clippe auf {model_limit}")

        # Zus√§tzliche Warnung NUR wenn Messages TATS√ÑCHLICH gr√∂√üer als Model-Limit
        if estimated_tokens > model_limit:  # Kontext √úBERSCHRITTEN
            console_print(f"‚ö†Ô∏è WARNUNG: Kontext √ºberschritten! ({estimated_tokens} Tokens > {model_limit} Tokens Limit)")
            console_print(f"‚ö†Ô∏è √Ñltere Messages werden abgeschnitten!")

        return model_limit

    return calculated_ctx


def detect_query_intent(user_query, automatik_model="qwen3:1.7b"):
    """
    Erkennt die Intent einer User-Anfrage f√ºr adaptive Temperature-Wahl

    Args:
        user_query: User-Frage
        automatik_model: LLM f√ºr Intent-Detection (default: qwen3:1.7b)

    Returns:
        str: "FAKTISCH", "KREATIV" oder "GEMISCHT"
    """
    prompt = get_intent_detection_prompt(user_query=user_query)

    try:
        debug_print(f"üéØ Intent-Detection f√ºr Query: {user_query[:60]}...")

        response = ollama.chat(
            model=automatik_model,
            messages=[{'role': 'user', 'content': prompt}],
            options={
                'temperature': 0.2,  # Niedrig f√ºr konsistente Intent-Detection
                'num_ctx': 4096  # Standard Context f√ºr Intent-Detection
            }
        )

        intent_raw = response['message']['content'].strip().upper()

        # Extrahiere Intent (auch wenn LLM mehr Text schreibt)
        if "FAKTISCH" in intent_raw:
            intent = "FAKTISCH"
        elif "KREATIV" in intent_raw:
            intent = "KREATIV"
        elif "GEMISCHT" in intent_raw:
            intent = "GEMISCHT"
        else:
            debug_print(f"‚ö†Ô∏è Intent unbekannt: '{intent_raw}' ‚Üí Default: FAKTISCH")
            intent = "FAKTISCH"  # Fallback

        debug_print(f"‚úÖ Intent erkannt: {intent}")
        return intent

    except Exception as e:
        debug_print(f"‚ùå Intent-Detection Fehler: {e} ‚Üí Fallback: FAKTISCH")
        return "FAKTISCH"  # Safe Fallback


def detect_cache_followup_intent(original_query, followup_query, automatik_model="qwen3:1.7b"):
    """
    Erkennt die Intent einer Nachfrage zu einer gecachten Recherche

    Args:
        original_query: Urspr√ºngliche Recherche-Frage
        followup_query: Nachfrage des Users
        automatik_model: LLM f√ºr Intent-Detection

    Returns:
        str: "FAKTISCH", "KREATIV" oder "GEMISCHT"
    """
    prompt = get_followup_intent_prompt(
        original_query=original_query,
        followup_query=followup_query
    )

    try:
        debug_print(f"üéØ Cache-Followup Intent-Detection mit {automatik_model}: {followup_query[:60]}...")

        response = ollama.chat(
            model=automatik_model,
            messages=[{'role': 'user', 'content': prompt}],
            options={
                'temperature': 0.2,
                'num_ctx': 4096  # Standard Context
            }
        )

        intent_raw = response['message']['content'].strip().upper()

        # Extrahiere Intent
        if "FAKTISCH" in intent_raw:
            intent = "FAKTISCH"
        elif "KREATIV" in intent_raw:
            intent = "KREATIV"
        elif "GEMISCHT" in intent_raw:
            intent = "GEMISCHT"
        else:
            debug_print(f"‚ö†Ô∏è Cache-Intent unbekannt: '{intent_raw}' ‚Üí Default: FAKTISCH")
            intent = "FAKTISCH"  # Bei Recherche-Nachfragen meist faktisch

        debug_print(f"‚úÖ Cache-Followup Intent ({automatik_model}): {intent}")
        return intent

    except Exception as e:
        debug_print(f"‚ùå Cache-Followup Intent-Detection Fehler: {e} ‚Üí Fallback: FAKTISCH")
        return "FAKTISCH"


def get_temperature_for_intent(intent):
    """
    Gibt die passende Temperature f√ºr einen Intent zur√ºck

    Args:
        intent: "FAKTISCH", "KREATIV" oder "GEMISCHT"

    Returns:
        float: Temperature (0.2, 0.5 oder 0.8)
    """
    temp_map = {
        "FAKTISCH": 0.2,
        "KREATIV": 0.8,
        "GEMISCHT": 0.5
    }
    return temp_map.get(intent, 0.2)  # Fallback: 0.2


def optimize_search_query(user_text, automatik_model, history=None):
    """
    Extrahiert optimierte Suchbegriffe aus User-Frage

    Args:
        user_text: Volle User-Frage (kann lang sein)
        automatik_model: Automatik-LLM f√ºr Query-Optimierung
        history: Chat History (optional, f√ºr Kontext bei Nachfragen)

    Returns:
        tuple: (optimized_query, reasoning_content)
    """
    prompt = get_query_optimization_prompt(user_text=user_text)

    # DEBUG: Zeige Query Optimization Prompt
    debug_print("=" * 60)
    debug_print("üìã QUERY OPTIMIZATION PROMPT:")
    debug_print("-" * 60)
    debug_print(prompt)
    debug_print("-" * 60)
    debug_print(f"Prompt-L√§nge: {len(prompt)} Zeichen, ~{len(prompt.split())} W√∂rter")
    debug_print("=" * 60)

    try:
        debug_print(f"üîç Query-Optimierung mit {automatik_model}")
        console_print("üîß Query-Optimierung startet")

        # Baue Messages mit History (letzte 2-3 Turns f√ºr Kontext bei Nachfragen)
        # Build messages from history (last 3 turns for context)
        messages = build_messages_from_history(history, prompt, max_turns=3)

        # DEBUG: Zeige Messages-Array vollst√§ndig
        debug_print("=" * 60)
        debug_print(f"üì® MESSAGES an {automatik_model} (Query-Opt):")
        debug_print("-" * 60)
        for i, msg in enumerate(messages):
            debug_print(f"Message {i+1} - Role: {msg['role']}")
            debug_print(f"Content: {msg['content']}")
            debug_print("-" * 60)
        # Dynamisches num_ctx basierend auf Automatik-LLM-Limit (100% f√ºr Query-Opt wegen History)
        query_num_ctx = min(8192, _automatik_llm_context_limit)  # Max 8192 oder volles Limit

        debug_print(f"Total Messages: {len(messages)}, Temperature: 0.3, num_ctx: {query_num_ctx} (Automatik-LLM-Limit: {_automatik_llm_context_limit})")
        debug_print("=" * 60)

        response = ollama.chat(
            model=automatik_model,
            messages=messages,
            options={
                'temperature': 0.3,  # Leicht kreativ f√ºr Keywords, aber stabil
                'num_ctx': query_num_ctx  # Dynamisch basierend auf Model
            }
        )

        raw_response = response['message']['content'].strip()

        # Extrahiere <think> Inhalt BEVOR wir ihn entfernen (f√ºr Debug-Output)
        think_match = THINK_TAG_PATTERN.search(raw_response)
        think_content = think_match.group(1).strip() if think_match else None

        # S√§ubern: Entferne <think> Tags und deren Inhalt
        optimized_query = THINK_TAG_PATTERN.sub('', raw_response)

        # Entferne Anf√ºhrungszeichen und Sonderzeichen
        optimized_query = re.sub(r'["\'\n\r]', '', optimized_query)
        optimized_query = ' '.join(optimized_query.split())  # Normalize whitespace

        # ============================================================
        # POST-PROCESSING: Temporale Kontext-Erkennung
        # ============================================================
        # Garantiert aktuelles Jahr bei zeitlich relevanten Queries, auch wenn LLM es vergisst

        # Dynamisches Jahr (nicht hardcoded!)
        current_year = str(datetime.now().year)

        temporal_keywords = [
            'neu', 'neue', 'neuer', 'neues', 'neueste', 'neuester', 'neuestes',
            'aktuell', 'aktuelle', 'aktueller', 'aktuelles',
            'latest', 'recent', 'new', 'newest',
            'beste', 'bester', 'bestes', 'best',
            'top', 'current'
        ]

        comparison_keywords = [
            'vs', 'versus', 'vergleich', 'compare', 'comparison',
            'oder', 'or', 'vs.', 'gegen', 'statt', 'instead'
        ]

        query_lower = optimized_query.lower()

        # Regel 1: "beste/neueste X" ‚Üí + aktuelles Jahr (falls nicht schon vorhanden)
        if any(kw in query_lower for kw in temporal_keywords) and current_year not in optimized_query:
            optimized_query += f" {current_year}"
            debug_print(f"   ‚è∞ Temporaler Kontext erg√§nzt: {current_year}")

        # Regel 2: "X vs Y" ‚Üí + aktuelles Jahr (falls nicht schon vorhanden)
        elif any(kw in query_lower for kw in comparison_keywords) and current_year not in optimized_query:
            optimized_query += f" {current_year}"
            debug_print(f"   ‚öñÔ∏è Vergleichs-Kontext erg√§nzt: {current_year}")

        debug_print(f"üîç Query-Optimierung:")
        debug_print(f"   Original: {user_text[:80]}{'...' if len(user_text) > 80 else ''}")
        debug_print(f"   Optimiert: {optimized_query}")

        # Return: Tuple (optimized_query, reasoning)
        return (optimized_query, think_content)

    except Exception as e:
        debug_print(f"‚ö†Ô∏è Fehler bei Query-Optimierung: {e}")
        debug_print(f"   Fallback zu Original-Query")
        return (user_text, None)


def ai_rate_urls(urls, titles, query, automatik_model):
    """
    KI bewertet alle URLs auf einmal (effizient!)

    Args:
        urls: Liste von URLs
        titles: Liste von Titeln (parallel zu URLs)
        query: Suchanfrage
        automatik_model: Automatik-LLM f√ºr URL-Bewertung

    Returns:
        Liste von {'url', 'score', 'reasoning'}, sortiert nach Score
    """
    if not urls:
        return []

    # Erstelle nummerierte Liste f√ºr KI mit Titel + URL
    url_list = "\n".join([
        f"{i+1}. Titel: {titles[i] if i < len(titles) else 'N/A'}\n   URL: {url}"
        for i, url in enumerate(urls)
    ])

    prompt = get_url_rating_prompt(query=query, url_list=url_list)

    # DEBUG: Zeige URL Rating Prompt
    debug_print("=" * 60)
    debug_print("üìã URL RATING PROMPT:")
    debug_print("-" * 60)
    debug_print(prompt)
    debug_print("-" * 60)
    debug_print(f"Prompt-L√§nge: {len(prompt)} Zeichen, ~{len(prompt.split())} W√∂rter")
    debug_print("=" * 60)

    try:
        debug_print(f"üîç URL-Rating mit {automatik_model}")

        messages = [{'role': 'user', 'content': prompt}]

        # DEBUG: Zeige Messages-Array vollst√§ndig
        debug_print("=" * 60)
        debug_print(f"üì® MESSAGES an {automatik_model} (URL-Rating):")
        debug_print("-" * 60)
        for i, msg in enumerate(messages):
            debug_print(f"Message {i+1} - Role: {msg['role']}")
            debug_print(f"Content: {msg['content']}")
            debug_print("-" * 60)
        # Dynamisches num_ctx basierend auf Automatik-LLM-Limit (100% f√ºr URL-Rating wegen vieler URLs)
        rating_num_ctx = min(8192, _automatik_llm_context_limit)  # Max 8192 oder volles Limit

        debug_print(f"Total Messages: {len(messages)}, Temperature: 0.0, num_ctx: {rating_num_ctx} (Automatik-LLM-Limit: {_automatik_llm_context_limit})")
        debug_print("=" * 60)

        response = ollama.chat(
            model=automatik_model,
            messages=messages,
            options={
                'temperature': 0.0,  # Komplett deterministisch f√ºr maximale Konsistenz!
                'num_ctx': rating_num_ctx  # Dynamisch basierend auf Model
            }
        )

        answer = response['message']['content']

        # Entferne <think> Bl√∂cke (falls Qwen3 Thinking Mode)
        answer_cleaned = THINK_TAG_PATTERN.sub('', answer).strip()

        # Parse Antwort
        rated_urls = []
        lines = answer_cleaned.strip().split('\n')

        for i, line in enumerate(lines):
            if not line.strip() or i >= len(urls):
                continue

            try:
                # Parse: "1. Score: 9 - Reasoning: ..."
                score_part = line.split('Score:')[1].split('-')[0].strip()
                score = int(score_part)

                reasoning_part = line.split('Reasoning:')[1].strip() if 'Reasoning:' in line else "N/A"

                rated_urls.append({
                    'url': urls[i],
                    'score': score,
                    'reasoning': reasoning_part
                })
            except Exception as e:
                debug_print(f"‚ö†Ô∏è Parse-Fehler f√ºr URL {i+1}: {e}")
                debug_print(f"   Problematische Zeile: '{line.strip()}'")
                debug_print(f"   Erwartet: '[NUM]. Score: [0-10] - Reasoning: [TEXT]'")
                # Fallback
                rated_urls.append({
                    'url': urls[i],
                    'score': 5,
                    'reasoning': "Parse-Fehler"
                })

        # Sortiere nach Score (beste zuerst)
        rated_urls.sort(key=lambda x: x['score'], reverse=True)

        debug_print(f"‚úÖ {len(rated_urls)} URLs bewertet")

        return rated_urls

    except Exception as e:
        debug_print(f"‚ùå Fehler bei URL-Rating: {e}")
        # Fallback: Gib URLs ohne Rating zur√ºck
        return [{'url': url, 'score': 5, 'reasoning': 'Rating fehlgeschlagen'} for url in urls]


def perform_agent_research(user_text, stt_time, mode, model_choice, automatik_model, history, session_id=None, temperature_mode='auto', temperature=0.2, llm_options=None):
    """
    Agent-Recherche mit AI-basierter URL-Bewertung

    Args:
        user_text: User-Frage
        stt_time: STT-Zeit
        mode: "quick" oder "deep"
        model_choice: Haupt-LLM f√ºr finale Antwort
        automatik_model: Automatik-LLM f√ºr Query-Opt & URL-Rating
        llm_options: Dict mit Ollama-Optionen (num_ctx, etc.) - Optional
        history: Chat History
        session_id: Session-ID f√ºr Research-Cache (optional)
        temperature_mode: 'auto' (Intent-Detection) oder 'manual' (fixer Wert)
        temperature: Temperature-Wert (0.0-2.0) - nur bei mode='manual'

    Returns:
        tuple: (ai_text, history, inference_time)
    """

    agent_start = time.time()
    tool_results = []

    # Extrahiere num_ctx aus llm_options oder nutze Standardwerte
    if llm_options is None:
        llm_options = {}

    # Context Window Gr√∂√üen
    # Haupt-LLM: Vom User konfigurierbar (None = Auto, sonst fixer Wert)
    user_num_ctx = llm_options.get('num_ctx')  # Kann None sein!

    # Debug: Zeige Context Window Modus
    if user_num_ctx is None:
        debug_print(f"üìä Context Window: Haupt-LLM=Auto (dynamisch, Ollama begrenzt auf Model-Max)")
    else:
        debug_print(f"üìä Context Window: Haupt-LLM={user_num_ctx} Tokens (manuell gesetzt)")

    # DEBUG: Session-ID pr√ºfen
    debug_print(f"üîç DEBUG: session_id = {session_id} (type: {type(session_id)})")

    # 0. Cache-Check: Nachfrage zu vorheriger Recherche (von Automatik-LLM oder explizit)
    cache_entry = get_cached_research(session_id)
    cached_sources = cache_entry.get('scraped_sources', []) if cache_entry else []

    if cached_sources:
            debug_print(f"üíæ Cache-Hit! Nutze gecachte Recherche (Session {session_id[:8]}...)")
            debug_print(f"   Urspr√ºngliche Frage: {cache_entry.get('user_text', 'N/A')[:80]}...")
            debug_print(f"   Cache enth√§lt {len(cached_sources)} Quellen")

            # Console-Output f√ºr Cache-Hit
            console_print(f"üíæ Cache-Hit! Nutze gecachte Daten ({len(cached_sources)} Quellen)")
            original_q = cache_entry.get('user_text', 'N/A')
            console_print(f"üìã Urspr√ºngliche Frage: {original_q[:60]}{'...' if len(original_q) > 60 else ''}")

            # Nutze ALLE Quellen aus dem Cache
            scraped_only = cached_sources
            context = build_context(user_text, scraped_only)

            # System-Prompt f√ºr Nachfrage (allgemein, LLM entscheidet Fokus)
            system_prompt = f"""Du bist ein AI Voice Assistant mit ECHTZEIT Internet-Zugang!

Der User stellt eine Nachfrage zu einer vorherigen Recherche.

**Urspr√ºngliche Frage:** "{cache_entry.get('user_text', 'N/A')}"
**Aktuelle Nachfrage:** "{user_text}"

# VERF√úGBARE QUELLEN (aus vorheriger Recherche):

{context}

# üö´ ABSOLUTES VERBOT - NIEMALS ERFINDEN:
- ‚ùå KEINE Namen von Personen, Preistr√§gern, Wissenschaftlern (au√üer explizit in Quellen genannt!)
- ‚ùå KEINE Daten, Termine, Jahreszahlen (au√üer explizit in Quellen genannt!)
- ‚ùå KEINE Entdeckungen, Erfindungen, wissenschaftliche Details (au√üer explizit beschrieben!)
- ‚ùå KEINE Zahlen, Statistiken, Messungen (au√üer explizit in Quellen!)
- ‚ùå KEINE Zitate oder w√∂rtliche Rede (au√üer explizit zitiert!)
- ‚ö†Ô∏è BEI UNSICHERHEIT: "Laut den Quellen ist [Detail] nicht spezifiziert"
- ‚ùå NIEMALS aus Kontext "raten" oder "folgern" was gemeint sein k√∂nnte!

# AUFGABE:
- Beantworte die Nachfrage AUSF√úHRLICH basierend auf den verf√ºgbaren Quellen
- Wenn der User eine spezifische Quelle erw√§hnt (z.B. "Quelle 1"), fokussiere darauf
- Gehe auf ALLE relevanten Details ein - ABER NUR was EXPLIZIT in Quellen steht!
- Zitiere konkrete Fakten: Namen, Zahlen, Daten, Versionen - NUR wenn EXPLIZIT genannt!
- ‚ö†Ô∏è WICHTIG: Nutze NUR Informationen die EXPLIZIT in den Quellen stehen!
- ‚ùå KEINE Halluzinationen oder Erfindungen!
- Falls Quelle nicht das enth√§lt was User fragt: "Diese Quelle enth√§lt keine Informationen √ºber [Detail]"

# ANTWORT-STIL:
- Sehr detailliert (3-5 Abs√§tze)
- Konkrete Details und Fakten nennen - aber NUR aus Quellen!
- Bei mehreren Quellen: Zeige Zusammenh√§nge auf
- Logisch strukturiert
- Deutsch

# QUELLENANGABE:
- LISTE AM ENDE **NUR** DIE TATS√ÑCHLICH GENUTZTEN QUELLEN AUF:

  **Quellen:**
  - Quelle 1: https://... (Thema: [Was wurde dort behandelt])
  - Quelle 2: https://... (Thema: [Was wurde dort behandelt])
  (etc.)"""

            # Generiere Antwort mit Cache-Daten
            messages = []

            # History hinzuf√ºgen (falls vorhanden) - LLM sieht vorherige Konversation
            for h in history:
                user_msg = h[0].split(" (STT:")[0].split(" (Agent:")[0] if " (STT:" in h[0] or " (Agent:" in h[0] else h[0]
                ai_msg = h[1].split(" (Inferenz:")[0] if " (Inferenz:" in h[1] else h[1]
                messages.extend([
                    {'role': 'user', 'content': user_msg},
                    {'role': 'assistant', 'content': ai_msg}
                ])

            # System-Prompt + aktuelle User-Frage
            messages.insert(0, {'role': 'system', 'content': system_prompt})
            messages.append({'role': 'user', 'content': user_text})

            # Dynamische num_ctx Berechnung f√ºr Cache-Hit (Haupt-LLM)
            final_num_ctx = calculate_dynamic_num_ctx(messages, llm_options, is_automatik_llm=False)
            if llm_options and llm_options.get('num_ctx'):
                debug_print(f"üéØ Cache-Hit Context Window: {final_num_ctx} Tokens (manuell)")
                console_print(f"ü™ü Context Window: {final_num_ctx} Tokens (manual)")
            else:
                estimated_tokens = estimate_tokens(messages)
                debug_print(f"üéØ Cache-Hit Context Window: {final_num_ctx} Tokens (dynamisch, ~{estimated_tokens} Tokens ben√∂tigt)")
                console_print(f"ü™ü Context Window: {final_num_ctx} Tokens (auto)")

            # Temperature entscheiden: Manual Override oder Auto (Intent-Detection)
            if temperature_mode == 'manual':
                final_temperature = temperature
                debug_print(f"üå°Ô∏è Cache-Hit Temperature: {final_temperature} (MANUAL OVERRIDE)")
                console_print(f"üå°Ô∏è Temperature: {final_temperature} (manual)")
            else:
                # Auto: Intent-Detection f√ºr Cache-Followup
                followup_intent = detect_cache_followup_intent(
                    original_query=cache_entry.get('user_text', ''),
                    followup_query=user_text,
                    automatik_model=automatik_model
                )
                final_temperature = get_temperature_for_intent(followup_intent)
                debug_print(f"üå°Ô∏è Cache-Hit Temperature: {final_temperature} (Intent: {followup_intent})")
                console_print(f"üå°Ô∏è Temperature: {final_temperature} (auto, {followup_intent})")

            # Console: LLM starts
            console_print(f"ü§ñ Haupt-LLM startet: {model_choice} (Cache-Daten)")

            llm_start = time.time()
            response = ollama.chat(
                model=model_choice,
                messages=messages,
                options={
                    'temperature': final_temperature,  # Adaptive oder Manual Temperature!
                    'num_ctx': final_num_ctx  # Dynamisch berechnet oder User-Vorgabe
                }
            )
            llm_time = time.time() - llm_start

            final_answer = response['message']['content']

            total_time = time.time() - agent_start

            # Console: LLM finished
            console_print(f"‚úÖ Haupt-LLM fertig ({llm_time:.1f}s, {len(final_answer)} Zeichen, Cache-Total: {total_time:.1f}s)")
            console_separator()

            # Formatiere <think> Tags als Collapsible (falls vorhanden)
            final_answer_formatted = format_thinking_process(final_answer, model_name=model_choice, inference_time=llm_time)

            # Zeitmessung-Text
            timing_text = f" (Cache-Hit: {total_time:.1f}s = LLM {llm_time:.1f}s)"
            ai_text_with_timing = final_answer_formatted + timing_text

            # Update History
            user_display = f"{user_text} (Agent: Cache-Hit, {len(cached_sources)} Quellen)"
            ai_display = ai_text_with_timing
            history.append([user_display, ai_display])

            debug_print(f"‚úÖ Cache-basierte Antwort fertig in {total_time:.1f}s")
            return (ai_text_with_timing, history, total_time)
    else:
        if session_id:
            debug_print(f"‚ö†Ô∏è Kein Cache f√ºr Session {session_id[:8]}... gefunden ‚Üí Normale Web-Recherche")

    # 1. Query Optimization: KI extrahiert Keywords (mit Zeitmessung und History-Kontext!)
    query_opt_start = time.time()
    optimized_query, query_reasoning = optimize_search_query(user_text, automatik_model, history)
    query_opt_time = time.time() - query_opt_start

    # 2. Web-Suche (Brave ‚Üí Tavily ‚Üí SearXNG Fallback) mit optimierter Query
    debug_print("=" * 60)
    debug_print(f"üîç Web-Suche mit optimierter Query")
    debug_print("=" * 60)

    search_result = search_web(optimized_query)
    tool_results.append(search_result)

    # Console Log: Welche API wurde benutzt?
    api_source = search_result.get('source', 'Unbekannt')

    # Zeige API-Stats (wenn vorhanden)
    stats = search_result.get('stats', {})
    apis_used = search_result.get('apis_used', [])

    if stats and apis_used:
        # Multi-API Search mit Stats
        total_urls = stats.get('total_urls', 0)
        unique_urls = stats.get('unique_urls', 0)
        duplicates = stats.get('duplicates_removed', 0)

        console_print(f"üåê Web-Suche: {', '.join(apis_used)} ({len(apis_used)} APIs)")
        if duplicates > 0:
            console_print(f"üîÑ Deduplizierung: {total_urls} URLs ‚Üí {unique_urls} unique ({duplicates} Duplikate)")
    else:
        # Single API oder alte Version
        console_print(f"üåê Web-Suche mit: {api_source}")

    # 2. URLs + Titel extrahieren (Search-APIs liefern bereits max 10)
    related_urls = search_result.get('related_urls', [])
    titles = search_result.get('titles', [])

    # Initialisiere Variablen f√ºr F√§lle ohne URLs
    rated_urls = []
    rating_time = 0.0  # Default: 0.0 statt None f√ºr sichere √úbergabe an build_debug_accordion

    if not related_urls:
        debug_print("‚ö†Ô∏è Keine URLs gefunden, nur Abstract")
    else:
        debug_print(f"üìã {len(related_urls)} URLs gefunden")

        # 3. AI bewertet alle URLs (1 Call!) - mit Titeln f√ºr bessere Aktualit√§ts-Erkennung
        debug_print(f"ü§ñ KI bewertet URLs mit {automatik_model}...")
        console_print(f"‚öñÔ∏è KI bewertet URLs mit: {automatik_model}")
        rating_start = time.time()
        rated_urls = ai_rate_urls(related_urls, titles, user_text, automatik_model)
        rating_time = time.time() - rating_start

        # Debug: Zeige ALLE Bewertungen (nicht nur Top 5)
        debug_print("=" * 60)
        debug_print("üìä URL-BEWERTUNGEN (alle):")
        debug_print("=" * 60)
        for idx, item in enumerate(rated_urls, 1):
            url_short = item['url'][:70] + '...' if len(item['url']) > 70 else item['url']
            reasoning_short = item['reasoning'][:80] + '...' if len(item['reasoning']) > 80 else item['reasoning']
            emoji = "‚úÖ" if item['score'] >= 7 else "‚ö†Ô∏è" if item['score'] >= 5 else "‚ùå"
            debug_print(f"{idx}. {emoji} Score {item['score']}/10: {url_short}")
            debug_print(f"   Grund: {reasoning_short}")
        debug_print("=" * 60)

        # 4. Scraping basierend auf Modus
        if mode == "quick":
            target_sources = 3
            initial_scrape_count = 3  # Quick-Modus: Kein Fallback n√∂tig
            debug_print(f"‚ö° Schnell-Modus: Scrape beste 3 URLs")
        elif mode == "deep":
            target_sources = 5  # Ziel: 5 erfolgreiche Quellen
            initial_scrape_count = 7  # Starte mit 7 URLs (Fallback f√ºr Fehler)
            debug_print(f"üîç Ausf√ºhrlich-Modus: Scrape beste {initial_scrape_count} URLs (Ziel: {target_sources} erfolgreiche)")
        else:
            target_sources = 3  # Fallback
            initial_scrape_count = 3

        # 4.5. Validierung: Fallback wenn rated_urls leer ist
        if not rated_urls:
            debug_print("‚ö†Ô∏è WARNUNG: Keine URLs konnten bewertet werden!")
            debug_print("   Fallback: Nutze Original-URLs ohne Rating")
            # Fallback: Nutze Original-URLs ohne Rating
            rated_urls = [{'url': u, 'score': 5, 'reasoning': 'No rating available'} for u in related_urls[:target_sources]]

        # 5. Scrape URLs PARALLEL (gro√üer Performance-Win!)
        console_print("üåê Web-Scraping startet (parallel)")

        # Filtere URLs nach Score und Limit
        # THRESHOLD GESENKT: 5 ‚Üí 3 (weniger restriktiv, mehr Quellen)
        # Deep-Modus: Starte mit initial_scrape_count URLs (Fallback f√ºr Fehler)
        scrape_limit = initial_scrape_count if mode == "deep" else target_sources
        urls_to_scrape = [
            item for item in rated_urls
            if item['score'] >= 3  # ‚Üê War 5, jetzt 3!
        ][:scrape_limit]  # Deep: 7 URLs, Quick: 3 URLs

        # FALLBACK: Wenn ALLE URLs < 3, nimm trotzdem die besten!
        if not urls_to_scrape and rated_urls:
            debug_print(f"‚ö†Ô∏è Alle URLs haben Score < 3 ‚Üí Nutze Top {target_sources} als Fallback")
            console_print(f"‚ö†Ô∏è Niedrige URL-Scores ‚Üí Nutze beste {target_sources} URLs als Fallback")
            urls_to_scrape = rated_urls[:target_sources]

        if not urls_to_scrape:
            debug_print("‚ö†Ô∏è Keine URLs zum Scrapen (rated_urls ist leer)")
            console_print("‚ö†Ô∏è Keine URLs verf√ºgbar ‚Üí 0 Quellen gescraped")
        else:
            debug_print(f"üöÄ Parallel Scraping: {len(urls_to_scrape)} URLs gleichzeitig")

            # Parallel Scraping mit ThreadPoolExecutor
            scraped_results = []
            with ThreadPoolExecutor(max_workers=min(5, len(urls_to_scrape))) as executor:
                # Starte alle Scrape-Tasks parallel
                future_to_item = {
                    executor.submit(scrape_webpage, item['url']): item
                    for item in urls_to_scrape
                }

                # Sammle Ergebnisse (in Completion-Order f√ºr Live-Feedback)
                for future in as_completed(future_to_item):
                    item = future_to_item[future]
                    url_short = item['url'][:60] + '...' if len(item['url']) > 60 else item['url']

                    try:
                        scrape_result = future.result(timeout=10)  # Max 10s pro URL (Download failed ‚Üí kein Playwright ‚Üí max 10s)

                        if scrape_result['success']:
                            tool_results.append(scrape_result)
                            scraped_results.append(scrape_result)
                            debug_print(f"  ‚úÖ {url_short}: {scrape_result['word_count']} W√∂rter (Score: {item['score']})")
                        else:
                            debug_print(f"  ‚ùå {url_short}: {scrape_result.get('error', 'Unknown')} (Score: {item['score']})")

                    except Exception as e:
                        debug_print(f"  ‚ùå {url_short}: Exception: {e} (Score: {item['score']})")

            debug_print(f"‚úÖ Parallel Scraping fertig: {len(scraped_results)}/{len(urls_to_scrape)} erfolgreich")

            # AUTOMATISCHES FALLBACK: Wenn zu wenige Quellen erfolgreich ‚Üí Scrape weitere URLs
            if mode == "deep" and len(scraped_results) < target_sources and len(urls_to_scrape) < len(rated_urls):
                missing_count = target_sources - len(scraped_results)
                already_scraped_urls = {item['url'] for item in urls_to_scrape}

                # Finde n√§chste URLs die noch nicht gescraped wurden
                remaining_urls = [
                    item for item in rated_urls
                    if item['url'] not in already_scraped_urls and item['score'] >= 3
                ][:missing_count + 2]  # +2 Reserve f√ºr weitere Fehler

                if remaining_urls:
                    debug_print(f"üîÑ Fallback: {len(scraped_results)}/{target_sources} erfolgreich ‚Üí Scrape {len(remaining_urls)} weitere URLs")
                    console_print(f"üîÑ Scrape {len(remaining_urls)} zus√§tzliche URLs (Fallback f√ºr Fehler)")

                    # Scrape zus√§tzliche URLs parallel
                    with ThreadPoolExecutor(max_workers=min(5, len(remaining_urls))) as executor:
                        future_to_item = {
                            executor.submit(scrape_webpage, item['url']): item
                            for item in remaining_urls
                        }

                        for future in as_completed(future_to_item):
                            item = future_to_item[future]
                            url_short = item['url'][:60] + '...' if len(item['url']) > 60 else item['url']

                            try:
                                scrape_result = future.result(timeout=10)

                                if scrape_result['success']:
                                    tool_results.append(scrape_result)
                                    scraped_results.append(scrape_result)
                                    debug_print(f"  ‚úÖ {url_short}: {scrape_result['word_count']} W√∂rter (Score: {item['score']})")

                                    # Stoppe wenn Ziel erreicht
                                    if len(scraped_results) >= target_sources:
                                        debug_print(f"üéØ Ziel erreicht: {len(scraped_results)}/{target_sources} Quellen")
                                        break
                                else:
                                    debug_print(f"  ‚ùå {url_short}: {scrape_result.get('error', 'Unknown')} (Score: {item['score']})")

                            except Exception as e:
                                debug_print(f"  ‚ùå {url_short}: Exception: {e} (Score: {item['score']})")

                    debug_print(f"‚úÖ Fallback-Scraping fertig: {len(scraped_results)} total (Ziel: {target_sources})")

            console_print(f"‚úÖ Web-Scraping fertig: {len(scraped_results)} URLs erfolgreich")

    # 6. Context Building - NUR gescrapte Quellen (keine SearXNG Ergebnisse!)
    # Filtere: Nur tool_results die 'word_count' haben (= erfolgreich gescraped)

    # DEBUG: Zeige ALLE tool_results Details BEVOR Filterung
    debug_print("=" * 80)
    debug_print(f"üîç SCRAPING RESULTS ANALYSE ({len(tool_results)} total results):")
    for i, result in enumerate(tool_results, 1):
        has_word_count = 'word_count' in result
        is_success = result.get('success', False)
        word_count = result.get('word_count', 0)
        url = result.get('url', 'N/A')[:80]
        debug_print(f"  {i}. {url}")
        debug_print(f"     success={is_success}, has_word_count={has_word_count}, words={word_count}")
    debug_print("=" * 80)

    scraped_only = [r for r in tool_results if 'word_count' in r and r.get('success')]

    debug_print(f"üß© Baue Context aus {len(scraped_only)} gescrapten Quellen (von {len(tool_results)} total)...")
    console_print(f"üß© {len(scraped_only)} Quellen mit Inhalt gefunden")

    # DEBUG: Zeige erste 200 Zeichen jeder gescrapten Quelle
    if scraped_only:
        debug_print("=" * 80)
        debug_print("üì¶ GESCRAPTE INHALTE (Preview erste 200 Zeichen):")
        for i, result in enumerate(scraped_only, 1):
            content = result.get('content', '')
            url = result.get('url', 'N/A')[:80]
            debug_print(f"Quelle {i} - {result.get('word_count', 0)} W√∂rter:")
            debug_print(f"  URL: {url}")
            debug_print(f"  Content: {content[:200].replace(chr(10), ' ')}...")
            debug_print("-" * 40)
        debug_print("=" * 80)
    else:
        debug_print("‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è WARNING: scraped_only ist LEER! Keine Daten f√ºr Context! ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è")
        console_print("‚ö†Ô∏è WARNUNG: Keine gescrapten Inhalte gefunden!")

    context = build_context(user_text, scraped_only)
    debug_print(f"üìä Context-Gr√∂√üe: {len(context)} Zeichen, ~{len(context)//4} Tokens")

    # DEBUG: Zeige ANFANG des Contexts (erste 800 Zeichen)
    debug_print("=" * 80)
    debug_print(f"üìÑ CONTEXT PREVIEW (erste 800 von {len(context)} Zeichen):")
    debug_print("-" * 80)
    debug_print(context[:800])
    if len(context) > 800:
        debug_print(f"\n... [{len(context) - 800} weitere Zeichen] ...")
    debug_print("=" * 80)

    # Console Log: Systemprompt wird erstellt
    console_print("üìù Systemprompt wird erstellt")

    # 7. Erweiterer System-Prompt f√ºr Agent-Awareness (MAXIMAL DIREKT!)
    system_prompt = get_system_rag_prompt(
        current_year=time.strftime("%Y"),
        current_date=time.strftime("%d.%m.%Y"),
        context=context
    )

    # 8. AI Inference mit History + System-Prompt
    messages = []

    # History hinzuf√ºgen (falls vorhanden)
    for h in history:
        user_msg = h[0].split(" (STT:")[0].split(" (Agent:")[0] if " (STT:" in h[0] or " (Agent:" in h[0] else h[0]
        ai_msg = h[1].split(" (Inferenz:")[0] if " (Inferenz:" in h[1] else h[1]
        messages.extend([
            {'role': 'user', 'content': user_msg},
            {'role': 'assistant', 'content': ai_msg}
        ])

    # System-Prompt + aktuelle User-Frage
    messages.insert(0, {'role': 'system', 'content': system_prompt})
    messages.append({'role': 'user', 'content': user_text})

    # DEBUG: Pr√ºfe Gr√∂√üe des System-Prompts
    debug_print(f"üìä System-Prompt Gr√∂√üe: {len(system_prompt)} Zeichen")
    debug_print(f"üìä Anzahl Messages an Ollama: {len(messages)}")
    total_message_size = sum(len(m['content']) for m in messages)
    estimated_tokens = estimate_tokens(messages)
    debug_print(f"üìä Gesamte Message-Gr√∂√üe an Ollama: {total_message_size} Zeichen, ~{estimated_tokens} Tokens")

    # DEBUG: Zeige ALLE Messages die an den Haupt-LLM gehen
    debug_print("=" * 80)
    debug_print(f"üì® MESSAGES an {model_choice} (Haupt-LLM mit RAG):")
    debug_print("-" * 80)
    for i, msg in enumerate(messages):
        debug_print(f"Message {i+1} - Role: {msg['role']}")
        content_preview = msg['content'][:500] if len(msg['content']) > 500 else msg['content']
        if len(msg['content']) > 500:
            debug_print(f"Content (erste 500 Zeichen): {content_preview}")
            debug_print(f"... [noch {len(msg['content']) - 500} Zeichen]")
        else:
            debug_print(f"Content: {content_preview}")
        debug_print("-" * 80)
    debug_print("=" * 80)

    # Console Logs: Stats
    console_print(f"üìä Systemprompt: {len(system_prompt)} Zeichen")
    console_print(f"üìä Messages: {len(messages)}, Gesamt: {total_message_size} Zeichen (~{estimated_tokens} Tokens)")

    # Dynamische num_ctx Berechnung (Haupt-LLM f√ºr Web-Recherche mit Research-Daten)
    final_num_ctx = calculate_dynamic_num_ctx(messages, llm_options, is_automatik_llm=False)
    if llm_options and llm_options.get('num_ctx'):
        debug_print(f"üéØ Context Window: {final_num_ctx} Tokens (manuell vom User gesetzt)")
        console_print(f"ü™ü Context Window: {final_num_ctx} Tokens (manuell)")
    else:
        debug_print(f"üéØ Context Window: {final_num_ctx} Tokens (dynamisch berechnet, ~{estimated_tokens} Tokens ben√∂tigt)")
        console_print(f"ü™ü Context Window: {final_num_ctx} Tokens (auto)")

    # Temperature entscheiden: Manual Override oder Auto (immer 0.2 bei Web-Recherche)
    if temperature_mode == 'manual':
        final_temperature = temperature
        debug_print(f"üå°Ô∏è Web-Recherche Temperature: {final_temperature} (MANUAL OVERRIDE)")
        console_print(f"üå°Ô∏è Temperature: {final_temperature} (manuell)")
    else:
        # Auto: Web-Recherche ‚Üí Immer Temperature 0.2 (faktisch)
        final_temperature = 0.2
        debug_print(f"üå°Ô∏è Web-Recherche Temperature: {final_temperature} (fest, faktisch)")
        console_print(f"üå°Ô∏è Temperature: {final_temperature} (auto, faktisch)")

    # Console Log: Haupt-LLM startet (im Agent-Modus)
    console_print(f"ü§ñ Haupt-LLM startet: {model_choice} (mit {len(scraped_only)} Quellen)")

    inference_start = time.time()
    response = ollama.chat(
        model=model_choice,
        messages=messages,
        options={
            'temperature': final_temperature,  # Adaptive oder Manual Temperature!
            'num_ctx': final_num_ctx  # Dynamisch berechnet oder User-Vorgabe
        }
    )
    inference_time = time.time() - inference_start

    agent_time = time.time() - agent_start

    ai_text = response['message']['content']

    # Console Log: Haupt-LLM fertig
    console_print(f"‚úÖ Haupt-LLM fertig ({inference_time:.1f}s, {len(ai_text)} Zeichen, Agent-Total: {agent_time:.1f}s)")
    console_separator()

    # 9. History mit Agent-Timing + Debug Accordion
    mode_label = "Schnell" if mode == "quick" else "Ausf√ºhrlich"
    user_with_time = f"{user_text} (STT: {stt_time:.1f}s, Agent: {agent_time:.1f}s, {mode_label}, {len(scraped_only)} Quellen)"

    # Formatiere mit Debug Accordion (Query Reasoning + URL Rating + Final Answer <think>) inkl. Inferenz-Zeiten
    ai_text_formatted = build_debug_accordion(query_reasoning, rated_urls, ai_text, automatik_model, model_choice, query_opt_time, rating_time, inference_time)

    history.append([user_with_time, ai_text_formatted])

    # Speichere Scraping-Daten im Cache (f√ºr Nachfragen) OHNE Metadata
    # Metadata wird sp√§ter asynchron generiert (nach UI-Update, damit User nicht warten muss)
    debug_print(f"üîç DEBUG Cache-Speicherung: session_id = {session_id}, scraped_only = {len(scraped_only)} Quellen")
    save_cached_research(session_id, user_text, scraped_only, mode, metadata_summary=None)

    debug_print(f"‚úÖ Agent fertig: {agent_time:.1f}s gesamt, {len(ai_text)} Zeichen")
    debug_print("=" * 60)
    debug_print("‚ïê" * 80)  # Separator nach jeder Anfrage

    return ai_text, history, inference_time


def chat_interactive_mode(user_text, stt_time, model_choice, automatik_model, voice_choice, speed_choice, enable_tts, tts_engine, history, session_id=None, temperature_mode='auto', temperature=0.2, llm_options=None):
    """
    Automatik-Modus: KI entscheidet selbst, ob Web-Recherche n√∂tig ist

    Args:
        user_text: User-Frage
        stt_time: STT-Zeit (0.0 bei Text-Eingabe)
        model_choice: Haupt-LLM f√ºr finale Antwort
        automatik_model: Automatik-LLM f√ºr Entscheidung
        voice_choice, speed_choice, enable_tts, tts_engine: F√ºr Fallback zu Eigenes Wissen
        history: Chat History
        session_id: Session-ID f√ºr Research-Cache (optional)
        temperature_mode: 'auto' (Intent-Detection) oder 'manual' (fixer Wert)
        temperature: Temperature-Wert (0.0-2.0) - nur bei mode='manual'
        llm_options: Dict mit Ollama-Optionen (num_ctx, etc.) - Optional

    Returns:
        tuple: (ai_text, history, inference_time)
    """

    debug_print("ü§ñ Automatik-Modus: KI pr√ºft, ob Recherche n√∂tig...")
    console_print("üì® User Request empfangen")

    # ============================================================
    # CODE-OVERRIDE: Explizite Recherche-Aufforderung (Trigger-W√∂rter)
    # ============================================================
    # Diese Keywords triggern SOFORT neue Recherche ohne KI-Entscheidung!
    explicit_keywords = [
        'recherchiere', 'recherchier',  # "recherchiere!", "recherchier mal"
        'suche im internet', 'such im internet',
        'schau nach', 'schau mal nach',
        'google', 'googel', 'google mal',  # Auch Tippfehler
        'finde heraus', 'find heraus',
        'check das', 'pr√ºfe das'
    ]

    user_lower = user_text.lower()
    if any(keyword in user_lower for keyword in explicit_keywords):
        debug_print(f"‚ö° CODE-OVERRIDE: Explizite Recherche-Aufforderung erkannt ‚Üí Skip KI-Entscheidung!")
        console_print(f"‚ö° Explizite Recherche erkannt ‚Üí Web-Suche startet")
        # Direkt zur Recherche, KEIN Cache-Check!
        return perform_agent_research(user_text, stt_time, "deep", model_choice, automatik_model, history, session_id, temperature_mode, temperature, llm_options)

    # ============================================================
    # Cache-Check: Baue Metadata f√ºr LLM-Entscheidung
    # ============================================================
    cache_metadata = ""
    cache_entry = get_cached_research(session_id)
    cached_sources = cache_entry.get('scraped_sources', []) if cache_entry else []

    if cached_sources:
            cache_age = time.time() - cache_entry.get('timestamp', 0)

            # üÜï Pr√ºfe ob KI-generierte Metadata verf√ºgbar ist
            metadata_summary = cache_entry.get('metadata_summary')

            if metadata_summary:
                # NEUE VERSION: Nutze KI-generierte semantische Zusammenfassung
                debug_print(f"üìù Nutze KI-generierte Metadata f√ºr Entscheidung: {metadata_summary}")
                sources_text = f"ü§ñ KI-Zusammenfassung der gecachten Quellen:\n\"{metadata_summary}\""
            else:
                # FALLBACK: Nutze URLs + Titel (alte Version)
                debug_print("üìù Nutze Fallback (URLs + Titel) f√ºr Entscheidung")
                source_list = []
                for i, source in enumerate(cached_sources[:5], 1):  # Max 5 Quellen zeigen
                    url = source.get('url', 'N/A')
                    title = source.get('title', 'N/A')
                    source_list.append(f"{i}. {url}\n   Titel: \"{title}\"")
                sources_text = "\n".join(source_list)

            cache_metadata = f"""

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ö†Ô∏è GECACHTE RECHERCHE VERF√úGBAR!

Urspr√ºngliche Frage: "{cache_entry.get('user_text', 'N/A')}"
Cache-Alter: {cache_age:.0f} Sekunden
Anzahl Quellen: {len(cached_sources)}

{sources_text}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

ENTSCHEIDUNG:
Kann "{user_text}" mit diesen gecachten Quellen beantwortet werden?

‚úÖ JA ‚Üí <search>context</search> (Cache nutzen!)
   Beispiele: "genauer?", "Quelle 1?", "mehr Details?"

‚ùå NEIN ‚Üí `<search>yes</search>` (neue Recherche n√∂tig!)
   Beispiele:
   - Andere Zeitangabe (morgen ‚Üí Wochenende)
   - Anderes Thema (Wetter ‚Üí Nobelpreis)
   - Quellen-URLs passen nicht zum neuen Thema
"""
            debug_print(f"üíæ Cache vorhanden: {len(cached_sources)} Quellen, {cache_age:.0f}s alt")
            debug_print(f"   Cache-Metadata wird an LLM √ºbergeben ({len(cache_metadata)} Zeichen)")
            debug_print("=" * 60)
            debug_print("üìã CACHE_METADATA CONTENT:")
            debug_print(cache_metadata)
            debug_print("=" * 60)

    # Schritt 1: KI fragen, ob Recherche n√∂tig ist (mit Zeitmessung!)
    decision_prompt = get_decision_making_prompt(
        user_text=user_text,
        cache_metadata=cache_metadata
    )

    # DEBUG: Zeige kompletten Prompt f√ºr Diagnose
    debug_print("=" * 60)
    debug_print("üìã DECISION PROMPT an phi3:mini:")
    debug_print("-" * 60)
    debug_print(decision_prompt)
    debug_print("-" * 60)
    debug_print(f"Prompt-L√§nge: {len(decision_prompt)} Zeichen, ~{len(decision_prompt.split())} W√∂rter")
    debug_print("=" * 60)

    try:
        # Zeit messen f√ºr Entscheidung
        debug_print(f"ü§ñ Automatik-Entscheidung mit {automatik_model}")

        # ‚ö†Ô∏è WICHTIG: KEINE History f√ºr Decision-Making!
        # Die History w√ºrde phi3:mini verwirren - es w√ºrde jede neue Frage
        # als "Nachfrage" interpretieren wenn vorherige √§hnliche Fragen existieren.
        # Beispiel: "Wie wird das Wetter morgen?" nach "Wie ist das Wetter?"
        # ‚Üí phi3:mini w√ºrde <search>context</search> antworten statt <search>yes</search>
        messages = [{'role': 'user', 'content': decision_prompt}]

        # Dynamisches num_ctx basierend auf Automatik-LLM-Limit (50% des Original-Context)
        decision_num_ctx = min(2048, _automatik_llm_context_limit // 2)  # Max 2048 oder 50% des Limits

        # DEBUG: Zeige Messages-Array vollst√§ndig
        debug_print("=" * 60)
        debug_print(f"üì® MESSAGES an {automatik_model} (Decision):")
        debug_print("-" * 60)
        for i, msg in enumerate(messages):
            debug_print(f"Message {i+1} - Role: {msg['role']}")
            debug_print(f"Content: {msg['content']}")
            debug_print("-" * 60)
        debug_print(f"Total Messages: {len(messages)}, Temperature: 0.2, num_ctx: {decision_num_ctx} (Automatik-LLM-Limit: {_automatik_llm_context_limit})")
        debug_print("=" * 60)

        decision_start = time.time()
        response = ollama.chat(
            model=automatik_model,
            messages=messages,
            options={
                'temperature': 0.2,  # Niedrig f√ºr konsistente yes/no Entscheidungen
                'num_ctx': decision_num_ctx  # Dynamisch basierend auf Model
            }
        )
        decision_time = time.time() - decision_start

        decision = response['message']['content'].strip().lower()

        debug_print(f"ü§ñ KI-Entscheidung: {decision} (Entscheidung mit {automatik_model}: {decision_time:.1f}s)")

        # ============================================================
        # Parse Entscheidung UND respektiere sie!
        # ============================================================
        if '<search>yes</search>' in decision or ('yes' in decision and '<search>context</search>' not in decision):
            debug_print("‚úÖ KI entscheidet: NEUE Web-Recherche n√∂tig ‚Üí Cache wird IGNORIERT!")
            console_print(f"üîç KI-Entscheidung: Web-Recherche JA ({decision_time:.1f}s)")

            # WICHTIG: Cache L√ñSCHEN vor neuer Recherche!
            # Die KI hat entschieden dass neue Daten n√∂tig sind (z.B. neue Zeitangabe)
            delete_cached_research(session_id)

            # Jetzt neue Recherche MIT session_id ‚Üí neue Daten werden gecacht
            return perform_agent_research(user_text, stt_time, "deep", model_choice, automatik_model, history, session_id, temperature_mode, temperature, llm_options)

        elif '<search>context</search>' in decision or 'context' in decision:
            debug_print("üîÑ KI entscheidet: Nachfrage zu vorheriger Recherche ‚Üí Versuche Cache")
            console_print(f"üíæ KI-Entscheidung: Cache nutzen ({decision_time:.1f}s)")
            # Rufe perform_agent_research MIT session_id auf ‚Üí Cache-Check wird durchgef√ºhrt
            # Wenn kein Cache gefunden wird, f√§llt es automatisch auf normale Recherche zur√ºck
            return perform_agent_research(user_text, stt_time, "deep", model_choice, automatik_model, history, session_id, temperature_mode, temperature, llm_options)

        else:
            debug_print("‚ùå KI entscheidet: Eigenes Wissen ausreichend ‚Üí Kein Agent")
            console_print(f"üß† KI-Entscheidung: Web-Recherche NEIN ({decision_time:.1f}s)")

            # Jetzt normale Inferenz MIT Zeitmessung
            # Build messages from history (all turns)
            messages = build_messages_from_history(history, user_text)

            # Console: Message Stats
            total_chars = sum(len(m['content']) for m in messages)
            console_print(f"üìä Messages: {len(messages)}, Gesamt: {total_chars} Zeichen (~{total_chars//4} Tokens)")

            # Dynamische num_ctx Berechnung f√ºr Eigenes Wissen (Haupt-LLM)
            final_num_ctx = calculate_dynamic_num_ctx(messages, llm_options, is_automatik_llm=False)
            if llm_options and llm_options.get('num_ctx'):
                debug_print(f"üéØ Eigenes Wissen Context Window: {final_num_ctx} Tokens (manuell)")
                console_print(f"ü™ü Context Window: {final_num_ctx} Tokens (manual)")
            else:
                estimated_tokens = estimate_tokens(messages)
                debug_print(f"üéØ Eigenes Wissen Context Window: {final_num_ctx} Tokens (dynamisch, ~{estimated_tokens} Tokens ben√∂tigt)")
                console_print(f"ü™ü Context Window: {final_num_ctx} Tokens (auto)")

            # Temperature entscheiden: Manual Override oder Auto (Intent-Detection)
            if temperature_mode == 'manual':
                final_temperature = temperature
                debug_print(f"üå°Ô∏è Eigenes Wissen Temperature: {final_temperature} (MANUAL OVERRIDE)")
                console_print(f"üå°Ô∏è Temperature: {final_temperature} (manual)")
            else:
                # Auto: Intent-Detection f√ºr Eigenes Wissen
                own_knowledge_intent = detect_query_intent(user_text, automatik_model)
                final_temperature = get_temperature_for_intent(own_knowledge_intent)
                debug_print(f"üå°Ô∏è Eigenes Wissen Temperature: {final_temperature} (Intent: {own_knowledge_intent})")
                console_print(f"üå°Ô∏è Temperature: {final_temperature} (auto, {own_knowledge_intent})")

            # Console: LLM starts
            console_print(f"ü§ñ Haupt-LLM startet: {model_choice}")

            # Zeit messen f√ºr finale Inferenz
            inference_start = time.time()
            response = ollama.chat(
                model=model_choice,
                messages=messages,
                options={
                    'temperature': final_temperature,  # Adaptive oder Manual Temperature!
                    'num_ctx': final_num_ctx  # Dynamisch berechnet oder User-Vorgabe
                }
            )
            inference_time = time.time() - inference_start

            ai_text = response['message']['content']

            # Console: LLM finished
            console_print(f"‚úÖ Haupt-LLM fertig ({inference_time:.1f}s, {len(ai_text)} Zeichen)")
            console_separator()

            # User-Text mit Timing (Entscheidungszeit + Inferenzzeit)
            if stt_time > 0:
                user_with_time = f"{user_text} (STT: {stt_time:.1f}s, Entscheidung: {decision_time:.1f}s, Inferenz: {inference_time:.1f}s)"
            else:
                user_with_time = f"{user_text} (Entscheidung: {decision_time:.1f}s, Inferenz: {inference_time:.1f}s)"

            # Formatiere <think> Tags als Collapsible (falls vorhanden) mit Modell-Name und Inferenz-Zeit
            ai_text_formatted = format_thinking_process(ai_text, model_name=model_choice, inference_time=inference_time)

            history.append([user_with_time, ai_text_formatted])
            debug_print(f"‚úÖ AI-Antwort generiert ({len(ai_text)} Zeichen, Inferenz: {inference_time:.1f}s)")
            debug_print("‚ïê" * 80)  # Separator nach jeder Anfrage
            console_separator()  # Separator auch in Console

            return ai_text, history, inference_time

    except Exception as e:
        debug_print(f"‚ö†Ô∏è Fehler bei Automatik-Modus Entscheidung: {e}")
        debug_print("   Fallback zu Eigenes Wissen")
        # Fallback: Verwende standard chat function (muss importiert werden in main)
        raise  # Re-raise to be handled by caller
