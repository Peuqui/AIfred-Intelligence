"""
URL Utilities - Normalization and Deduplication

Extracted from agent_tools.py for better modularity.
"""

import logging
from typing import List
from urllib.parse import urlparse

# Logging Setup
logger = logging.getLogger(__name__)


# ============================================================
# URL DEDUPLICATION UTILITIES
# ============================================================

def normalize_url(url: str) -> str:
    """
    Normalisiert URL fÃ¼r Deduplizierung

    Behandelt:
    - www. vs non-www
    - http vs https
    - Trailing slashes
    - URL fragments (#)
    - Query parameters (?) [optional - aktuell NICHT entfernt!]

    Args:
        url: URL zum Normalisieren

    Returns:
        Normalisierte URL
    """
    try:
        parsed = urlparse(url.lower().strip())

        # Normalisiere Domain (entferne www.)
        domain = parsed.netloc.replace('www.', '')

        # Normalisiere Path (entferne trailing slash)
        path = parsed.path.rstrip('/')

        # Behalte Query-Params (kÃ¶nnen wichtig sein, z.B. ?id=123)
        # Ignoriere Fragments (# Anker)
        query = parsed.query

        # Baue normalisierte URL
        normalized = f"{domain}{path}"
        if query:
            normalized += f"?{query}"

        return normalized

    except Exception as e:
        logger.warning(f"âš ï¸ URL-Normalisierung fehlgeschlagen fÃ¼r {url}: {e}")
        return url  # Fallback: Original-URL


def deduplicate_urls(urls: List[str]) -> List[str]:
    """
    Entfernt doppelte URLs aus Liste

    Nutzt Normalisierung um auch Ã¤hnliche URLs zu erkennen:
    - https://www.example.com/path/
    - https://example.com/path
    â†’ Beide zÃ¤hlen als gleich!

    Args:
        urls: Liste von URL-Strings

    Returns:
        Deduplizierte Liste (behÃ¤lt Reihenfolge)
    """
    seen = set()
    unique = []

    for url in urls:
        normalized = normalize_url(url)

        if normalized not in seen:
            seen.add(normalized)
            unique.append(url)  # Original-URL behalten, nicht normalisierte!

    duplicates_removed = len(urls) - len(unique)
    if duplicates_removed > 0:
        logger.info(f"ğŸ”„ Deduplizierung: {len(urls)} URLs â†’ {len(unique)} unique ({duplicates_removed} Duplikate entfernt)")

    return unique
